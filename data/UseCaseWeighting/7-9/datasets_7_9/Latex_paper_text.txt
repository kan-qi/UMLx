\documentclass[sigconf]{acmart}

%\pdfpagewidth=8.5in 
%\pdfpageheight=11in 

\usepackage{booktabs} % For formal tables

\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
% \setlength{\abovecaptionskip}{2pt} % Chosen fairly arbitrarily
% \setlength{\textfloatsep}{2pt}
% \setlength\abovedisplayskip{0pt}
% \setlength{\abovedisplayskip}{0pt}
% \setlength{\belowdisplayskip}{0pt}
% \g@addto@macro\normalsize{%
%   \setlength\abovedisplayskip{0pt}
%   \setlength\belowdisplayskip{0pt}
%   \setlength\abovedisplayshortskip{0pt}
%   \setlength\belowdisplayshortskip{0pt}
% }

% \addtolength{\itemsep}{-0.8in}
% DOI
% \acmDOI{10.475/123_4}

% % ISBN
% \acmISBN{123-4567-24-567/08/06}

% %Conference
% \acmConference[ICSE'18]{2018 International Conference on Software Engineering}{May 2018}{Gothenburg, Sweden} 
% \acmYear{2018}
% \copyrightyear{2018}

% \acmPrice{15.00}

% \copyrightyear{2018} 
% \acmYear{2018} 
% \setcopyright{acmcopyright}
% \acmConference[MiSE'18]{MiSE'18:IEEE/ACM 10th International Workshop on Modelling in Software Engineering }{May 27, 2018}{Gothenburg, Sweden}
% \acmBooktitle{MiSE'18: MiSE'18:IEEE/ACM 10th International Workshop on Modelling in Software Engineering , May 27, 2018, Gothenburg, Sweden}
% \acmPrice{15.00}
% \acmDOI{10.1145/3193954.3193955}
% \acmISBN{978-1-4503-5735-7/18/05}


\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{pdfpages}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% \SetAlFnt{\tiny}
% \SetAlCapFnt{\small}
% \SetAlCapNameFnt{\small}
% \usepackage{multirow}

% \newtheorem{mydef}{Definition}

\usepackage{flushend}
\usepackage{svg}


%\clubpenalty=10000
%\widowpenalty=10000
%\displaywidowpenalty=10000

%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          
% The testflow support page is at:                                        ***
% http://www.michaelshell.org/tex/testflow/


\begin{document}
%\includepdf[pages=-]{IMG_20170909_0001}
%\includepdf[pages=-]{IMG_20170909_0002}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Calibrating Use Case Points Using Bayesian Analysis}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
% \author{\IEEEauthorblockN{Kan Qi}
% \IEEEauthorblockA{University of Southern California\\
% USA\\
% kqi@usc.edu}
% \and
% \IEEEauthorblockN{Anandi Hira}
% \IEEEauthorblockA{University of Southern California\\
% USA\\
% a.hira@usc.edu}
% \and
% \IEEEauthorblockN{Elaine Venson}
% \IEEEauthorblockA{University of Southern California\\
% USA\\
% Venson@usc.edu}
% \and
% \IEEEauthorblockN{Barry W. Boehm}
% \IEEEauthorblockA{University of Southern California\\
% USA\\
% boehm@usc.edu}}

% \author{\IEEEauthorblockN{Kan Qi\IEEEauthorrefmark{1},
% Anandi Hira\IEEEauthorrefmark{2}, Elaine Venson\IEEEauthorrefmark{3} and
% Barry W. Boehm\IEEEauthorrefmark{4}}
% \IEEEauthorblockA{University of Southern California,
% USA\\
% Email: \IEEEauthorrefmark{1}kqi@usc.edu,
% \IEEEauthorrefmark{2}a.hira@usc.edu,
% \IEEEauthorrefmark{3}venson@usc.edu,
% \IEEEauthorrefmark{4}boehm@usc.edu}}
% \maketitle

% \author{Kan Qi}
%  \email{...}
%  \author{Anandi Hira}
%  \email{...}
%  \affiliation{University of Southern California}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Use Case Points (UCPs) have been widely used to estimate software size for object-oriented projects. Yet, many research papers criticize the UCPs methodology for not being verified and validated with data, leading to inaccurate size estimates. This paper explores the use of Bayesian analysis to calibrate the use case complexity weights of the UCPs method to improve size and effort estimation accuracy. Bayesian analysis integrates prior information (in this study, we use the weights defined by the UCPs method and the weights suggested by other research papers) with parameter values suggested by data. The effectiveness of the calibration approach is being empirically evaluated in this paper with 112 use case driven projects. We found that the Bayesian estimates of the use case complexity weights consistently provide better estimation accuracy compared to the original UCPs method, empirically calibrated weights, and expert-based weights.

%Removed this sentence from abstract, because it seemed too redundant: To be specific, Bayesian Analysis integrates prior information - the weights defined by the Use Case Points method and the weights suggested by other research papers, with parameter values suggested by data.

\end{abstract}

% no keywords


% \begin{IEEEkeywords}
% effort estimation, use case analysis, use case driven process, model calibration, data normalization, software size metrics, model-based analysis, incremental estimation model, use case points, object-oriented modeling, project management, bayesian analysis, local calibration
% \end{IEEEkeywords}

\keywords{effort estimation, use case analysis, use case driven process, model calibration, data normalization, software size metrics, model-based analysis, incremental estimation model, use case points, object-oriented modeling, project management, bayesian analysis, local calibration}



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


%\IEEEpubid{\makebox[\columnwidth]{\hfill \copyright~2017 IEEE}
%\hspace{\columnsep}\makebox[\columnwidth]{}}

% make the title area
\maketitle

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
% \IEEEpeerreviewmaketitle



\section{Introduction}
% Despite the amount of research that goes into cost and effort estimation, a survey from 2003 concluded that “out of 15 areas of software engineering and engineering management addressed, the topic of estimation was the single most problematic area” \cite{mcfall_software_2003}. 

Effort estimation has been regarded as a crucial driver for various software managerial decisions. For example, accurate effort estimation can avoid the risks of being over schedule or budget \cite{Boehm:cocomoII}\cite{Boehm:economics}. To ensure the utility of effort estimation in decision making, it is necessary for an effort estimation model to provide estimates at early stages of a project, during which time, very little information about the project or system is known. Use Case Points (UCPs) provides a functional size metric based on use cases, which can be measured early in a project. Due to its early applicability during the software development lifecycle, use case based metrics have gained wide acceptance \cite{kamal2011proposed}. While practitioners and research papers have reported the effectiveness of UCPs \cite{da2008applying, usman2014effort, schneider2001applying, agarwal2001estimating, anda2002improving, anda2001estimating}, the UCPs method has also been criticized because the complexity weights are not validated with data \cite{nassif2016enhancing} \cite{kamal2011proposed}. Karner set the complexity weights based on his domain knowledge gained from Objectory Systems \cite{karner1993metrics}. \par
% * <flyqk1991@gmail.com> 00:19:01 24 Apr 2018 UTC-0700:
% Not very sure about the meaning of this expression. Should we just say "over schedule or budget"
% ^ <flyqk1991@gmail.com> 10:07:56 24 Apr 2018 UTC-0700:
% Also updated the first line for the survey is not recent. Elaine also suggested this point
% * <flyqk1991@gmail.com> 11:12:09 06 Jan 2018 UTC+0800:
% As Elaine mentioned before, 2003 survey seems a little outdated. Would be an issue?
% ^ <anandihira1@gmail.com> 23:23:15 25 Feb 2018 UTC-0800:
% We can look for a more updated reference. Or, find another way to motivate the work we are doing. Maybe we don't necessarily need to describe that estimation is still an issue.


% Is this needed in below paragraph: For instance, Garner first proposed to classify use cases into different complexity levels based on the number of transactions \cite{karner1993resource}, and different weights are assigned to different complexity levels to reflect the relative influences on software size.

To overcome the difficulties, the authors of this paper use Bayesian analysis to calibrate the use case complexity weights, combining empirically calibrated results and expert estimates. Bayesian analysis has been applied in software project effort estimation models. For example, the weights for the cost drivers: AEXP, LTEX, FLEX, RUSE, and TEAM, in the COCOMO\textregistered II \cite{Chulani:Bayesian} cost estimation model are derived with Bayesian analysis to resolve the uninterpretable results that came from data analysis. In this paper, we follow a similar method of applying Bayesian analysis to combine prior information (weights set by experts) and sample information (data analysis results) with the goal of improving effort estimates with UCPs. We empirically validate the effectiveness of our approach on 112 historical projects and the results show that the Bayesian estimates of the weights consistently outperform the weights proposed by original UCPs method, the purely empirically calibrated weights, and the expert-based weights in terms of out-of-sample effort estimation accuracy.\par
% * <flyqk1991@gmail.com> 00:16:58 24 Apr 2018 UTC-0700:
% @Anandi, I guess bayesian analysis were used for : AEXP, LTEX, FLEX, RUSE, and TEAM drivers. Is it right?


 % Necessary? Also, by the nature of bayessian averaging, the variance of weight estimates by our approach is better than the weights calibrated/suggested by other approachs, which means our estimates of the weights are more stable if calibrated with a different data set.\par
 

The rest of the paper is structured into 6 sections. Section II instroduces the background of this research in terms of the underlying theories and methods. Section III details our approach in calibrating the use case complexity weights with Bayesian analysis. The datasets used for and the results from model calibration and validation are presented in Section IV. In Section V, previous work completed in modifying the UCPs method for better effort estimation is introduced. Lastly, we discuss the threats to validity of the results and make the conclusions of the empirical analysis in Sections VI and VII, respectively. \par
% * <flyqk1991@gmail.com> 10:09:53 24 Apr 2018 UTC-0700:
% Updated here too for the updated structure

%\addtocounter{footnote}{1}
%\footnotetext{CASE: Computer-aided software engineering}

\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{The counting process of UCPs}
\label{tab:counting_process_of_UCP}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{cp{5.0cm}c}
\hline
Step&Rules&Results\\
\hline
1
&Classify the use cases (C) into 3 levels of complexity ($LOC$), based on number of transactions ($NT$) in each use case:\newline
$\begin{aligned}
    LOC_c= 
\begin{cases}
    Simple,& NT_c <= 3\\
    Average, & NT_c <= 7\\
    Complex,  & NT_c > 7\\
\end{cases}
\end{aligned}$
&$LOC_c$\\
2
&Sum the number of weighted use cases as unadjusted use case weight ($UUCW$):\newline
$\begin{aligned}
UUCW = \sum_{c\in C}{W_c}
\end{aligned}$ \newline
Where:  \newline
$\begin{aligned}
    W_c= 
\begin{cases}
    5,& LOC_c = Simple\\
    10, & LOC_c = Average\\
    15,  & LOC_c = Complex\\
\end{cases}
\end{aligned}$
&$UUCW$\\
3
&Classify the actors (A) into 3 levels of complexity and assign a weight for each actor based on its level of complexity. Sum the number of weighted actors as unadjusted actor weight ($UAW$) \newline
$\begin{aligned}
UAW = \sum_{a\in A}{W_a}
\end{aligned}$  \newline
Where:  \newline
$\begin{aligned}
    W_a= 
\begin{cases}
    1,& LOC_a = Simple\\
    2, & LOC_a = Average\\
    3,  & LOC_a = Complex\\
\end{cases}
\end{aligned}$
&$UAW$\\
4
&Evaluate the 13 technical factors and calculate $TCF$ based on the sum of their impact ($TFactor$):\newline
$\begin{aligned}
TCF = 0.6 + (0.01*TFactor)
\end{aligned}$
&$TCF$\\
5
&Evaluate the 8 environmental factors and calculate $EF$ based on the sum of their impact ($EFactor$):\newline
$\begin{aligned}
EF = 1.4 + (-0.03*EFactor)
\end{aligned}$
&$EF$\\
6
&Calculate use case points ($UCP$) :\newline
$\begin{aligned}
UCP = (UUCW+UAW)*TCF*EF
\end{aligned}$
&$UCP$\\
\hline
\end{tabular}
\end{table}


%\IEEEpubidadjcol
\section{Background}
\subsection{Use Case Points (UCPs)}
Karner developed Use Case Points (UCPs) to estimate the effort for objected-oriented projects using the use case technique of gathering and understanding requirements \cite{karner1993metrics}. TABLE \ref{tab:counting_process_of_UCP} summarizes the steps and rules to calculate UCPs. Use cases are classified into three levels of complexity, based on the number of internal transactions within each use case. Each complexity level is assigned a weight to calculate the Unadjusted Use Case Weight (UUCW). The different weights represent their different effects on the software size. For example, if a use case contains 1-3 transactions, it is determined as a simple use case and has a weight of 5; whereas a use case with 5 transactions is determined as an average use case with a weight of 10. Karner explains how the complexity weights were set by stating: "The weights in this article are a first approximation by people at Objective Systems." \cite{karner1993resource}. As the software development environment has changed greatly since the development of the UCPs method, this weighting schema may not be applicable for modern use case driven projects. Therefore, an approach of calibrating the weights by updating the expert-based estimates with empirical data is proposed in this paper to make UCPs more adaptable for modern use case driven projects. \par

% Through empirical experiments, Karner found that a UCP required 20 person-hours to develop \cite{karner1993resource}. Schneider and Winters noticed that the ratio between UCPs and person-hours depend on the environmental factors \cite{schneider2001applying}. Other datasets found that the UCP to effort ratio could range from 15 to 30 person-hours, which concludes that a team or organization should use historical data to determine the ratio that best describes their environment \cite{sparks1999art}. \par
% * <flyqk1991@gmail.com> 00:44:27 24 Apr 2018 UTC-0700:
% I'm not feeling this part fit the theme of the paper, since it is about calibrating the productivity?

\subsection{Bayesian Analysis}
Bayesian Analysis has been used in building the COCOMO\textregistered II effort estimation models to combine domain experience and empirical study results \cite{Chulani:Bayesian}\cite{Boehm:cocomoII}. COCOMO\textregistered II combines effects of the cost drivers estimated by experts and the effects calibrated by data to solve the unintuitive results from the calibration - calibration returned negative values for some of the parameters. An empirical study verified that the parameter values resulting from Bayesian Analysis, which integrates expert judgment and data analysis, led to superior results compared to a model only based on expert judgment or data analysis \cite{Chulani:Bayesian}. \par


The Bayesian approach derives the posterior probability ($P(\theta|X)$) of a hypothesis by updating the prior probability ($P(\theta)$) as more evidence becomes available, which is defined as a likelihood function ($P(X|\theta)$) derived from a statistical model of the observed data. The posterior probability can be computed according to Bayes' theorem by Eq. (\ref{eq:bayesian_inference_of_posterior_probability}). 


\begin{equation}\label{eq:bayesian_inference_of_posterior_probability}
P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
\end{equation}

Using the derived posterior probability distribution, we are able to use the posterior mean to estimate the value of a parameter or variable using Eq. (\ref{eq:bayesian_esitmate_of_parameter}).

\begin{equation}\label{eq:bayesian_esitmate_of_parameter}
\tilde{\theta} = E(\theta) = \int_{\theta}\theta P(\theta|X)\,d\theta
\end{equation}

In the context of linear regression, the posterior mean can be computed as the weighted average of the a prior mean and the sample mean using Eq. (\ref{eq:bayesian_esitmate_of_parameter_by_weighted_average})\cite{o2004kendall}. The weights in the weighted average are the precision of the two sources of information (defined as the inverse of the variance). 
 
\begin{equation}\label{eq:bayesian_esitmate_of_parameter_by_weighted_average}
\tilde{\theta} = {(\alpha+\beta)}^{-1}*{(\alpha*\theta^{*} + \beta*\theta^{**})}
\end{equation}

In Eq. (\ref{eq:bayesian_esitmate_of_parameter_by_weighted_average}), $\tilde{\theta}$ is the posterior mean, or in other words, the Bayesian estimate of the parameter $\theta$. $\theta^{*}$ and $\theta^{**}$ are means of the a prior and sample information, and $\alpha$ and $\beta$ are the inverse of their variances called precision. The variance of the posterior distribution can be correspondingly computed as Eq.(\ref{eq:variance_for_bayesian_averaged_mean}).

\begin{equation}\label{eq:variance_for_bayesian_averaged_mean}
Var(\tilde{\theta}) = (\alpha + \beta)^{-1}
\end{equation}

The prior information usually consists of expert judgment, which has not been validated by data, while the sample information can be drived from statistical analysis on data. In this paper, the use case complexity weights from prevously published papers are used as prior information. The sample information consists of the use case weights determined by running multiple linear regression (MLR) on the dataset of 112 historical projects.


\section{The Bayesian Approach to Calibrate Use Case Weights}
\subsection{The calibration process}
As depicted in Figure 1, our approach of combining the prior information and the sample information of use case complexity weights using Bayesian analysis generally goes through the following steps:

\begin{enumerate}
\item
% * <venson@usc.edu> 15:56:06 03 Jan 2018 UTC-0800:
% Kan, it seems that the steps are not corresponding to the items in the picture. For example, the Step 1 in the text is the step 3 in the picture.
% ^ <flyqk1991@gmail.com> 11:01:45 06 Jan 2018 UTC+0800:
% Yes, will adjust the order.
Calculate the means and variances based on the complexity weights ($UC_{simple}$, $UC_{average}$, and $UC_{complex}$) proposed by the experts, as the prior information, denoted by the vectors $w_{a-pri} = \{w_1,w_2,w_3\}$,and $\delta_{a-pri}^2 = \{\delta_1^2,\delta_2^2,\delta_3^2\}$. The details of our approach to derive the prior information of the use case complexity weights are presented in Section III.B.

\item
Calculate $UUCW_{norm}$ by having the project effort $Effort_{real}$ (person-hours) divided by $EF$, $TCF$, and $\alpha$. $\alpha$ is the productivity factor calibrated by running a linear regression of $Effort_{real}$ against Use Case Points (UCPs). The details of calculating $Effort_{norm}$ are in Section III.C.

\item
Calibrate the weights for simple use cases ($UC_{simple}$), average use cases ($UC_{average}$), and complex use cases ($UC_{complex}$) by running multiple linear regression on the empirical dataset. The calibrated weights and their variances, denoted by vectors $w_{reg}=\{w_1^*,w_2^*,w_3^*\}$, and $\delta_{reg}^2=\{{\delta_1^*}^2,{\delta_2^*}^2,{\delta_3^*}^2\}$, are used as the sample information input to the Bayesian analysis process. This is further explained in Section III.C.

\item
Calculate the Bayesian estimates of the use case weights and their variances by perfoming a weighted average of the a prior means and the empirically calibrated weights for the use case complexity levels. The Bayesian estimates and variances are denoted by $w_{bayes}=\{\hat{w_1},\hat{w_2},\hat{w_3}\}$ and $\delta_{bayes}^2=\{\hat{\delta_1}^2,\hat{\delta_2}^2,\hat{\delta_3}^2\}$. The weights used in the averaging process are based on the variances of the two sources of information. Combining the prior information and sample information is further explained in Section III.D.

\end{enumerate}


\begin{figure}[!t]
% * <anandihira1@gmail.com> 20:36:52 11 Apr 2018 UTC-0700:
% Should define abbreviations used in figure either within the figure or somewhere in text: UC (Use Case), MLR (multiple linear regression)
\centering
\includegraphics[width=3.2in]{bayesian_averaged_weight_calibration_process.png}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Bayesian Averaged Weight Calibration}
\label{fig:bayesian_average_weight_calibration}
\end{figure}

\subsection{The prior information}
We did a systematic review of the papers that proposed different use case complexity weights in order to understand the differences in the weights applied in practice. The weights proposed by these authors are based on their domain knowledge or analyses on datasets for the purpose of improving the estimation accuracy of the original Use Case Points (UCPs). We calculated the mean and the variance of the weights proposed by these experts for each of the three use case complexity levels, and use them as the prior information in the Bayesian analysis. There are 6 different weighting schemes proposed by different researchers and practitioners (including the weighting schema used in the original UCPs definition). For instance, Kirmani and Wahid introduce an extra level of complexity to cover the number of transactions being larger than 15 \cite{Periyasamy:e-ucp}. Wang et al. use fuzzy logic to calculate weights and determine how complexity levels should be set, leading to different weights being applied to different ranges of transactions from the original UCPs method \cite{wang2009extended}. The sources of the weighting schemas are presented in TABLE \ref{tab: The_proposed_weighting_schema}, and their representations are in Fig. \ref{fig:use_case_point_weight_distribution}. \par

\begin{figure*}[!t]
% * <anandihira1@gmail.com> 20:49:35 10 Apr 2018 UTC-0700:
% Maybe make the dotted lines darker, because it's hard to see the first one (one of the bar graph's bars is covering most of it).
\centering
\includegraphics[width=16cm]{use_case_point_weight_distribution.png}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Use Case Point Weight Distribution}
\label{fig:use_case_point_weight_distribution}
\end{figure*}

\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{The weighting schemes from previous papers, which are represented in Fig. \ref{fig:use_case_point_weight_distribution}}
\label{tab: The_proposed_weighting_schema}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{cccc}
\hline
Wght. Schm.&Study&Year&Metric\\
\hline
1&Karner\cite{karner1993metrics}&1993&UCP\\
2&Wang et al.\cite{wang2009extended}&2009&EUCP\\
3&Kirmani and Wahid\cite{Periyasamy:e-ucp}&2009&Re-UCP\\
4&Nassif \cite{bou2012software}&2012& Soft-UCP\\
5&Minkiewicz\cite{Arlene}&2015&UCP Sizing\\
6&Nassif et al.\cite{nassif2016enhancing}&2016&Enhanced UCP\\
\hline
\end{tabular}
\end{table}

In Fig. \ref{fig:use_case_point_weight_distribution}, the two vertical dashed lines separate the number of transactions into three ranges representing different use case complexity levels (defined in the original UCPs method), and the weights for the different numbers of transactions in each range represent the experts' opinions on the effect a use case complexity level has on software size. We account for this when calculating the mean of the domain experts' weights: 
\begin{enumerate}
    \item For each use case complexity level ($l$), an expert ($i$) may have different ratings ($r_t(i)$) for different numbers of transactions ($t \in l$).
    \item The mean value ($r_l(i)$) of the ratings provided by an expert for each use case complexity level represents the expert's estimate of the weight ($w_i(l)$) that should be assigned to the use case complexity level. The mean value ($r_l(i)$) is calculated as the average of the ratings corresponding to the numbers of transactions within the original UCPs method's complexity level ($t \in l$) weighted by the probability ($Pr(t)$) that a use case will have that number of transactions. This is formally defined by Eq (\ref{eq: mean_of_weights_for_each_complexity_level}).
% * <venson@usc.edu> 16:11:17 03 Jan 2018 UTC-0800:
% I think some words are missing in the first phrase.
% ^ <flyqk1991@gmail.com> 22:47:15 05 Jan 2018 UTC+0800:
% Right, corrected it.
\end{enumerate}

\begin{equation}\label{eq: mean_of_weights_for_each_complexity_level}
w_l(i) = r_l(i) = \sum_{t \in l}r_t(i)*Pr(t)
\end{equation}

The probability ($Pr(t)$) is approximated by the relative frequency of use cases having $t$ transactions with respect to the total number of use cases of the complexity level ($l$) where $t \in l$. The frequency distribution ($f$) of use cases with respect to the number of transations ($NT$) is plotted in Fig. \ref{fig:use_case_point_weight_distribution}, which is based on a dataset of 35 projects being used as the sample data for this analysis.

\begin{equation}\label{eq: probability_of_weight_for_each_complexity_level}
Pr(t) = \frac{f(t)}{\sum_{k \in l}f(k)}
\end{equation}

After we calculate the mean weight value for each use case complexity level provided by the experts, the mean value ($w_l$) and variance ($\delta^2_l$) are calculated over the weights of each use case complexity level ($ l \in L$) (formalized by Eq. (\ref{eq: mean_of_means_for_each_complexity_level}) and Eq. (\ref{eq: variance_of_means_for_each_complexity_level})). Following the Bayesian analysis process, we calculate the weights $w_{a-pri} = \{w_1,w_2,w_3\}$ and their variances $\delta_{a-pri}^2 = \{\delta_1^2,\delta_2^2,\delta_3^2\}$ for the three levels of use case complexity. The results are displayed in TABLE \ref{tab:information_of_different_levels_of_use_case_complexity}. 

\begin{equation}\label{eq: mean_of_means_for_each_complexity_level}
w_l = \frac{1}{N}*{\sum_i^N{w_l(i)}}
\end{equation}

\begin{equation}\label{eq: variance_of_means_for_each_complexity_level}
\delta^2_l = \frac{1}{N}*{\sum_i^N(w_l(i)- w_l)^2}
\end{equation}

To simplify the calculation, one can also assume that the probability distribution of the number of transactions within a use case is uniform, such that the expert's estimate of the effect a complexity level has on the software size can be calculated as the average of the ratings for the numbers of transactions within that use case complexity level. In this case, no empirical distribution of the use cases with respect to different number of transactions ($NT$) is needed. The calculation is formalized by Eq. (\ref{eq: mean_of_weight_for_each_complexity_level_easy}).\par
% * <anandihira1@gmail.com> 21:01:52 10 Apr 2018 UTC-0700:
% But we don't do this, correct? Maybe this is not needed, then.

\begin{equation}\label{eq: mean_of_weight_for_each_complexity_level_easy}
w_l(i) = \frac{1}{|l|}*\sum_{t \in l}r_t(i)
\end{equation}

, where $|l|$ represents the length of the $NT$ range a complexity level $l$ covers.
% * <flyqk1991@gmail.com> 22:44:58 06 Jan 2018 UTC+0800:
% Is this sentence easy to understand?
% ^ <anandihira1@gmail.com> 20:45:34 11 Apr 2018 UTC-0700:
% I'm not really sure what you are trying to say here. This does need to be clarified.



\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Different Proposals of the Averaged Weights for Each UC Complexity Level}
\label{tab: different_proposals_of_the_averaged_weights}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{cccc}
\hline
Weight. Schm.&UC_{simple}&UC_{average}&UC_{complex}\\
\hline
1&5&10&15\\
2&5.93&10.24&18.95\\
3&5&7.66&15\\
4&5&7.66&16.84\\
5&5&10&16.84\\
6&5.81&8.49&14.19\\
Avg.&5.29&9.00&16.14\\
Var.&0.20&1.48&3.06\\
\hline
\end{tabular}
\end{table}

\subsection{The sample information}

%The dataset used in this analysis consists of 35 educational projects completed by teams of 5-12 people in 4-12 months. Most of the projects consist of mobile applications, web applications, and information management systems. The effort was retrieved from weekly timesheets. Fig. \ref{fig:project_descriptive_statistics} demonstrate the distribution of the effort, size in KSLOC (thousands of source lines of code), UCPs, and application type.  \par
% * <anandihira1@gmail.com> 21:04:47 10 Apr 2018 UTC-0700:
% Are the projects really that big to range between 0 and 20 KSLOC? Or should it just be SLOC?

% \begin{figure}[!t]
% \centering
% \includegraphics[width=8cm]{project_counting_statistics_for_bayesian_analysis.png}
% % where an .eps filename suffix will be assumed under latex, 
% % and a .pdf suffix will be assumed for pdflatex; or what has been declared
% % via \DeclareGraphicsExtensions.
% \caption{Use Case Points Counting Results}
% \label{fig:project_counting_statistics_for_bayesian_analysis}
% \end{figure}

Three steps are required to estimate the weights and their variances for each use case complexity level based on an empirical data set. 

First, we follow the normal UCP counting process to calculate the UCPs for each project. Specifically, we evaluate the numbers of simple use cases ($C_{sample}$), averages use cases ($C_{average}$), and complex use cases ($C_{complex}$) to calculate the Unadjusted Use Case Weight ($UUCW$); the numbers of simple actors ($A_{simple}$), average actors ($A_{Average}$), and complex actors ($A_{Complex}$) to calculate the Unadjusted Actor Weight ($UAW$); rated the environmental factors ($EF$) and technical complexity factors ($TCF$). Using all these numbers, we calculate the UCPs for each project by Eq. (\ref{eq: ucp}). An example of the distributions of the counting results based on our empirical data set (D1) are presented in Fig. \ref{fig:project_counting_statistics_for_bayesian_analysis}. We also record the number of transactions ($NT$) in each use case to generate the frequency distribution of the use cases with respect to $NT$, which is used in the prior calculation process.\par

% \begin{figure}[!t]
% \centering
% \includegraphics[width=8cm]{project_comprehensive_statistics_for_bayesian_analysis.png}
% % where an .eps filename suffix will be assumed under latex, 
% % and a .pdf suffix will be assumed for pdflatex; or what has been declared
% % via \DeclareGraphicsExtensions.
% \caption{Project Descriptive Statistics}
% \label{fig:project_descriptive_statistics}
% \end{figure}


Applying linear regression between actual effort and UCPs, we calibrate the productivity factor $\alpha$ using Eq. (\ref{eq:uc_linear_model}). The empirical productivity factor $\alpha$ represents the number of person-hours required to develop one unit of UCPs, which is used to calculate normalized effort ($Effort_{norm}$) of each data point using Eq. (\ref{eq:nuucw}). $Effort_{norm}$ is the expected effort under the normal conditions of UAW, EF, and TCF. Our goal is to calibrate the contributions of use case complexity levels to project effort. 

\begin{equation}\label{eq:uc_linear_model}
Effort_{real} = \alpha*UCP
\end{equation}

\begin{equation}\label{eq:nuucw}
UUCW_{norm} = \frac{Effort_{real}}{EF*TCF*\alpha}-UUAW
\end{equation}

% \begin{figure}[!t]
% \centering
% \includegraphics[width=8cm]{project_UC_counting_statistics_for_bayesian_analysis.png}
% % where an .eps filename suffix will be assumed under latex, 
% % and a .pdf suffix will be assumed for pdflatex; or what has been declared
% % via \DeclareGraphicsExtensions.
% \caption{Use Case Counts by Levels of Complexity}
% \label{fig:project_UC_counting_statistics_for_bayesian_analysis}
% \end{figure}

Lastly, we perform a multiple linear regression on $UC_{simple}$, $UC_{average}$, and $UC_{complexity}$, described by Eq. (\ref{eq:multipleregression_on_use_cases}), to calibrate the parameters $w_{reg}= \{w^*_1, w^*_2, w^*_3\}$. The variances of the parameters are calculated using Eq. (\ref{eq:variance_of_regression_estimates}). 

\begin{multline}\label{eq:multipleregression_on_use_cases}
UUCW_{norm} = w_1*UC_{simple}+w_2*UC_{average}
\\+w_3*UC_{complex}
\end{multline}

\begin{equation}\label{eq:variance_of_regression_estimates}
\theta^2 = {(X^TX)}^{-1}*{s^2}
\end{equation}

In our expriments, we apply the method to three datasets to evaluate the performance of the Bayesian approach of calibrating use case weights. TABLE \ref{tab:information_of_different_levels_of_use_case_complexity} provides the results of assessing the sample information from D1, which includes the calibrated weights and their variances for the three levels of complexity.


\begin{table*}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Empirical Data Set}
\label{tab:empirical_data_set}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{cccccccccccccccc}
 \hline
       Proj. No.&Simple&Average&Complex&UAW&TCF&EF&PH&Proj. No.&S&A&C&UAW&TCF&EF&PH\\
    \hline
       1&8&8&3&1&0.9&1.03&146&18&33&8&1&4&1.14&1.15&759\\
2&11&1&2&7&0.9&1.025&724&19&11&4&3&5&0.925&0.965&302.5\\
3&11&3&9&3&0.8&1.23&1125&20&9&2&5&16&0.935&0.935&1965\\
4&9&1&6&4&0.835&0.995&482&21&16&11&9&4&1.135&1.25&1392\\
5&8&2&17&2&0.925&1.04&657.5&22&10&13&3&2&0.89&1.025&804.5\\
6&12&6&3&2&0.925&0.995&263&23&6&15&13&3&0.94&0.875&1592\\
7&9&5&8&3&0.88&1.04&571&24&8&6&7&16&0.94&0.89&3113\\
8&12&33&2&1&0.96&1.03&547&25&3&4&15&1&1.175&1.31&737\\
9&15&6&3&1&0.915&1.025&268.5&26&2&4&12&3&1.25&1.025&1510.08\\
10&6&2&5&4&0.855&1.025&746&27&2&12&2&1&1.25&1.025&581.87\\
11&13&4&3&1&0.855&1.025&140.5&28&2&3&12&5&1.25&1.025&1560.53\\
12&6&4&3&4&0.795&0.92&1281&29&2&18&5&6&1.25&1.01375&3484\\
13&6&4&21&1&0.925&0.95&1482.5&30&5&4&22&3&1&1.0175&1561.38\\
14&6&12&5&6&1.04&1.12&1142&31&2&3&3&1&1.85&1.0025&784.4\\
15&16&16&6&2&0.805&1.025&617&32&2&12&14&28&1.25&1.030625&8094.34\\
16&9&5&2&13&0.885&0.89&1347&33&5&11&7&8&1.39&1.013&2810.95\\
17&13&2&9&27&1.12&1.325&3680&34&16&1&0&16&1.25&1.016&2130\\
  \hline
\end{tabular}
\end{table*}

\subsection{The Bayesian approach of combining a prior and sample information}
We update the prior information with the sample information by taking the weighted average of the weights $w_{a-pri}$ from expert judgment and the empirically calibrated weights $w_{reg}$. The weights used in the averaging process is based on the precision of the estimates, which is calculated as the inverses of the variances of the estimates: $\delta_{a-pri}^2$ and $\delta_{reg}^2$. The Bayesian averaged estimates of the weights for the different use case complexity levels and their variances are calculated using Eq. (\ref{eq:bayesian-estimate}) and Eq. (\ref{eq:bayesian-estimate-varaince}). \par

\begin{equation}\label{eq:bayesian-estimate}
w_{bayes}  = ( H_{a-pri} + H_{reg})^{-1}*(H_{a-pri}*w_{a-pri} + H_{reg}*w_{reg})
\end{equation}

, where $H_{a-pri} = \frac{1}{\delta_{a-pri}^2}$ and $H_{reg}= \frac{1}{\delta_{reg}^2}$

\begin{equation}\label{eq:bayesian-estimate-varaince}
\delta_{bayes}^2 = ( H_{a-pri} + H_{reg})^{-1}
\end{equation}


\section{Empirical Study}

\subsection{Data Sets}
We experimented our approach on three different datasets to evaluate the effectiveness of our approach in different software engineering settings.

The first data set is composed of 35 data points collected from master-level computer science student projects during 2014-2016, which lasted for 4-8 months. The projects were about developing web applications, mobile applications, mobile games, information systems, scientific tools, etc., and yielded source code from 1-10 KSLOC. 5-8 people collaborated on the projects by taking specific roles, including project manager, designer, architect, quality focal point, developer, tester, etc. All the projects followed the formal methods of software development, including use case driven, design-driven, risk-driven, plan-based, and agile approaches. The requirements were given by the real-word clients from start-ups, non-profits, education institutes, government agencies, etc., and they were closely involved in the engineering activities throughout the entire lifecycle. The products were tested and evaluated before their acceptance. Project effort was recorded through Jira tickets and weekly effort reports. The counting results for the factors used in the calculation of the size metrics are presented in Table \ref{tab:empirical_data_set}. We call this data set as D1.

The second data set is use case points bench marking dataset published by Radek Silhavy at the Promise Repository\cite{Silhavy20171}. This dataset consists of 71 data points collected from three software houses \cite{Silhavy2017benchmark} and has used for many use case related researches \cite{}. The projects are from different sectors of software development, including manufacturing, banking, communication, etc. The software products were developed in 3rd generation programming languages, including java, C\#, C++, etc., and were categorized into business application, real-time application, mathematically-intensive application, etc. Dfferent methodologies, for example, waterfall, personal software process, etc. were used in the development of the software products. We call this data set as D2.

D3 is the combination of D1 and D2. With this data set, we are able to observe the performance of the Bayesian estimator in the mixed environments to further evaluate the applicability of this approach in the general software engineering settings.


\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{The prior information, the sample information, and the Bayesian averaged estimates of different levels of use case complexity}
\label{tab:information_of_different_levels_of_use_case_complexity}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{cccccc}
 \hline
       Dataset&Estimator&Param.&S&A&C\\
    \hline
        \multirow{4}{*}{D1}&\multirow{2}{*}{Regression}&Est.&5.39&6.60&19.38\\
        &&Var.&3.13&4.60&4.43\\
        &\multirow{2}{*}{Bayesian}&Est.&5.30&8.42&17.46\\
        &&Var.&0.19&1.18&1.81\\
    \hline
        \multirow{4}{*}{D2}&\multirow{2}{*}{Regression}&Est.&-0.47&9.81&19.85\\
        &&Var.&51.40&8.25&9.29\\
        &\multirow{2}{*}{Bayesian}&Est.&5.27&9.13&17.06\\
        &&Var.&0.20&1.25&2.30\\
    \hline
       \multirow{4}{*}{D3}&\multirow{2}{*}{Regression}&Est.&-4.77&12.90&20.59\\
        &&Var.&9.95&5.62&6.52\\
        &\multirow{2}{*}{Bayesian}&Est.&5.09&9.82&17.56\\
        &&Var.&0.20&1.17&2.08\\
  \hline
\end{tabular}
\end{table}


\begin{figure*}[!t]
\centering
\includegraphics[width=17cm]{use_case_weight_bayesian_combination.png}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Use Case Weights by Bayesian Combination}
\label{fig:use_case_weight_bayesian_combination}
\end{figure*}

% \begin{table*}[!t]
% %% increase table row spacing, adjust to taste
% \renewcommand{\arraystretch}{1.3}
% % if using array.sty, it might be a good idea to tweak the value of
% % \extrarowheight as needed to properly center the text within the cells
% \caption{The prior information, the sample information, and the Bayesian averaged estimates of different levels of use case complexity}
% \label{tab:information_of_different_levels_of_use_case_complexity}
% \centering
% %% Some packages, such as MDW tools, offer better commands for making tables
% %% than the plain LaTeX2e tabular which is used here.
% \begin{tabular}{cccccccccccccccc}
%  \hline
%  &&&
%       \multicolumn{4}{c}{D1}&
%       \multicolumn{4}{c}{D2}&
%       \multicolumn{4}{c}{D3}\\
%  \hline
%       &
%       \multicolumn{2}{c}{Prior}&
%       \multicolumn{2}{c}{Regression}&
%       \multicolumn{2}{c}{Bayesian}&
%       \multicolumn{2}{c}{Regression}&
%       \multicolumn{2}{c}{Bayesian}&
%         \multicolumn{2}{c}{Regression}&
%       \multicolumn{2}{c}{Bayesian}\\
%         \hline
%       Level&Est.&Var.&Est.&Var.&Est.&Var&Est.&Var&Est.&Var&Est.&Var&Est.&Var\\
%     \hline
% Simple&5.29&0.20&4.53&2.66&5.24&0.19&-0.47&51.40&5.27&0.20&-4.77&9.95&5.09&0.20\\
% Average&9.00&1.48&6.97&3.59&8.42&1.05&9.81&8.25&9.13&1.25&12.90&5.62&9.82&1.17\\
% Complex&16.14&3.06&19.80&3.45&17.86&1.62&19.85&9.29&17.06&2.30&20.59&6.52&17.56&2.08\\
%   \hline
% \end{tabular}
% \end{table*}


\subsection{Model Calibration}
We applied the sample information calculation processs introduced in Section III.C on the three datasets to calibrate the weights for the use case complexities. After that, we updated the sample estimates using the prior information calculated in Section III.B. The sample information and the Bayesian estimates of the weights and variances are presented in Table \ref{tab:information_of_different_levels_of_use_case_complexity}. A graphical representation of the results from the model calibration based on D1 is presented in Fig. \ref{fig:use_case_weight_bayesian_combination}.

We observed three interesting facts based on the calibration results: \par
1. the Bayesian estimates of the weights for sample use cases are generally smaller than the prior estimates (by 0.9\% for D1, 0.3\% for D2, and 3.7\% for D3), while the Bayesian estimates of the weights assigned to complex use cases are generally larger than prior estimates (by 10.7\% for D1, 5.6\% for D2, and 8.8\% for D3 respectively). This phenomenon implies that the influence from the complex use cases toward project effort tends to be non-linearly increasing, instead of the linear relationship - simple being 5, average being 10, and complex being 15 - proposed by the original UCPs. This phenomenon is similarly obeserved by COCOMOII and formalized as the rule of "diseconomy of scale" in terms of SLOC \cite{Boehm:cocomoII}.\par

2. The Bayesian averaging approach corrects the counter-intuitive results from purely empirically calibrated results. For instance, the empirically calibrated weights for simple use cases are negative for both D2 and D3. Theoratically, this may be because of the small variances of the numbers of simple use cases in the empirical data set \cite{Boehm:cocomoII}\cite{Chulani:Bayesian}, which is testified by the large empirically estimated variances of the weights for simple use cases. For instance, the ratio between $\delta_{reg}^2 : \delta_{a-pri}^2$ is about 16:1 for D1, 257:1 for D2, and 50:1 for D3. The Bayesian approach corrects this counter-intuitive estimates for D2 and D3. Another potential solution to solve this problem may be adjusting the classification rules for complexity levels to allow more use cases to be determined as simple. This is beyond the scope of this research, but an interesting direction for our future study.\par

3. The variances of the Bayesian estimates of weights are smaller than both the prior estimates and empirically calibrated estimates, which means the Bayesian estimates of the weights are more stable if the weights are calibrated with the data points that are collected from different development environments.

\subsection{Model Evaluation}
\subsubsection{Evaluation Criteria} 
To evaluate the effectiveness of our proposed Bayesian method in determining use case complexity weights, we evaluate the effort estimation accuracy by MMRE and PRED, which are the commonly used accuracy measures in software engineering \cite{Boehm:cocomoII} \cite{nguyen2009studies}. Both MMRE and PRED rely on the quantity called magnitude of relative error (MRE), which is defined in Eq. (\ref{eq:mre}). MMRE measures the sample mean of MRE, while PRED($x$) measures the percentage of MRE within $x$. MMRE and PRED(x) can be calculated using Eq. (\ref{eq:mmre}) and Eq. (\ref{eq:pred}), respectively. These statistics give cost estimation practitioners the ability to state how often estimates can be expected to be within an acceptable margin of error. Since MMRE and PRED(.25) are most frequently used criteria \cite{nguyen2009studies}, in our discussion about the evaluation results (Section IV.C.2), we present the results for other metrics while emphasize on these two metrics. Since, as pointed out in \cite{nguyen2009studies}, there is no standard value of x for PRED(x) to be used for accuracy evaluation, and also we observed in our experiments that a model may perform better than another model in terms of PRED(.25) while perform worse than the same model for PRED(.30)), we propose to evaluate PRED(.01) to PRED(0.99) to comprehensively monitor the performance of the estimators. In addition to MMRE and PRED(.25), we also use the average over PRED(0.01) to PRED(0.99) in our comparison to decide which model is better. \par

\begin{equation}\label{eq:mre}
\begin{aligned}
MRE_i = \frac{|y_i-\hat{y_i}|}{y_i}
\end{aligned}
\end{equation}

\begin{equation}\label{eq:mmre}
\begin{aligned}
MMRE = \frac{1}{N}\sum_{i=1}^{N}MRE_i
\end{aligned}
\end{equation}

\begin{equation}\label{eq:pred}
\begin{aligned}
PRED(x) = \frac{1}{N}{\sum_{i=1}^{N}
\begin{cases}
      1, & \text{if}\ MRE_i \leq x \\
      0, & \text{otherwise}
\end{cases}
}
\end{aligned}
\end{equation}


\begin{figure*}[!t]
\centering
\includegraphics[width=17.5cm]{error_rate_estimate_cross_validation_comparison.png}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{5-fold Cross Validation Estimation Error Rates}
\label{fig:error_rate_estimates_by_10_fold_cross_validation}
\end{figure*}

\subsubsection{Evaluation Method and Results} We applied 5-fold cross validation to estimate the out-of-sample predication accuracy of Bayesian estimates of the weights, and compared it with the the out-of-sample accuracy by other estimation approaches. Cross validation is a technique to test a predication model on an independent dataset to better assess the performance of the model on new observations[20]. Specifically, for each of three data sets, it is separated into 5 folds, and 5 runs of training and testing are applied to evaluate the effort estimation accuracy with the chosen metrics: MMRE, PRED(.15), PRED(.25), and PRED(.50). The average of the values of MMRE, PRED(.15), PRED(.25), and PRED(.50) across the five runs are used as the final estimation prediction accuracy indicators. In each iteration of model training and testing, 80\% of the total data set (27 data points for D1, 56 data points for D2, and 89 data points for D3) are used as the training set to calibrate the weights for different use case complexity levels ($UC_{simple}$, $UC_{average}$, and $UC_{complex}$), and the remaining 20\% of the data points (7 data points for D1, 15 data points for D2, and 23 data points for D3) are used as the testing set to evaluate the performance of the estimation models. The predication accuracy testing results are presented in TABLE. \ref{tab:MMRE_PRED_for_10_fold_cross_validation}, and a more comprehensive evaluation of the predication accuracy of the three complexity weights is presented in Fig. \ref{fig:error_rate_estimates_by_10_fold_cross_validation}, which plots PRED(.01) - PRED(0.99) for the four size esitmators for the three data sets.
% * <anandihira1@gmail.com> 12:08:52 26 Dec 2017 UTC-0800:
% What does this mean for the Figure {error_rate_estimates_by_10_fold_cross_validation}: where depicts the percentage ($y$) of the estimated effort within relative deviation ($x$) of the actual value
% ^ <flyqk1991@gmail.com> 00:00:03 07 Jan 2018 UTC+0800:
% I wanted to say that, the figure evaluates the percentage of the estimates within certain relative deviation (from 1% - 99%) of the actual. I used the average over relative deviation from 1% - 99% to make the conclusions.
% ^ <flyqk1991@gmail.com> 00:00:40 07 Jan 2018 UTC+0800:
% Is it necessary to explain, or just leave out the details?

% Necessary? In each run of model testing, software size (UUCW) is estimated using the three use case complexity weights  calibrated from the training data set based on  in the testing data set, and $MMRE$ and $PRED$ are calculated based on the estimated software sizes and $UUCW_{norm}$ of the testing data set.


% I think we explained this earlier: The productivity factor $w$ used to calculate $UUC_{norm}$ of testing data set is calculated by having linear regression of real effort data on UCP using the testing data set, such that the calibration of the size estimators is independent from testing data set.


% Right, no need to reiterate the detail.

\subsubsection{Comparison of the size models} 
In the comparison between the four software sizing models, the effort estimation accuracy resulting from use case complexity weights calibrated with Bayesian analysis consistently outperforms the acccuracy using the models using the empirically calibrated weights, the expert-based weights, and the weights proposed by the original UCPs method for all the three datasets. For instance, based on our data set D1, the Bayesian method outperforms A-Priori, Original, and Regression methods by 32.9\%, 28.1\%, and 37.0\% respectively for MMRE, and 10.8\%, 18.8\%, and 11.6\% respectively for PRED(.25). The plot from PRED(.01) to PRED(.99) in Fig. \ref{fig:error_rate_estimates_by_10_fold_cross_validation} also certifies this significant improvement. We can also observe the improvement of the weights based on Bayesian analysis for other datasets, the Bayesian estimation model outperfoms other size methods. For example, the Bayesian method outperforms original UCPs method by 37.0\% for D1, 13.2\% for D2, and 2.2\% for D3 in terms of MMRE, and also by 11.6\%, 5.6\%, and 0.3\% for PRED(.25) for D1, D2, and D3 respectively. The same applies to the original and regression model. Based on the results, we conclude that the weights decided by Bayesian analysis, which combines expert-based estimates and data analysis, performs better than the weights decided using any one of them alone across the software engineering environment that the datasets represent.



\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Averge MMRE, PRED(.15), PRED(.25), PRED(.50) for 5-fold cross validation}
\label{tab:MMRE_PRED_for_10_fold_cross_validation}
\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{ccccccc}
 \hline
       Dataset&Estimator&M.&P.(.15)&P.(.25)&P.(.50)&P_Avg\\
    \hline
        \multirow{4}{*}{D1}&Bayesian&0.414&0.325&0.483&0.725\\
        &A-Priori&0.617&0.275&0.375&0.641\\
        &Original&0.576&0.245&0.295&0.585\\
        &Regression&0.657&0.233&0.367&0.558\\
    \hline
        \multirow{4}{*}{D2}&Bayesian&0.236&0.380&0.579&0.900\\
        &A-Priori&0.265&0.282&0.479&0.930\\
        &Original&0.265&0.296&0.464&0.930\\
        &Regression&0.272&0.296&0.523&0.888\\
    \hline
        \multirow{4}{*}{D3}&Bayesian&0.814&0.208&0.386&0.653\\
        &A-Priori&0.886&0.188&0.285&0.635\\
        &Original&0.889&0.179&0.286&0.635\\
        &Regression&0.832&0.240&0.383&0.607\\
  \hline
\end{tabular}
\end{table}

\subsubsection{Discussion of the Results} In addition to the proof of the improvement of the Bayesian analysis in measuring software size, we also have a few other observations based on our testing results. \par
First, In the mixed environment (D3), all the four size metrics decreases in effort estimation accuracy significantly. For instance, Bayesian method decreases by 43.0\% and 71.0\% in terms of MMRE and 9.7\% and 19.3\% in terms of PRED(.25) for D2 and D3 respectively. We can observe tther size metrics have similar deteriorations. This result strongly suggests to stratify the data set based on their inherent properties, for example, by teams, organizations, times, or development environments when developing calibrating effort estimation models based on size metrics, since better effort estimation accuracy would be achieved by calibrating effort estimation models based on homogeneious datasets. \par
Second, for samll and heterogenious datasets, expert-based size metrics, including Bayesian, A-Priori, and Original methods perform better than statistically determined size metric. Since as we can see, For PRED(.25), A-Priori and Original method performance better than Regression by 0.03 and 0.04 over PRED(.01) - PRED (0.99) for D1, and by 0.03 and 0.40 over PRED(.01) - PRED (0.99) for D2. For MMRE, we can also observe this tendency. However, as more data points added to calibrating and testing, Regression starts to perform better. For example, Regression model performs better than A-Priori and Original models for both D2 and D3. This phenomenon can be explained by tw reasons. First, the issue of overfitting exists when multiple regression is applied on small dataset, and as more data points available, parameters are more calibrated to the true values. Second, for heterogenious environments, the effects of different levels of use case complexity vary on effort. For example, more strict environment may require functions implemented for realizing use cases be more tested while less strict environment may requires less testing. The conflicts may be more considered by regression model due to its ability to find a mean number to cover the situations, while expert-based estimates of weights may be biased in the confliected situations.\par


\section{Related Work}

% * <flyqk1991@gmail.com> 10:03:13 15 Feb 2018 UTC-0800:
% maybe we should replace "constants" with "parameters"
% ^ <anandihira1@gmail.com> 19:17:00 11 Apr 2018 UTC-0700:
% I agree. I replaced constants with parameters.
When Karner first proposed UCP method in 1993, he explained that the weights in the model were a first approximation obtained by development team members at Objective Systems \cite{karner1993resource}. At that point, Karner also pointed out that more data were needed to adjust the model, weights and parameters. Since then, the method has been highly used, yet some limitations have also been reported. Fox example, Nassif et al.  argues that the lack of gradation when classifying the complexity of uses cases negatively affects the accuracy of estimations \cite{nassif2016enhancing}.\par
% * <flyqk1991@gmail.com> 10:11:00 15 Feb 2018 UTC-0800:
% Maybe we should put "For example, Nassif argues....". I wondered can we find other examples of criticism. Also in the end of the examples, we may also put "we consider that not having the weights empirically calibrated may lead to inaccuracy when applying UCP technique in other software engineering settings." ?
% ^ <venson@usc.edu> 10:03:20 17 Apr 2018 UTC-0700:
% Kan, I don't know if we can argue this consideration in this section. We also would have to include evidences for this statement.

To tackle the criticism that the originally defined complexity levels and the weights assigned to the levels might not reflect the actual situations, new approaches were proposed to improve this aspect of the UCPs estimation method. We can distinguish three main groups of approaches: the first group is based on on the addition of complexity levels, the second group is based the discretization of the complexity levels, and the third group is based on the calibration of the use case's and actor's weights.\par

Mudasir Manzoor Kirmani and Abdul Wahid proposed the Re-UCP method which is a revision of the UCP and e-UCP (extended use case point method \cite{periyasamy2009cost}). Re-UCP adds one extra rating level - "critical"- for both the use case and actor weighting schemes \cite{Kirmani:Revised}. They conducted an experiment with 51 students, who were trained and divided in groups to estimate the effort of 14 projects. They observed that the effort estimated using Re-UCP method was closer to the actual effort in comparison with estimated effort using UCP \& e-UCP methods. Nassif added three more use case complexity levels to the UCP original model, extending their complexity weights to 20, 25, and 30 points. Including other improvements to the UCPs method, he evaluated the proposed model with 65 industrial data points and achieved promising results \cite{bou2012software}. \par

Using fuzzy logic, Wang et al.\cite{Wang:Extended} and Nassif\cite{bou2012software} suggested discretizing the levels of complexity into more granular options and assigning corresponding weights to differentiate their effect on software size. Wang et al. applied fuzzy set theory to smooth over the abrupt classification of use cases, extending it from three to five categories. The authors demonstrated the effectiveness of their method through a case study \cite{Wang:Extended}. Nassif et al. proposed an enhancement to the model using fuzzy logic and neural networks. The authors used fuzzy logic to discretize the use case complexity levels into ten categories according to the number of transactions in a use case, maintaining the maximum level of transactions as 10 and the complexity factor of the largest use case is 15, as originally defined by Karner. Then, a neural networks approach was used to map the input vectors (use cases and actors) to an output vector (UUCP). The evaluation of this approach was conducted on 20 different projects and the results showed that the UCPs-based software estimation can be improved by up to 22\% in some projects \cite{nassif2016enhancing}. In another paper, Nassif proposed to empirically calibrate the weights of use cases into different complexity levels using neural networks \cite{nassif2014calibrating}. However, specific experiment results or details of the approach were not found. \par

Although these studies showed good results by extending the UCPs method, none of them consider calibrating the complexity weights. Our study presents a valuable contribution to the research in the UCPs method, by demonstrating how the complexity weights of use cases and actors can be empirically calibrated. We demonstrated that it is possible to improve the accuracy of UCPs-based software estimation if we replace the original weights by calibrating the complexity weights through Bayesian analysis, which combines data analysis results with experts' suggestions.\par


\section{Threats to Validity}
There are some aspects of this research that limit the wide or general applicability of the results. \par
As mentioned in the model calibration process (section III.B and III.D), both the processes of gathering the prior information and the sample information relies on the properties of the data points. Specifically, the prior information relies on the use case distribution with respect to $NT$, and the sample information is calculated by applying multiple linear regression on the numbers of use cases within different complexity levels. Therefore, the results of the use case complexity weights calibration and estimation accuracy may vary in different development environments. As shown in Fig \ref{fig:project_descriptive_statistics}, the studied projects are considered small as the sizes range from 1-20 ksloc and are done with 5-8 team members. Hence, the results presented in this paper may not be applicable to larger projects. Local calibration is encouraged to improve the accuracy when an empirical dataset is available for specific development environments.\par
% * <anandihira1@gmail.com> 20:15:08 11 Apr 2018 UTC-0700:
% 20 KSLOC = 20,000 SLOC. I don't know if this would be considered small? Are you sure they are in KSLOC and not just SLOC?
% Additionally, since the dataset is relatively small, the ratio between data points and parameters is about 13:1. It is possible that the complexity weights determined by multiple linear regression are overfitted, implying that there may be non-negligible influence on the conclusions when a different dataset (from the same population) is used. Only 8 data points are in the test dataset when applying five-fold cross validation, which is a relatively small set to make statistically significant conclusions about the size estimation accuracy and the superiority of the Bayesian analysis method to calibrate complexity weights.
% * <flyqk1991@gmail.com> 02:52:39 25 Apr 2018 UTC-0700:
% this is not applicable anymore


\section{Conclusions}
In this paper, we introduce a systematic approach of combining prior information (complexity weights suggested previously by experts) and the sample information to better calibrate use case complexity weights. To derive the prior information, we did a systematic review of previously published papers to summarize different proposals of the effects that use case complexity levels should have on software size. We also introduced a method of synthesizing the different proposals of weights for use case complexity within Bayesian analysis. Additionally, we perform an empirical study on 112 projects and calibrate the effects that use case complexity levels have on software size to derive the sample information in the Bayesian analysis. The results have shown the effectiveness of the Bayesian approach in improving the effort estimation accuracy and stability of the weights' estimates, by comparing its effort estimation accuracy with regression analysis, the original UCPs method, and the expert proposed weigths. The validation based on the data sets from open source use case benchmark dataset proves the applicability to more general software development environments. Future directions include collecting more data points that are representative for other typeical software development sitations and update the calibrated results for more general use, and also adjusting the underlying structure of classifying use cases based on our observations in this study to better estimate project effort.
% * <venson@usc.edu> 16:33:55 03 Jan 2018 UTC-0800:
% The last sentence is a little complicated. You may try to divide it into two sentences.
% ^ <flyqk1991@gmail.com> 00:28:44 07 Jan 2018 UTC+0800:
% Simplified the last sentence a little bit. Looks better?

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

% conference papers do not normally have an appendix


% use section* for acknowledgment
%\section*{Acknowledgment}


%he authors would like to thank...
%\cite{highsmith2000adaptive}
%\cite{azzeh:2013software}




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the 



% is modified later
%\Etriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\nocite{*}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,./bibili}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{2}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
% 0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}




% that's all folks
\end{document}


