benchmarkResults <- modelBenchmark(models, modelData)
models$smtiii = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
benchmarkResults <- modelBenchmark(models, modelData)
source("utils/model_funcs.R")
models$smtiii = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
benchmarkResults <- modelBenchmark(models, modelData)
source("transaction_based_model.R")
models$smtiii = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
benchmarkResults <- modelBenchmark(models, modelData)
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
# for model testing
m_predict.swtiii <- function(swtiii, testData){
print("swtiii predict function")
#using the means for each esimulation results as the final estimates of the parameters
swtiii_model <- apply(swtiii$m, 2, mean)
estimateEffortWithTrainedModel(swtiii_model, testData)
}
modelParams = analysisResults[[modelSelector]][["bayesModel"]]
swtiiiParams = list(
cuts = modelParams$cuts
)
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/model_funcs.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("utils/data_selection.R")
source("accuracy_confidence_evaluation.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
require(MASS)
benchmarkResults <- modelBenchmark(models, modelData)
View(models)
View(models)
modelBenchmark <- function(models, dataset){
nfold = 2
folds <- cut(seq(1,nrow(dataset)),breaks=nfold,labels=FALSE)
modelNames = names(models)
nmodels <- length(modelNames)
accuracy_metrics <- c('mmre','pred15','pred25','pred50', "mdmre", "mae")
nmetrics <- length(accuracy_metrics)
predRange <- 50
#data structure to hold the data for 10 fold cross validation
model_accuracy_indice <- c()
for(i in 1:length(modelNames)){
modelName = modelNames[i]
model_accuracy_indice <- cbind(model_accuracy_indice, paste(modelName, accuracy_metrics, sep="_"));
}
foldResults <- matrix(nrow=nfold,ncol=nmodels*nmetrics)
colnames(foldResults) <- model_accuracy_indice
foldResults1 <- array(0,dim=c(predRange,nmodels,nfold))
#Perform 10 fold cross validation
for(i in 1:nfold){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- dataset[testIndexes, ]
trainData <- dataset[-testIndexes, ]
eval_metrics = c()
eval_pred = c()
for(j in 1:nmodels){
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
model_eval_predict = cbind(predicted=m_predict(model, testData), actual=testData$Effort)
model_eval_mre = apply(model_eval_predict, 1, function(x) abs(x[1] - x[2])/x[2])
model_eval_mmre = mean(model_eval_mre)
model_eval_pred15 = length(model_eval_mre[model_eval_mre<=0.15])/length(model_eval_mre)
model_eval_pred25 = length(model_eval_mre[model_eval_mre<=0.25])/length(model_eval_mre)
model_eval_pred50 = length(model_eval_mre[model_eval_mre<=0.50])/length(model_eval_mre)
model_eval_mdmre = median(model_eval_mre)
model_eval_mae = sum(apply(model_eval_predict, 1, function(x) abs(x[1] - x[2])))/length(model_eval_predict)
eval_metrics <- c(
eval_metrics, model_eval_mmre,model_eval_pred15,model_eval_pred25,model_eval_pred50, model_eval_mdmre, model_eval_mae
)
eval_pred <- c()
for(k in 1:predRange){
eval_pred <- c(eval_pred, length(model_eval_mre[model_eval_mre<=0.01*k])/length(model_eval_mre))
}
}
foldResults[i,] = eval_metrics
foldResults1[,,i] = eval_pred
}
cvResults <- apply(foldResults, 2, mean);
names(cvResults) <- model_accuracy_indice
avgPreds <- matrix(nrow=predRange,ncol=nmodels+1)
colnames(avgPreds) <- c("Pred",modelNames)
for(i in 1:predRange)
{
avgPreds[i,] <- c(i, rep(0, length(modelNames)))
for(j in 1:length(modelNames)){
model_fold_mean = mean(foldResults1[i,j,]);
avgPreds[i,j+1] <- model_fold_mean
}
}
bootstrappingSE(dataset, models)
ret <-list(cvResults = cvResults, avgPreds = avgPreds)
}
benchmarkResults <- modelBenchmark(models, modelData)
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
# for model testing
m_predict.swtiii <- function(swtiii, testData){
print("swtiii predict function")
#using the means for each esimulation results as the final estimates of the parameters
swtiii_model <- apply(swtiii$m, 2, mean)
estimateEffortWithTrainedModel(swtiii_model, testData)
}
benchmarkResults <- modelBenchmark(models, modelData)
trainsaction_based_model <- function(analysisResults, modelSelector){
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
# for model testing
m_predict.swtiii <- function(swtiii, testData){
print("swtiii predict function")
#using the means for each esimulation results as the final estimates of the parameters
swtiii_model <- apply(swtiii$m, 2, mean)
estimateEffortWithTrainedModel(swtiii_model, testData)
}
modelParams = analysisResults[[modelSelector]][["bayesModel"]]
swtiiiParams = list(
cuts = modelParams$cuts
)
}
benchmarkResults <- modelBenchmark(models, modelData)
View(models)
View(models)
print(models[[j]])
j=1
print(models[[j]])
j]
print(modelNames[j])
modelNames = names(models)
modelName <- modelNames[j]
j=1
print(modelNames[j])
model = fit(trainData, modelNames[j], models[[j]])
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
source("utils/feature_selection.R")
source("utils/model_funcs.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("utils/data_selection.R")
source("accuracy_confidence_evaluation.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
require(MASS)
modelData <- selectData("dsets/modelEvaluations-1-3.csv")
models = list()
#first perform model search for the swtiii parameters
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#create an initialized model by selecting a candidate model
SWTIIIModelSelector <- 1
models$smtiii = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
View(models)
View(models)
model <- models$smtiii
View(model)
View(model)
benchmarkResults <- modelBenchmark(models, modelData)
modelNames = names(models)
nmodels <- length(modelNames)
modelName <- modelNames[j]
j = 1
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
model = fit(trainData, modelNames[j], models[[j]])
model <- structure(params, class = label)
model <- structure(params, class = label)
model <- m_fit(model, dataset)
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
View(m_fit)
View(m_fit)
View(m_fit)
View(m_fit)
View(m_fit)
View(m_fit)
#Define the generic functions and classes for model calibration and evaluation
m_fit <- function(x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
# for model training
m_fit.swtiii <<- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
# for model testing
m_predict.swtiii <<- function(swtiii, testData){
print("swtiii predict function")
#using the means for each esimulation results as the final estimates of the parameters
swtiii_model <- apply(swtiii$m, 2, mean)
estimateEffortWithTrainedModel(swtiii_model, testData)
}
modelParams = analysisResults[[modelSelector]][["bayesModel"]]
swtiiiParams = list(
cuts = modelParams$cuts
)
model = fit(trainData, modelNames[j], models[[j]])
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
model_eval_predict = cbind(predicted=m_predict(model, testData), actual=testData$Effort)
#first perform model search for the swtiii parameters
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#first perform model search for the swtiii parameters
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#create an initialized model by selecting a candidate model
SWTIIIModelSelector <- 1
models$smtiii = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
benchmarkResults <- modelBenchmark(models, modelData)
# for model training
m_fit.swtiii <<- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
# for model testing
m_predict.swtiii <- function(swtiii, testData){
print("swtiii predict function")
#using the means for each esimulation results as the final estimates of the parameters
swtiii_model <- apply(swtiii$m, 2, mean)
estimateEffortWithTrainedModel(swtiii_model, testData)
}
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
m_fit <- function(x,...) UseMethod('m_fit')
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
benchmarkResults <- modelBenchmark(models, modelData)
#Define the generic functions and classes for model calibration and evaluation
m_fit <- function(x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
#Define the generic functions and classes for model calibration and evaluation
m_fit <- function(x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
#Define the generic functions and classes for model calibration and evaluation
m_fit <- function(x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
#Define the generic functions and classes for model calibration and evaluation
m_fit <- function(x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
install.packages("invgamma")
install.apcakges("invgamma")
install.packages("invgamma")
#Define the generic functions and classes for model calibration and evaluation
m_fit <- function(x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
# for model training
m_fit.swtiii <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- transactionData$projects
regressionData <- generateRegresssionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
fitResults <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
swtiii$m = fitResults;
swtiii
}
# for model testing
m_predict.swtiii <- function(swtiii, testData){
print("swtiii predict function")
#using the means for each esimulation results as the final estimates of the parameters
swtiii_model <- apply(swtiii$m, 2, mean)
estimateEffortWithTrainedModel(swtiii_model, testData)
}
j = 1
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
model_eval_predict = cbind(predicted=m_predict(model, testData), actual=testData$Effort)
model = fit(trainData, modelNames[j], models[[j]])
model <- structure(models[[j]], class = "swtiii")
View(model)
View(model)
model <- m_fit(model, dataset)
model <- m_fit(model, trainData)
testData <- dataset[testIndexes, ]
trainData <- dataset[-testIndexes, ]
model <- m_fit(model, modelData)
model = fit(trainData, modelNames[j], models[[j]])
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
#Define the generic functions and classes for model calibration and evaluation
m_fit <- function(x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
model_eval_predict = cbind(predicted=m_predict(model, testData), actual=testData$Effort)
model = fit(trainData, modelNames[j], models[[j]])
x,...) UseMethod('m_fit')
fit <- function(dataset, label, params = list()){
model <- structure(params, class = label)
model <- m_fit(model, dataset)
}
m_predict <- function(x,...) UseMethod('m_predict')
model = fit(trainData, modelNames[j], models[[j]])
model_eval_predict = cbind(predicted=m_predict(model, testData), actual=testData$Effort)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("accuracy_confidence_evaluation.R")
#source("utils/model_funcs.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
require(MASS)
modelData <- selectData("dsets/modelEvaluations-1-3.csv")
#options(show.error.locations = TRUE)
#options(error=function()traceback(2))
#initialize models for training, testing, and evaluation
models = list()
#first perform model search for the swtiii parameters
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#first perform model search for the swtiii parameters
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#create an initialized model by selecting a candidate model
SWTIIIModelSelector <- 1
models$smtiii = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
benchmarkResults <- modelBenchmark(models, modelData)
