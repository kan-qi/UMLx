```{r setup, include=FALSE}
source("transaction_weights_calibration4.R")
source("transaction_weights_calibration4.R")
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv('./UCP_Dataset_OnlineV1.1.csv',stringsAsFactors= T)
print(cor(data$Trans, data$Real_Effort_Person_Hours))
data <- read.csv('./csv_result-desharnais.csv',stringsAsFactors= T)
data <- read.csv('./csv_result-desharnais.csv',stringsAsFactors= T)
print(cor(data$Transactions, data$Effort))
data <- read.csv('./UCP_Dataset_OnlineV1.1.csv',stringsAsFactors= T)
#data <- read.csv('./csv_result-desharnais.csv',stringsAsFactors= T)
print(cor(data$Transactions, data$Effort))
data <- read.csv('./UCP_Dataset_OnlineV1.1.csv',stringsAsFactors= T)
print(cor(data$Trans, data$Real_Effort_Person_Hours))
data <- read.csv('./csv_result-albrecht.csv',stringsAsFactors= T)
print(cor(data$Transactions, data$Effort))
<<<<<<< HEAD
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration4.R")
install.packages("ggplot2")
dataSet <- selectData("modelEvaluations-1-3.csv")
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration4.R")
install.packages("invgamma")
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration4.R")
source("comparison_between_size_metrics_sloc.R")
source("data_selection.R")
library(jsonlite)
=======
>>>>>>> 38c140a694f5dde98b6f208d32c3f6e9cc866237
source("transaction_weights_calibration4.R")
size_metric_models <- function(model, dataset){
#otherSizeMetricsData=modelData[c("Effort", "KSLOC", "COCOMO_Estimate", "Priori_COCOMO_Estimate", "UCP", "IFPUG", "MKII", "COSMIC")]
#otherSizeMetricsData <- na.omit(otherSizeMetricsData)
models = list()
print('ucp size metric based model')
ucp.train = function(trainData){
ucp.m = lm(Effort~UCP, data=trainData)
}
ucp.predict = function(testData){
predict(ucp.m, testData)
}
models[["ucp"]] = ucp
models
}
size_metric_models <- function(){
#otherSizeMetricsData=modelData[c("Effort", "KSLOC", "COCOMO_Estimate", "Priori_COCOMO_Estimate", "UCP", "IFPUG", "MKII", "COSMIC")]
#otherSizeMetricsData <- na.omit(otherSizeMetricsData)
models = list()
print('ucp size metric based model')
ucp.train = function(trainData){
ucp.m = lm(Effort~UCP, data=trainData)
}
ucp.predict = function(testData){
predict(ucp.m, testData)
}
models[["ucp"]] = ucp
models
}
source("transaction_weights_calibration4.R")
#adding two additional models: sloc and ln_sloc models.
modelBenchmark <- function(models, dataset){
modelNames <- names(models)
nfold = 5
folds <- cut(seq(1,nrow(SWTIIIModelData)),breaks=nfold,labels=FALSE)
nmodels <- length(models)
accuracy_metrics <- c('mmre','pred15','pred25','pred50', "mdmre", "mae",)
nmetrics <- length(accuracy_metrics)
predRange <- 50
#data structure to hold the data for 10 fold cross validation
model_accuracy_indice <- c()
for(i in 1:length(modelNames)){
modelName = modelNames[i]
model_accuracy_indice <- cbind(model_accuracy_indice, paste(moelName, accuracy_metrics, sep="_"));
}
foldResults <- matrix(nrow=nfold,ncol=nmodels*nmetrics)
colnames(foldResults) <- model_accuracy_indice
foldResults1 <- array(0,dim=c(predRange,nmodels,nfold))
#Perform 10 fold cross validation
for(i in 1:nfold){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- SWTIIIModelData[testIndexes, ]
trainData <- SWTIIIModelData[-testIndexes, ]
otherTestData <- otherSizeMetricsData[testIndexes, ]
otherTrainData <- otherSizeMetricsData[-testIndexes,]
for(j in 1:nmodels){
model <- models[[j]]
model.eval.predict = cbind(predicted=predict(model, testData), actual=testData$Effort)
model.eval.mre = apply(model.eval.predict, 1, function(x) abs(x[1] - x[2])/x[2])
model.eval.mmre = mean(model.eval.mre)
model.eval.pred15 = length(model.eval.mre[model.eval.mre<=0.15])/length(model.eval.mre)
model.eval.pred25 = length(model.eval.mre[model.eval.mre<=0.25])/length(model.eval.mre)
model.eval.pred50 = length(model.eval.mre[model.eval.mre<=0.50])/length(model.eval.mre)
model.eval.mdmre = median(model.eval.mre)
model.eval.mae = sum(apply(model.eval.predict, 1, function(x) abs(x[1] - x[2])))/length(model.eval.predict)
model.eval.pred <- c()
for(k in 1:predRange){
model.eval.pred <- c(model.eval.pred, length(model.eval.mre[model.eval.mre<=0.01*k])/length(model.eval.mre))
}
foldResults[i,] <- cbin(foldResults[i,], c(
model.eval.mmre,model.eval.pred15,model.eval.pred25,model.eval.pred50, model.eval.mdmre, model.eval.mae
))
foldResults1[,,i] = cbind(foldResults1[,,i], model.eval.pred);
}
}
cvResults <- lappy(foldResults, mean);
names(cvResults) <- model_accuracy_indice
avgPreds <- matrix(nrow=predRange,ncol=nmodels+1)
colnames(avgPreds) <- c("Pred",modelNames)
for(i in 1:predRange)
{
avgPreds[i,] <- c(i)
for(j in modelNames){
model_fold_mean = mean(foldResults1[i,j,]);
}
avgPreds[i,] <- cbind(avgPreds[i,], model_fold_mean)
}
bootstrappingSE(dataset, models)
ret <-list(cvResults = cvResults, avgPreds = avgPreds)
}
bootstrappingSE <- function(dataset, models){
#bootstrapping the sample and run the run the output sample testing.
}
source("transaction_based_model.R")
install.packages("mvtnorm")
