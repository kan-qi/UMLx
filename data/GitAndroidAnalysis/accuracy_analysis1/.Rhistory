colnames(chain) <- c(names(priorB), paste(names(priorB), "sigma", sep="_"), "normFactor", "sd")
chain[1, "normFactor"] <- normFactor
chain[1, names(priorB)] <- priorB
chain[1, "sd"] <- 10
sigma <- c(0.1)
if(length(priorB)>1){
for (i in 2:length(priorB)) {
sigma <- c(sigma, 1/5* (abs(priorB[i] - priorB[i - 1]))+0.1)
}
}
chain[1, paste(names(priorB), "sigma", sep="_")] = sigma
#probabs <- c()
#acceptance <- c()
for (i in 1:N){
proposal = proposalfunction(chain[i,names(priorB)], chain[i,"normFactor"], chain[i, "sd"])
#proposal = sample
`%ni%` <- Negate(`%in%`)
update <- posterior(proposal, priorB, varianceMatrix, normFactor, var, regressionData[ , !(colnames(regressionData) %in% c("Effort"))], regressionData[,c("Effort")])
postP <- posterior(chain[i,], priorB, varianceMatrix, normFactor, var, regressionData[ , !(colnames(regressionData) %in% c("Effort"))], regressionData[,c("Effort")])
#probabs = c(probabs, probab)
#print(probab)
#the better way of calculating the acceptance rate
probab = exp(update - postP)
if (runif(1) < probab){
chain[i+1,] = proposal
#print("accept")
#acceptance = c(acceptance, "accept")
}else{
chain[i+1,] = chain[i,]
#print("not accept")
#acceptance = c(acceptance, "not accept")
}
}
return(chain)
}
run_metropolis_MCMC <- function(regressionData, N, priorB, varianceMatrix, normFactor, var){
#regressionData <- regressionData
#N <- 10000
#priorB <- means
#varianceMatrix <- covar
#normFactor <- normFactor['mean']
#var <- normFactor['var']
chain = matrix(nrow=N+1, ncol=2*length(priorB)+2)
#print(paste(names(priorB), "sigma", sep="_"))
colnames(chain) <- c(names(priorB), paste(names(priorB), "sigma", sep="_"), "normFactor", "sd")
chain[1, "normFactor"] <- normFactor
chain[1, names(priorB)] <- priorB
chain[1, "sd"] <- 10
sigma <- c(0.1)
if(length(priorB)>1){
for (i in 2:length(priorB)) {
sigma <- c(sigma, 1/5* (abs(priorB[i] - priorB[i - 1]))+0.1)
}
}
chain[1, paste(names(priorB), "sigma", sep="_")] = sigma
#probabs <- c()
#acceptance <- c()
for (i in 1:N){
proposal = proposalfunction(chain[i,names(priorB)], chain[i,"normFactor"], chain[i, "sd"])
#proposal = sample
`%ni%` <- Negate(`%in%`)
update <- posterior(proposal, priorB, varianceMatrix, normFactor, var, regressionData[ , !(colnames(regressionData) %in% c("Effort"))], regressionData[,c("Effort")])
postP <- posterior(chain[i,], priorB, varianceMatrix, normFactor, var, regressionData[ , !(colnames(regressionData) %in% c("Effort"))], regressionData[,c("Effort")])
probab = min(c(1, exp(update + proposalProbability(chain[i,], proposal) - postP - proposalProbability(proposal, chain[i, ]))))
#probabs = c(probabs, probab)
#print(probab)
#the better way of calculating the acceptance rate
if (runif(1) < probab){
chain[i+1,] = proposal
#print("accept")
#acceptance = c(acceptance, "accept")
}else{
chain[i+1,] = chain[i,]
#print("not accept")
#acceptance = c(acceptance, "not accept")
}
}
return(chain)
}
bayesfit3<-function(regressionData, N, B, varianceMatrix, normFactor, var){
#regressionData <- regressionData
#N <- 10000
#B <- means
#varianceMatrix <- covar
#normFactor <- normFactor['mean']
#var <- normFactor['var']
#startvalue = c(4,0,10)
#chain = run_metropolis_MCMC(regressionData, N, B, varianceMatrix, normFactor, var)
chain = run_metropolis_MCMC(regressionData, N, B, varianceMatrix, normFactor, var)
burnIn = 5000
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
ret <- data.frame(chain[-(1:burnIn), names(B)], chain[-(1:burnIn),"normFactor"], chain[-(1:burnIn),"sd"])
colnames(ret) <- c(names(B), "normFactor", "sd")
#par(mfrow = c(2,3))
#hist(chain[-(1:burnIn),1],nclass=30, , main="Posterior of a", xlab="True value = red line" )
#abline(v = mean(chain[-(1:burnIn),1]))
#hist(chain[-(1:burnIn),2],nclass=30, main="Posterior of b", xlab="True value = red line")
#abline(v = mean(chain[-(1:burnIn),2]))
#hist(chain[-(1:burnIn),3],nclass=30, main="Posterior of sd", xlab="True value = red line")
#abline(v = mean(chain[-(1:burnIn),3]) )
#plot(chain[-(1:burnIn),1], type = "l", xlab="True value = red line" , main = "Chain values of a", )
#plot(chain[-(1:burnIn),2], type = "l", xlab="True value = red line" , main = "Chain values of b", )
#plot(chain[-(1:burnIn),3], type = "l", xlab="True value = red line" , main = "Chain values of sd", )
#print(acceptance)
return(ret)
}
bayesfit2<-function(regressionData, N, normFactor) {
# Function to compute the bayesian analog of the lmfit using Gaussian
# priors and Monte Carlo scheme based on N samples. Adapted from:
# https://www.r-bloggers.com/bayesian-linear-regression-analysis-without-tears-r/
# 6/14/18.
# The solution for the posterior distribution of Bayesian linear regression
# with Gaussian likelihood and Gaussian prior:
# N ~ (w | Wn, Vn)
# Wn = Vn(V0)^-1w0 + (1/sigma^2)VnX'y
# Vn = sigma^2(sigma^2(V0)^-1 + X'X)^-1
#
# Reference: Murphy, Kevin. Machine Learning: A Probabilistic Perspective.
# Cambridge: The MIT Press, 2012. Print. Section 7.6.1.
#
# Args:
#   lmfit: a lm object created from lmfit()
#   N: the number of data points to use for Monte Carlo method
#
# Returns:
#   A dataframe containing results of the Bayes line fit.
df.residual<-lmfit$df.residual
s2<-(t(lmfit$residuals)%*%lmfit$residuals)
s2<-s2[1,1]/df.residual
means <- genMeans(lmfit$rank)
covar <- genVariance(means, 1)
## now to sample residual variance
#sigma<-df.residual*s2/rchisq(N,df.residual)
#coef.sim<-sapply(sigma, function(x) {
#  Vn <- calcVn(x, covar, lmfit)
#  Wn <- calcWn(Vn, x, means, covar, lmfit)
#  mvrnorm(1,Wn,Vn)
#})
for(i in 1:N){
scaleFactor <- rnorm(normFactor$mean, normFactor$variance)
priorWeights <- mvrnorm(1, means, covar)
scaledPriorWeights <- scaleFactor * priorWeights
res <- sapply(regressionData, function(datapoint){
res <- datapoint$Effort - scalePriorWeights %*% datapoint[, -c("Effort")]
})
results <- list()
results$res <- res
results$scaleFactor <- scaleFactor
results$priorWeights <- priorWeigths
}
library(plyr)
counts <- ddply(df, .(df$y, df$m), nrow)
names(counts) <- c("y", "m", "Freq")
if (is.vector(coef.sim)) {
ret <- data.frame(coef.sim)
}
else {
ret <- data.frame(t(coef.sim))
}
names(ret)<-names(lmfit$coef)
ret$sigma<-sqrt(sigma)
ret
}
Bayes.sum<-function(x) {
# Provides a summary for a variable of a Bayesian linear regression.
#
# Args:
#   x: a column of the data frame returned by the bayesfit() function
#
# Returns:
#   A vector containing the summary
c("mean"=mean(x),
"se"=sd(x),
"t"=mean(x)/sd(x),
"median"=median(x),
"CrI"=quantile(x,prob=0.025),
"CrI"=quantile(x,prob=0.975)
)
}
predict.blm <- function(model, newdata) {
# predict.lm() analogue for Bayesian linear regression
#
# Args:
#   model: a bayes linear regression model
#   newdata: new data to perform prediction
#
# Returns:
#   Vector of new predictions
#newdata <- subset(newdata, !colnames(newdata) %in% c("Effort"))
#newdata = testData
#model = bayesianModel
#print(mean(model[, col]))
`%ni%` <- Negate(`%in%`)
newdata <- subset(newdata,select = colnames(newdata) %ni% c("Effort"))
ret <- apply(newdata, 1, function(x) {
effort <- 0
for (col in colnames(newdata)) {
effort <- effort + (mean(model[, col]) * x[col])
}
effort
})
ret*mean(model[,"normFactor"])
}
calNormFactor <- function(regressionData){
#n <- length(levels)
#if(n == 1){
#  return(1)
#}
nominalWeights <- genMeans(ncol(regressionData)-1)
nominalWeights <- as.matrix(nominalWeights)
#print(nominalWeights)
transactionData <- as.matrix(regressionData[, !(colnames(regressionData) %in% c("Effort"))])
transactionSum <-  transactionData %*% nominalWeights
transactionRegressionData <- matrix(nrow = nrow(regressionData), ncol=2)
colnames(transactionRegressionData) <- c("transactionSum", "Effort")
rownames(transactionRegressionData) <- rownames(regressionData)
transactionRegressionData[, "transactionSum"] = transactionSum
transactionRegressionData[, "Effort"] = regressionData[, "Effort"]
summary <- summary(lm(Effort ~ . - 1, as.data.frame(transactionRegressionData)))
normFactor <- c(mean = summary$coefficients["transactionSum","Estimate"], var=summary$coefficients["transactionSum","Std. Error"]^2)
#regressionData[, "Effort"]/normFactor
}
cachedTransactionFiles = list()
readTransactionData <- function(filePath){
if (!file.exists(filePath)) {
print("file doesn't exist")
if(is.null(cachedTransactionFiles[[filePath]])){
cachedTransactionFiles[[filePath]] <<- data.frame(TL = numeric(),
TD = numeric(),
DETs = numeric())
}
cachedTransactionFiles[[filePath]]
}
else if(!is.null(cachedTransactionFiles[[filePath]])){
cachedTransactionFiles[[filePath]]
}
else {
#filePath = "..\\..\\577 Projects\\12-22\\F13a_City_of_LosAngeles_Public_Safety_Applicant_Resource_Center_2018-11-22@1545546245122_analysis\\filteredTransactionEvaluation.csv"
fileData <- read.csv(filePath)
if(nrow(fileData) == 0){
fileData <- data.frame(TL = numeric(),
TD = numeric(),
DETs = numeric())
}
else{
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
}
fileData <- na.omit(fileData)
cachedTransactionFiles[[filePath]] <<- fileData
fileData
}
}
loadTransactionData <- function(modelData){
#projects <- rownames(effortData)
#combinedData <- combineData(transactionFiles)
#modelData$Project <- as.character(modelData$Project)
#rownames(modelData) <- modelData$Project
modelData$transaction_file <- as.character(modelData$transaction_file)
effort <- subset(modelData, select=c("Effort"))
projects <- rownames(modelData)
rownames(effort) <- projects
#print(projects)
transactionFileList <- subset(modelData, select=c("transaction_file"))
rownames(transactionFileList) <- projects
numOfTrans <- 0
transactionFiles <- list()
for (project in projects) {
filePath <- transactionFileList[project, "transaction_file"]
fileData <- readTransactionData(filePath)
transactionFiles[[project]] <- fileData
numOfTrans = numOfTrans + nrow(fileData)
}
print(numOfTrans)
combined <- combineData(transactionFiles)
#dataSet[["combined"]] <- combined
#dataSet[["transactionFiles"]] <- transactionFiles
transactionData = list(combined=combined, transactionFiles = transactionFiles, effort = effort, projects=projects)
}
performSearch <- function(n, dataset, parameters = c("TL", "TD", "DETs"), k = 5) {
# Performs search for the optimal number of bins and weights to apply to each
# bin through linear regression.
#
# Args:
#   n: Specifies up to how many bins per parameter to search.
#   folder: Folder containg all the transaction analytics data to analyze.
#   effortData: a data frame containing effort data corresponding to each of
#               the files contained in the folder argument. Rows must be named
#               the same as the filename and effort column should be named "Effort".
#   parameters: A vector of which parameters to analyze. Ex. "TL", "TD", "DETs". When the parameters is an empty array, just apply linear regression on number of transactions.
#   k: How many folds to use for k-fold cross validation.
#
# Returns:
#   A list in which the ith index gives the results of the search for i bins.
#n = 1
#dataset = modelData
#parameters = c("TL", "TD", "DETs")
#k = 5
#effortData = effort
#transactionFiles = transactionFiles
#combinedData <- combined
#i = 6
#load transaction data from the datasheet
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles = transactionData$transactionFiles
projects <- names(transactionData$transactionFiles)
distParams = list();
distParams[['TL']] = list(shape=6.543586, rate=1.160249);
distParams[['TD']] = list(shape=3.6492150, rate=0.6985361);
distParams[['DETs']] = list(shape=1.6647412, rate=0.1691911);
paramAvg <- if (length(parameters) == 1) mean(combinedData[, parameters]) else colMeans(combinedData[, parameters])
paramSD <- if (length(parameters) == 1) sd(combinedData[, parameters]) else apply(combinedData[, parameters], 2, sd)
if(length(parameters) == 0){
n = 1
}
searchResults <- list()
for (i in seq(1,n)) {
cutPoints <- matrix(NA, nrow = length(parameters), ncol = i + 1)
rownames(cutPoints) <- parameters
for (p in parameters) {
#cutPoints[p, ] <- discretize(combinedData[, p], i)
cutPoints[p, ] <- discretize(distParams[[p]][['shape']], distParams[[p]][['rate']], i)
}
#numFiles <- sum(grepl(".csv", dir(folder), ignore.case = TRUE))
levels <- genColNames(length(parameters), i)
#print(levels)
#generate classified regression data
regressionData <- generateRegressionData(projects, cutPoints, effortData, transactionFiles)
# the variance is actually not used.
normFactor <- calNormFactor(regressionData)
#print(normFactor)
means <- genMeans(length(levels))
covar <- genVariance(means, 1/3)
#regressionData <- regressionData[rownames(regressionData) != "Aggregate", ]
#the bayesian model fit
paramVals <- bayesfit3(regressionData, 10000, means, covar, normFactor['mean'], normFactor['var'])
bayesianModel = list()
bayesianModel$weights = subset(paramVals, select = levels)
bayesianModel$normFactor = paramVals[,"normFactor"]
bayesianModel$sd = paramVals[,"sd"]
bayesianModel$cuts <- cutPoints
#apply cross validation to understand the out-of-sample estimation accuracy
validationResults <- crossValidate(regressionData, k)
#the regression model fit
regressionModel <- lm(Effort ~ . - 1, as.data.frame(regressionData));
#print(regressionModel)
validationResults1 <- crossValidate1(regressionData, k)
#calculate the bayesian regression data
nominalWeights <- as.matrix(means)
#print(nominalWeights)
transactionData1 <- as.matrix(regressionData[, !(colnames(regressionData) %in% c("Effort"))])
transactionSum1 <-  transactionData1 %*% nominalWeights
transactionregressionData <- matrix(nrow = nrow(regressionData), ncol=2)
colnames(transactionregressionData) <- c("transactionSum", "Effort")
rownames(transactionregressionData) <- rownames(regressionData)
transactionregressionData[, "transactionSum"] = transactionSum1
transactionregressionData[, "Effort"] = regressionData[, "Effort"]
transactionregressionData <- as.data.frame(transactionregressionData)
#the prior model fit
priorModel <- lm(Effort ~ . - 1, transactionregressionData)
#print(priorModel)
validationResults2 <- crossValidate2(transactionregressionData, k)
#validationResults <- crossValidate(regressionData, k, means, covar, normFactor['mean'], normFactor['var'])
#print(cutPoints)
#print("cross validation")
searchResults[[i]] <- list(
MSE = validationResults["MSE"],
MMRE = validationResults["MMRE"],
PRED = validationResults["PRED"],
bayesModel = bayesianModel,
priorModel = priorModel,
regressionModel = regressionModel,
priorModelAccuracyMeasure = validationResults2,
regressionModelAccuracyMeasure = validationResults1,
#modelAvg = lapply(bayesianModel, mean),
regressionData = regressionData
)
}
searchResults
}
#effort <- read.csv("modelEvaluations_8_12.csv")
#rownames(effort) <- effort$Project
#SWTIresults <- performSearch(3, effort, c("TL"))
estimateEffortWithTrainedModel <- function(trainedModel, cuts, testData){
#trainedModelParameters <- readRDS(file="train_model_parameters.rds")
transactionData <- loadTransactionData(testData)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles = transactionData$transactionFiles
projects <- names(transactionData$transactionFiles)
#cuts = model$m$cuts
regressionData <- generateRegressionData(projects, cuts, effortData, transactionFiles)
trainedModel = t(as.matrix(trainedModel))
predicted <- predict.blm(trainedModel, newdata = regressionData)
predicted*trainedModel[,"normFactor"]
}
generateRegressionData <- function(projects, cutPoints, effortData, transactionFiles){
nParams =  nrow(cutPoints)
nBins =   ncol(cutPoints)-1
levels = genColNames(nParams, nBins)
regressionData <- matrix(nrow = length(projects), ncol = length(levels) + 1)
rownames(regressionData) <- projects
colnames(regressionData) <- c(levels, "Effort")
for (project in projects) {
fileData <- transactionFiles[[project]]
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
regressionData <- na.omit(regressionData)
regressionData <- as.data.frame(regressionData)
}
m_fit.tt1 <- function(swtiii,dataset){
print("swtiii model training")
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- names(transactionData$transactionFiles)
regressionData <- generateRegressionData(projects, swtiii$cuts, effortData, transactionFiles)
normFactor <- calNormFactor(regressionData)
levels = ncol(regressionData) - 1
means <- genMeans(levels)
covar <- genVariance(means, 1)
paramVals <- bayesfit3(regressionData, 100000, means, covar, normFactor['mean'], normFactor['var'])
bayesianModel = list()
bayesianModel$paramVals <- paramVals
bayesianModel$cuts <- swtiii$cuts
swtiii$cuts = NULL;
swtiii$m = bayesianModel;
swtiii
}
# for model testing
m_predict.tt1 <- function(swtiii, testData){
print("swtiii predict function")
#using the means for each esimulation results as the final estimates of the parameters
swtiii_model <- apply(swtiii$m$paramVals, 2, mean)
estimateEffortWithTrainedModel(swtiii_model, swtiii$m$cuts, testData)
}
trainsaction_based_model <- function(analysisResults, modelSelector){
# for model training
modelParams = analysisResults[[modelSelector]][["bayesModel"]]
swtiiiParams = list(
cuts = modelParams$cuts
)
}
benchmarkResults <- modelBenchmark(models, modelData)
#options(show.error.locations = TRUE)
#options(error=function()traceback(2))
#initialize models for training, testing, and evaluation
models = list()
#create an initialized model by selecting a candidate model
SWTIIIModelSelector <- 1
models$tt1 = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
size_models <- size_metric_models()
models = append(models, size_models)
benchmarkResults <- modelBenchmark(models, modelData)
View(benchmarkResults)
View(benchmarkResults)
print(benchmarkResults)
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
require(MASS)
modelData <- selectData("dsets/modelEvaluations-1-3.csv")
rownames(modelData) <- modelData$Project
modelData$Project <- NULL
models = list()
#first perform model search for the swtiii parameters
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#create an initialized model by selecting a candidate model
SWTIIIModelSelector <- 1
models$tt1 = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
#initialize the size metric based models
size_models <- size_metric_models()
models = append(models, size_models)
View(models)
View(models)
benchmarkResults <- modelBenchmark(models, modelData)
print(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
avgPreds <- benchmarkResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
