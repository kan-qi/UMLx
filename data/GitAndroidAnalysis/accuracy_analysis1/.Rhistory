direction <- '-'
}
if (d==0){
direction <- '='
}
dfHypothesis[i,] <- c(model_labels[id1], model_labels[id2], metric_labels[id1], direction)
dfPValue[i,1:3] <- c(mu_1, mu_2, p)
i <- i+1
}
}
}
}
to_pair <- seq(from = j , to = j+nmodels*nmetrics ,by = nmetrics)
print(nmetrics)
nmodels <- length(model_names)
nmetrics <- length(metric_names)
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
metric_names = accuracy_metrics
nmodels <- length(model_names)
if(nmodels < 2){
print("At least two models should be provided for hypothesis test.")
return
}
nmetrics <- length(metric_names)
nhypo <- choose(nmodels,2) * nmetrics
dfHypothesis <- matrix(nrow=nhypo,ncol=4)
colnames(dfHypothesis) <- c('model1','model2','metric','direction')
dfPValue <- matrix(nrow=nhypo,ncol=5)
colnames(dfPValue) <- c('model1_mean','model2_mean','p_value','BH_p_value','bonferroni_p_value')
model_metric_name <- names(iterationResults)
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
singleHypoTest <- function(x,y){
is_var_equal <- function(x,y){
var_test = var.test(x,y)
if (z$p.value > 0.05){
return(TRUE)
}
else {
return(FALSE)
}
}
var_euqal <- is_var_equal(x,y)
t_test <- t.test(x,y,var.equal = var_euqal)
p <- t_test$p.value
return(p)
}
# for each accuracy metric, pair two models to construct hypothesis and run t test
# total number of hypothesis = C(nmodels, 2) * nmetrics
i <- 1
while (i<=nhypo){
for (j in 1:nmetrics){
to_pair <- seq(from = j , to = j+nmodels*nmetrics ,by = nmetrics)
#print(to_pair)
for (k in 1:(nmodels-1)){
for (l in (k+1):nmodels){
id1 <- to_pair[k]
id2 <- to_pair[l]
#print(id1)
#print(id2)
if (is.na(id2)){
break
}
else {
x <- iterationResults[,id1]
y <- iterationResults[,id2]
p <- singleHypoTest(x,y)
mu_1 <- mean(x)
mu_2 <- mean(y)
d <- mu_1 - mu_2
if (d>0){
direction <- '+'
}
if (d<0){
direction <- '-'
}
if (d==0){
direction <- '='
}
dfHypothesis[i,] <- c(model_labels[id1], model_labels[id2], metric_labels[id1], direction)
dfPValue[i,1:3] <- c(mu_1, mu_2, p)
i <- i+1
}
}
}
}
}
singleHypoTest <- function(x,y){
is_var_equal <- function(x,y){
var_test = var.test(x,y)
if (var_test$p.value > 0.05){
return(TRUE)
}
else {
return(FALSE)
}
}
var_euqal <- is_var_equal(x,y)
t_test <- t.test(x,y,var.equal = var_euqal)
p <- t_test$p.value
return(p)
i <- 1
while (i<=nhypo){
for (j in 1:nmetrics){
to_pair <- seq(from = j , to = j+nmodels*nmetrics ,by = nmetrics)
#print(to_pair)
for (k in 1:(nmodels-1)){
for (l in (k+1):nmodels){
id1 <- to_pair[k]
id2 <- to_pair[l]
#print(id1)
#print(id2)
if (is.na(id2)){
break
}
else {
x <- iterationResults[,id1]
y <- iterationResults[,id2]
p <- singleHypoTest(x,y)
mu_1 <- mean(x)
mu_2 <- mean(y)
d <- mu_1 - mu_2
if (d>0){
direction <- '+'
}
if (d<0){
direction <- '-'
}
if (d==0){
direction <- '='
}
dfHypothesis[i,] <- c(model_labels[id1], model_labels[id2], metric_labels[id1], direction)
dfPValue[i,1:3] <- c(mu_1, mu_2, p)
i <- i+1
}
}
}
}
}
dfPValue[,4] <- p.adjust(dfPValue[,'p_value'], method='BH')
dfPValue[,5] <- p.adjust(dfPValue[,'p_value'], method='bonferroni')
#ret <- list(dfHypothesis=dfHypothesis, dfPValue=dfPValue)
sig_df <- data.frame(model1=dfHypothesis[,'model1'],
model2=dfHypothesis[,'model2'],
metric=dfHypothesis[,'metric'],
direction=dfHypothesis[,'direction'],
model1_mean=dfPValue[,'model1_mean'],
model2_mean=dfPValue[,'model2_mean'],
p_value=dfPValue[,'p_value'],
BH_p_value=dfPValue[,'BH_p_value'],
bonferroni_p_value=dfPValue[,'bonferroni_p_value'])
return(sig_df)
familywiseHypoTest <- function(iterationResults, metric_names, model_names){
#Args:
#iterationResults: iteration results from CV or bootstrapping, a matrix of 54 columns (9 models * 6 metrics)
#        if more models or metrics added, update: model_metric_name, model_name, metric_name
#Returns:
#sig_df: dataframe of 9 columns: model1, model2, metric, direction, model1_mean, model2_mean, p_value, BH_p_value, bonferroni_p_value
#        metric: mmre, pred15, pred25, pred50, mdmre, mae
#        direction: +/-/=: model1_mean is greater than/less than/equal to model2_mean on accuracy value measured by metric
#        p_value: p value from t t_test
#        BH_p_value: adjusted p value using BH method
#        bonferroni_p_value: adjusted p value using Bonferroni method
#iterationResults = iterResults
nmodels <- length(model_names)
if(nmodels < 2){
print("At least two models should be provided for hypothesis test.")
return
}
nmetrics <- length(metric_names)
nhypo <- choose(nmodels,2) * nmetrics
dfHypothesis <- matrix(nrow=nhypo,ncol=4)
colnames(dfHypothesis) <- c('model1','model2','metric','direction')
dfPValue <- matrix(nrow=nhypo,ncol=5)
colnames(dfPValue) <- c('model1_mean','model2_mean','p_value','BH_p_value','bonferroni_p_value')
model_metric_name <- names(iterationResults)
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
singleHypoTest <- function(x,y){
is_var_equal <- function(x,y){
var_test = var.test(x,y)
if (var_test$p.value > 0.05){
return(TRUE)
}
else {
return(FALSE)
}
}
var_euqal <- is_var_equal(x,y)
t_test <- t.test(x,y,var.equal = var_euqal)
p <- t_test$p.value
return(p)
}
# for each accuracy metric, pair two models to construct hypothesis and run t test
# total number of hypothesis = C(nmodels, 2) * nmetrics
i <- 1
while (i<=nhypo){
for (j in 1:nmetrics){
to_pair <- seq(from = j , to = j+nmodels*nmetrics ,by = nmetrics)
#print(to_pair)
for (k in 1:(nmodels-1)){
for (l in (k+1):nmodels){
id1 <- to_pair[k]
id2 <- to_pair[l]
#print(id1)
#print(id2)
if (is.na(id2)){
break
}
else {
x <- iterationResults[,id1]
y <- iterationResults[,id2]
p <- singleHypoTest(x,y)
mu_1 <- mean(x)
mu_2 <- mean(y)
d <- mu_1 - mu_2
if (d>0){
direction <- '+'
}
if (d<0){
direction <- '-'
}
if (d==0){
direction <- '='
}
dfHypothesis[i,] <- c(model_labels[id1], model_labels[id2], metric_labels[id1], direction)
dfPValue[i,1:3] <- c(mu_1, mu_2, p)
i <- i+1
}
}
}
}
}
dfPValue[,4] <- p.adjust(dfPValue[,'p_value'], method='BH')
dfPValue[,5] <- p.adjust(dfPValue[,'p_value'], method='bonferroni')
#ret <- list(dfHypothesis=dfHypothesis, dfPValue=dfPValue)
sig_df <- data.frame(model1=dfHypothesis[,'model1'],
model2=dfHypothesis[,'model2'],
metric=dfHypothesis[,'metric'],
direction=dfHypothesis[,'direction'],
model1_mean=dfPValue[,'model1_mean'],
model2_mean=dfPValue[,'model2_mean'],
p_value=dfPValue[,'p_value'],
BH_p_value=dfPValue[,'BH_p_value'],
bonferroni_p_value=dfPValue[,'bonferroni_p_value'])
return(sig_df)
}
sig_cv <- familywiseHypoTest(iterationResults=foldResults, accuracy_metrics, model_names)
View(sig_bs)
View(sig_cv)
View(sig_cv)
View(sig_df)
#source("transaction_weights_calibration4.R")
#adding two additional models: sloc and ln_sloc models.
mre <- function(x) abs(x[1] - x[2])/x[2]
mmre <- function(mre) mean(mre)
pred15 <- function(mre) length(mre[mre<=0.15])/length(mre)
pred25 <- function(mre) length(mre[mre<=0.15])/length(mre)
pred50 <- function(mre) length(mre[mre<=0.50])/length(mre)
mdmre <- function(mre) median(mre)
mae<- function(x) sum(apply(x, 1, mre))/length(x)
predR <- function(mre, predRange) {
eval_pred <- c()
for(k in 1:predRange){
eval_pred <- c(eval_pred, length(mre[mre<=0.01*k])/length(mre))
}
eval_pred
}
#modelBenchmark would preform both cross validation and bootrapping significance test
modelBenchmark <- function(models, dataset){
accuracy_metrics <- c('mmre','pred15','pred25','pred50', "mdmre", "mae")
cvResults <- cv(models, dataset, accuracy_metrics)
bsResults <- bootstrappingSE(models, dataset, accuracy_metrics)
ret <-list(cvResults = cvResults,
bsResults = bsResults,
model_names = names(models),
accuracy_metrics = accuracy_metrics
)
}
cv <- function(models, dataset, accuracy_metrics){
#dataset = modelData
nfold = 2
folds <- cut(seq(1,nrow(dataset)),breaks=nfold,labels=FALSE)
modelNames = names(models)
nmodels <- length(modelNames)
nmetrics <- length(accuracy_metrics)
predRange <- 50
#data structure to hold the data for 10 fold cross validation
model_accuracy_indice <- c()
for(i in 1:length(modelNames)){
modelName = modelNames[i]
model_accuracy_indice <- cbind(model_accuracy_indice, paste(modelName, accuracy_metrics, sep="_"));
}
foldResults <- matrix(nrow=nfold,ncol=nmodels*nmetrics)
colnames(foldResults) <- model_accuracy_indice
foldResults1 <- array(0,dim=c(predRange,nmodels,nfold))
#Perform 10 fold cross validation
for(i in 1:nfold){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- dataset[testIndexes, ]
trainData <- dataset[-testIndexes, ]
eval_metrics = c()
eval_pred = c()
for(j in 1:nmodels){
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
predicted = m_predict(model, testData)
#print(predicted)
actual = testData$Effort
names(actual) <- rownames(testData)
#print(actual)
intersectNames <- intersect(names(predicted), names(actual))
model_eval_predict = data.frame(predicted = predicted[intersectNames],actual=actual[intersectNames] )
eval_metric_results = list()
model_eval_mre = apply(model_eval_predict, 1, mre)
#print(modelName)
model_eval_mre <- na.omit(model_eval_mre)
#print(model_eval_mre)
model_eval_mmre = mmre(model_eval_mre)
model_eval_pred15 = pred15(model_eval_mre)
model_eval_pred25 = pred25(model_eval_mre)
model_eval_pred50 = pred50(model_eval_mre)
model_eval_mdmre = mdmre(model_eval_mre)
model_eval_mae = sum(model_eval_mre)/length(model_eval_predict)
eval_metrics <- c(
eval_metrics, model_eval_mmre,model_eval_pred15,model_eval_pred25,model_eval_pred50, model_eval_mdmre, model_eval_mae
)
#print(eval_metrics)
eval_pred = c(eval_pred, predR(model_eval_mre, predRange))
}
foldResults[i,] = eval_metrics
foldResults1[,,i] = eval_pred
}
#print(foldResults)
accuracyResults <- apply(foldResults, 2, mean);
names(accuracyResults) <- model_accuracy_indice
#print(cvResults)
avgPreds <- matrix(nrow=predRange,ncol=nmodels+1)
colnames(avgPreds) <- c("Pred",modelNames)
for(i in 1:predRange)
{
avgPreds[i,] <- c(i, rep(0, length(modelNames)))
for(j in 1:length(modelNames)){
model_fold_mean = mean(foldResults1[i,j,]);
avgPreds[i,j+1] <- model_fold_mean
}
}
ret <-list(accuracyResults = accuracyResults, avgPreds = avgPreds, foldResults = foldResults, accuracy_metrics = accuracy_metrics)
}
bootstrappingSE <- function(models, dataset, accuracy_metrics){
#bootstrapping the sample and run the run the output sample testing.
#bootstrappingSE <- function(SWTIIIModelData, otherSizeMetricsData, model, niters, confidence_interval){
set.seed(42)
# create 10000 samples of size 50
N <- nrow(dataset)
#niters <- 10
sample_size <- 50
niters <- 10000
confidence_interval <- 0.83
nfold = 2
folds <- cut(seq(1,nrow(dataset)),breaks=nfold,labels=FALSE)
modelNames = names(models)
nmodels <- length(modelNames)
accuracy_metrics <- c('mmre','pred15','pred25','pred50', "mdmre", "mae")
nmetrics <- length(accuracy_metrics)
predRange <- 50
#data structure to hold the data for 10 fold cross validation
model_accuracy_indice <- c()
for(i in 1:length(modelNames)){
modelName = modelNames[i]
model_accuracy_indice <- cbind(model_accuracy_indice, paste(modelName, accuracy_metrics, sep="_"));
}
iterResults <- matrix(nrow=niters, ncol=nmodels*nmetrics)
colnames(iterResults) <- model_accuracy_indice
iterResults1 <- array(0,dim=c(predRange,nmodels,niters))
for (i in 1:niters){
sampleIndexes <- sample(1:N, size=sample_size)
# train:test = 40:10
trainIndexes <- sample(sampleIndexes, size=40)
trainData <- dataset[trainIndexes, ]
testData <- dataset[-trainIndexes, ]
eval_metrics = c()
eval_pred = c()
for(j in 1:nmodels){
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
predicted = m_predict(model, testData)
#print(predicted)
actual = testData$Effort
names(actual) <- rownames(testData)
#print(actual)
intersectNames <- intersect(names(predicted), names(actual))
model_eval_predict = data.frame(predicted = predicted[intersectNames],actual=actual[intersectNames] )
eval_metric_results = list()
model_eval_mre = apply(model_eval_predict, 1, mre)
#print(modelName)
model_eval_mre <- na.omit(model_eval_mre)
#print(model_eval_mre)
model_eval_mmre = mmre(model_eval_mre)
model_eval_pred15 = pred15(model_eval_mre)
model_eval_pred25 = pred25(model_eval_mre)
model_eval_pred50 = pred50(model_eval_mre)
model_eval_mdmre = mdmre(model_eval_mre)
model_eval_mae = sum(model_eval_mre)/length(model_eval_predict)
eval_metrics <- c(
eval_metrics, model_eval_mmre,model_eval_pred15,model_eval_pred25,model_eval_pred50, model_eval_mdmre, model_eval_mae
)
#print(eval_metrics)
eval_pred = c(eval_pred, predR(model_eval_mre, predRange))
}
if (i%%500 == 0){
print(i)
}
iterResults[i,] = eval_metrics
iterResults1[,,i] = eval_pred
}
#confidence_interval <- 0.83
t <- confidence_interval/2
# estimatied value falls in [mean(x) - t * se, mean(m) + t * se]
calEstimation <- function(x){
return(c(mean(x)-t*sd(x), mean(x), mean(x)+t*sd(x)))
}
bsEstimations <- apply(iterResults, 2, calEstimation)  # 3*54 matrix
colnames(bsEstimations) <- model_accuracy_indice
rownames(bsEstimations) <- c('lower','mean','upper')
ret <- list(bsEstimations = bsEstimations, iterResults = iterResults, accuracy_metrics = accuracy_metrics)
}
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
require(MASS)
modelData <- selectData("dsets/modelEvaluations-1-3.csv")
rownames(modelData) <- modelData$Project
modelData$Project <- NULL
models = list()
#first perform model search for the swtiii parameters
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#intialize the model with hyper parameters (cutpoints) decided by cross validatoin results for different ways of binning
SWTIIIModelSelector <- 1
models$tm1 = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
#initialize the size metric based models
size_models <- size_metric_models()
models = append(models, size_models)
models = list()
#first perform model search for the swtiii hypter parameters, for example, the cut points.
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#intialize the model with hyper parameters (cutpoints) decided by cross validatoin results for different ways of binning
SWTIIIModelSelector <- 1
models$tm1 = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
#initialize the size metric based models
size_models <- size_metric_models()
models = append(models, size_models)
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
require(MASS)
modelData <- selectData("dsets/modelEvaluations-1-3.csv")
rownames(modelData) <- modelData$Project
modelData$Project <- NULL
models = list()
#first perform model search for the swtiii hypter parameters, for example, the cut points.
SWTIIIresults <- performSearch(1, modelData, c("TL", "TD", "DETs"))
#intialize the model with hyper parameters (cutpoints) decided by cross validatoin results for different ways of binning
SWTIIIModelSelector <- 1
models$tm1 = trainsaction_based_model(SWTIIIresults, SWTIIIModelSelector)
#initialize the size metric based models
size_models <- size_metric_models()
models = append(models, size_models)
