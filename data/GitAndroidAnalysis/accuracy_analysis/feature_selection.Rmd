---
title: "Report"
author: "Kan Qi"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading data form the csv file

```{r}
data <- read.csv('./modelEvaluations-1-3.csv',stringsAsFactors= T)
summary(data)
```

## Preprocessing the data
Select the list of variables for study
Replacing all the NaN with the mean value. 

```{r}
#print(as.vector(t(colnames(data))))
cols <- c(
    "Tran_Num"  
  , "UseCase_Num"                              
  , "KSLOC"     
  , "Personnel"   
  , "Priori_COCOMO_Estimate"                   
  , "COCOMO_Estimate"                          
 , "Activity_Num"                             
 , "Actor_Num" 
 , "Avg_Actor" 
 , "Boundary_Num"                             
 , "ControlNum"
 , "Entity_Num"
 , "Component_num"                            
 , "Attribute_num"                            
 , "Operation_num"                            
 , "class_num" 
 , "Top_Level_Classes"                        
 , "Average_Depth_Inheritance_Tree"           
 , "Average_Number_Of_Children_Per_Base_Class"
 , "Number_Of_Inheritance_Relationships"      
 , "Depth_Inheritance_Tree"                   
 , "para_num"  
 , "usage_num" 
 , "real_num"  
 , "assoc_num" 
 , "externaloper_num"                         
 , "objectdata_num"                           
 , "avg_operation"                            
 , "avg_attribute"                            
 , "avg_parameter"                            
 , "avg_usage" 
 , "avg_real"  
 , "avg_assoc" 
 , "avg_instVar"                              
 , "weightedoper_num"                         
 , "method_size"                              
 , "EI"        
 , "EQ"        
 , "INT"       
 , "DM"        
 , "CTRL"      
 , "EXTIVK"    
 , "EXTCLL"    
 , "TRAN_NA"   
 , "NT"        
 , "Avg_TL"    
 , "Avg_TD"    
 , "Arch_Diff" 
 , "Type.1"    
 , "Effort_Norm"                              
 , "Norm_Factor"                              
 , "Effort"    
 , "UAW"       
 , "TCF"       
 , "EF"        
 , "Simple_UC" 
 , "Average_UC"
 , "Complex_UC"
 , "Effort_Norm_UCP"                          
 , "Effort_Norm_COCOMO"                       
 , "UUCP"      
 , "UCP"       
 , "EUCP"      
 , "EXUCP"     
 , "DUCP"      
 , "SWTI"      
 , "SWTII"     
 , "SWTIII"    
 , "NOET"      
 , "NOAAE"     
 , "NORT"      
 , "NEM"       
 , "NSR"       
 , "NOA"       
 , "NOS"       
 , "WMC"       
 , "MPC"       
 , "NOCH"      
 , "DIT"       
 , "CBO"       
 , "NIVPC"     
 , "NUMS"      
 , "NCI"       
 , "NCIF"      
 , "RR"        
 , "NTLC"      
 , "ANWMC"     
 , "ADIT"      
 , "NOCPBC"    
 , "EIF"       
 , "ILF"       
 , "IFPUG"     
 , "NI"        
 , "NE"        
 , "MKII"      
 , "EXT"       
, "ERY"       
, "RED"       
, "WRT"       
, "COSMIC"    
, "DET"       
, "FTR"       
, "NT.1"      
, "NOC"       
, "NOA.1"     
, "NOA.2"     
, "NOUC"      
, "NOR"       
, "ANAPUC"    
, "ANRPUC"    
, "UCP.1"     
, "NOC.1"     
, "NOIR"      
, "NOUR"      
, "NORR"      
, "NOM"       
, "NOP"       
, "NOCAL"     
, "NOASSOC"   
, "ANMC"      
, "ANPC"      
, "ANCAC"     
, "ANASSOCC"  
, "ANRELC"    
, "NOC.2"     
, "NOAPC"     
, "NODET"     
, "NORET"     
, "NOA.3"     
, "NOMPC"     
, "NPPM"      
, "NMT"       
, "Num_User_Story"                           
, "Num_Tasks" 
, "project_manager_estimate"                 
, "developer_estimate"
          )
ncol <- length(cols)

data[cols] <- lapply(data[cols], function(x) ifelse(is.na(x), ave(x, FUN = function(y) mean(y, na.rm = TRUE)), x))


```

## Preparing the independent variables
1. Removing all the variables with zero value for all the observations. 
2. Facoterizing the type variable
3. Calculating the corelation between all the independent and dependent variables.
4. Choosing all the variables with highest corelation values. 

Calculating the correlation and choosing the independent variables with correlation higher than 0.6 with the dependent variable (Effort).

``` {r}
# rank the variables by the correlation.
r_limit <- 0.5
x <- data[cols]
x <- subset(x, select = colnames(x) != "Effort")
y <- data[c("Effort")]
corr <- cor(x, y)
#corr <- corr[!is.na(corr)]
corr <- na.omit(corr)
#corr <- subset(corr, !is.na(corr[c("Effort")]))
print(row.names(corr))
plot(corr)
text(1:length(corr),corr,row.names(corr),cex=0.4, pos=4, col="blue")
abline(h=r_limit,col="red")
abline(h=-1*r_limit,col="red")

corr <- as.data.frame(corr)
corr <- subset(corr, corr["Effort"] >= r_limit)
ind_variables <- row.names(corr)

# Rank the variables by other criteria
library(BayesFactor)

stats <- data.frame(matrix(nrow = 0, ncol = 5))
colnames(stats) <- c("var", "r2", "AIC", "BIC", "BF")
features <- cols[cols != "Effort"]
for (var in features[2:47]) {
  regression_formula <- as.formula(paste("Effort ~ ", var))
  model <- lm(regression_formula, data = data)
  r2 = summary(model)$r.squared
  aic = AIC(model)
  bic = BIC(model)
  bf = lmBF(regression_formula, data = data)
  stats[nrow(stats) + 1,] <- list(var, r2, aic, bic, bf)
}

#R2
r2_data <- stats[, c("var", "r2")]
r2_data.sort <- r2_data[order(-r2_data$r2), ]
r2_data.sort
r2_d <- r2_data[,c("r2")]
names(r2_d) <- r2_data$var
plot(r2_d)
text(1:length(r2_d),r2_d,r2_data$var,cex=0.4, pos=4, col="blue")
abline(h=r_limit,col="red")
abline(h=-1*r_limit,col="red")

#AIC
aic_data <- stats[, c("var", "AIC")]
aic_data.sort <- aic_data[order(aic_data$AIC), ]
aic_data.sort
aic_d <- aic_data[, c("AIC")]
plot(aic_d)
text(1:length(aic_d),aic_d,aic_data$var,cex=0.4, pos=4, col="blue")
abline(h=r_limit,col="red")
abline(h=-1*r_limit,col="red")

#BIC
bic_data <- stats[, c("var", "BIC")]
bic_data.sort <- bic_data[order(bic_data$BIC), ]
bic_data.sort
bic_d <- bic_data[, c("BIC")]
plot(bic_d)
text(1:length(bic_d),bic_d,bic_data$var,cex=0.4, pos=4, col="blue")
abline(h=r_limit,col="red")
abline(h=-1*r_limit,col="red")

#Bayes Factor
bf_data <- stats[, c("var", "BF")]
bf_data.sort <- bf_data[order(-bf_data$BF), ]
bf_data.sort
bf_d <- bf_data[, c("BF")]
plot(bf_d)
text(1:length(bf_d),bf_d,bf_data$var,cex=0.4, pos=4, col="blue")
abline(h=r_limit,col="red")
abline(h=-1*r_limit,col="red")
```

Looking at the graph, following are the most correlated independent variables:  
1. Effort_Norm_UCP
2. UseCase_Num
3. CTRL
4. NT
5. Complex_UC
6. UEUCW
7. TCF
8. EUCP
9. EXUCP
10. DUCP
11. Top_Level_Classes
12. Number_Of_Inheritance_Relationships
13. Number_Of_Derived_Classes
14. Number_Of_Classes_Inherited
15. Number_Of_Classes_Inherited_From
16. Number_Of_Children
17. Depth_Inheritance_Tree
18. Coupling_Between_Objects
19. FUNC_NA
20. UAW
  

## Model Fitting

Using all the above variables except UseCase_NUM and Diagram_Num for fitting the model.  

```{r}
vars <- data[ind_variables]
vars <- cbind(vars, data[c("Effort")])

fit <- lm(Effort ~ .,data=vars)
summary(fit)
plot(fit)
```


```{r}
library(glmnet)
library(plotmo) # for plot_glmnet
x_data <- data.matrix(data[ind_variables])
y_data <- data.matrix(data[c("Effort")])
lasso_lm <- glmnet(x = x_data, y = y_data, alpha = 1, standardize = T)
print(lasso_lm$lambda)
plot(lasso_lm)
 #for 10 biggest final features
plot_glmnet(lasso_lm)                             # default colors
#plot_glmnet(lasso_lm, label=10)
```

```{r}
Lasso_range = function(x, y, k){
  # inputs:
      # x_matrix, a matrix containing independent variables
      # y: vector of dependent varaibles
      # k: the length of sequence
  # output:
      # seq: a sequence of lambdaa from high to low
  
  
  # define my own scale function to simulate that in glmnet
  myscale = function(x) sqrt(sum((x - mean(x)) ^ 2) / length(x))
  
  # normalize x
  sx = as.matrix(scale(x, scale = apply(x, 2, myscale)))
  # sy = as.vector(scale(y, scale = myscale(y)))
  max_lambda = max(abs(colSums(sx * as.vector(y)))) / dim(sx)[1]
  # The default depends on the sample size nobs relative to the number of variables nvars. 
  # If nobs > nvars, the default is 0.0001, close to zero. 
  
  # If nobs < nvars, the default is 0.01. 
  # A very small value of lambda.min.ratio will lead to a saturated fit in the nobs < nvars case. 
  ratio = 0
  if(dim(sx)[1] > dim(sx)[2]){
    ratio = 0.0001
  }else{
    ratio = 0.01
  }
  min_lambda = max_lambda * ratio
  log_seq = seq(from  = log(min_lambda), to = log(max_lambda), length.out = k)
  seq = sort(exp(log_seq), decreasing = T)
  return(seq)
}


Lasso_range(x_data,y_data, 100)

```

```{r}
set.seed(2)
lambda_list <- Lasso_range(x_data,y_data,100)
percent = 50
cvfit = cv.glmnet(x_data,y_data,
                  standardize = T, type.measure = 'mse', nfolds = 5, alpha = 1)
# # 5 fold cross validation
 k <- 5
# 
# function to calculate MMRE
calcMMRE <- function(testData,pred){
  mmre <- abs(testData - pred)/testData
  mean_value <- mean(mmre)
  mean_value
}
# # function to calculate PRED
calcPRED <- function(testData,pred,percent){
  value <- abs(testData - pred)/testData
  percent_value <- percent/100
  pred_value <- value <= percent_value
  mean(pred_value)
}
# 
 folds <- cut(seq(1,nrow(x_data)),breaks=k,labels=FALSE)
 mean_mmre <- vector("list",k)
 mean_pred <- vector("list",k)
 overall_mean_mmre <- vector("list",100)
 overall_mean_pred <- vector("list",100)
 for(iterator in seq(1,100)){
   for(i in 1:k){
     testIndexes <- which(folds==i,arr.ind=TRUE)
     testData <- y_data[testIndexes]
     pred <- predict(cvfit,newx=x_data,s=lambda_list[[iterator]])
     mean_mmre[[i]] <- calcMMRE(testData,pred[testIndexes])
     mean_pred[[i]] <- calcPRED(testData,pred[testIndexes],percent)
 }
 overall_mean_mmre[[iterator]] <- mean(as.numeric(mean_mmre))
 overall_mean_pred[[iterator]] <- mean(as.numeric(mean_pred))
 }
```

```{r}
plot(log(lambda_list),overall_mean_mmre,xlab="log(Lambda)",ylab="MMRE")
lines(log(lambda_list),overall_mean_mmre,xlim=range(log(lambda_list)), ylim=range(overall_mean_mmre), pch=16)

plot(log(lambda_list),overall_mean_pred,xlab="log(Lambda)",ylab = "PRED")
lines(log(lambda_list),overall_mean_pred,xlim=range(log(lambda_list)), ylim=range(overall_mean_pred), pch=16)

plot(cvfit)
```
## Feature ranking
```{r}

```