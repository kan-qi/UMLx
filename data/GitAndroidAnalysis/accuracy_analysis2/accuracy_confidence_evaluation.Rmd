---
title: "Transaction Weight Calibration Visualized"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)

```
Combined Data Effort Values
```{r descriptive statistics, fig.width=5,fig.height=2.5}
#The previous dataset
#modelData <- selectData("dsets/modelEvaluations-1-3.csv")
#The current dataset
#modelData <- selectData("dsets/android_dataset_6_4.csv")
#modelData <- selectData("dsets/android_dataset_transactions_5_19.csv")
#modelData <- selectData("../android_analysis_datasets/android_dataset_6_20_1.csv", selector=c("size", "large"))

modelData <- selectData("../android_analysis_datasets/android_dataset_6_29.csv")

#modelData <- selectData("../android_analysis_datasets/android_dataset_6_20_1.csv", selector=c("initialDate", "late"))

#modelData <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("type", "Tools"))

#data set exploratory statistics, eg, correlation statistics, marginal distributions, etc.
```

#load the comparative models. Each model for comparison should include two functions for evaluateion:
#1. m_fit(params, trainSet)
# params: a list of hyper parameters
# trainSet: the training dataset
#2. m_predict(model, testSet)
# model: the trained model
# testSet: the testing Set

# After create the two functions, register your model into the "models" list, as shown beblow, with a list of hyper-parameters indexed with the model name.

```{r swtiii, warning = FALSE}

#initialize models for training, testing, and evaluation. The models are put into a list with model names referencing a list of hyper-parameters, which will be passed to the model training function.

models = list()
#register the model into the models list with the hyper parameters returned from  the "trainsaction_based_model" function
#transaction_models <- trainsaction_based_model(modelData)
#models = append(models, transaction_models)

models$tm3 <- trainsaction_based_model3(modelData)

#initialize the size metric based models
size_models <- size_metric_models(modelData)
#register the list of the size metric based models. 
models = append(models, size_models)

#intialize the step-wise learning model
##models$step_lnr <- stepwise_linear_model(modelData)

#initialize the neuralnet model
##models$neuralnet = neuralnet_model(modelData)

#initialize the lasso regression model. having problems, need to debug
##models$lasso = lasso_model(modelData)

#initialize the regression tree model
##models$reg_tree = regression_tree_model(modelData)

#load the machine learning based models
#to create a model, following 3 steps:
# 1.create the model training function by rewriting this following template function
#  m_fit.MODEL_NAME <- function(MODEL_NAME,dataset){}
# 2.create the prediction function by rewriting this following template function
#  m_predict.MODEL_NAME <- function(MODEL_NAME, testData){}
# 3.add your model name into the "models"(above) variable for referencing
#  models.MODEL_NAME <- list(hyper-params...)
# examples can be found in the size_metric_based_models.R or transaction_based_model.R
  
```

```{r model attributes, warning = FALSE}
#using the entire dataset to train the models and evaluate some of the properties of the trained parameters.
#currentModels = models
#models = currentModels

fitEval = evalFit(models, modelData, c("R2", "eta-squared"))

trainedModels <-modelTrain(models, modelData)
outputData <- profileData(trainedModels, modelData)

sizeMetricLabels <- c('SWTI', 'SWTII', 'SWTIII', 'UUCP', 'AFP', 'COSMIC', 'SLOC', "LOG_SLOC")
sizeMeasures <- data.frame(matrix(nrow=nrow(outputData), ncol=length(sizeMetricLabels)))
rownames(sizeMeasures) <- rownames(outputData)
colnames(sizeMeasures) <- sizeMetricLabels
outputData$LOG_SLOC <- log(outputData$SLOC)
#sizeMeasures$SWTI <- outputData$SWTI
#sizeMeasures$SWTII <- outputData$SWTII
#sizeMeasures$SWTIII <- outputData$SWTIII
#sizeMeasures$UUCP <- outputData$UUCP
#sizeMeasures$AFP <- outputData$AFP
#sizeMeasures$COSMIC <- outputData$COSMIC
#sizeMeasures$SLOC <- outputData$SLOC
#sizeMeasures$LOG_SLOC <- log(outputData$SLOC)
sizeMeasures[,sizeMetricLabels] <- outputData[,sizeMetricLabels]
sizeMeasures <- na.omit(sizeMeasures)
effortData <- outputData[rownames(sizeMeasures), 'Effort']

#run the correlation analysis of the size metrics
sizeCor <- cor(sizeMeasures)
print(round(sizeCor, digits=2))
  
#run the correlation analysis of size metrics and project effort
effortCor <- apply(sizeMeasures, 2, function(x) {
    #print(x)
    d <- cbind(x, effortData)
    #print(d)
    corT <- cor.test(d$x, d$Effort)
    #corT$p.value
    c(corT$estimate, corT$p.value)
  })
effortCor <- t(effortCor)
colnames(effortCor) <- c("r", "p-value")
print(round(effortCor, 2))

```
comparison between the candidate models: SWTIII, UCP, COCOMO, a-priori COCOMO, using cross-validation and bootstrapping
```{r modelPlot, warning = FALSE, fig.width=5,fig.height=4}

benchmarkResults <- modelBenchmark(models, modelData)

model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")

print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
				ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")

print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() + 
		geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
		stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
		ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")


print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() + 
		geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
		scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
		stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
		ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")

###plot for the bootstrapping results
bsRet <- benchmarkResults$bsResults
#bootstrappingSE(SWTIIIModelData, otherSizeMetricsData, model3, 10000, 0.83)
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]

#save as csv
#write.csv(bsEstimations, file='bsEstimations.csv', quote=F, row.names = F)
#write.csv(iterResults, file='iterResults.csv', quote=F, row.names = F)

#read from csv
#bsEstimations <- read.csv('bsEstimations.csv')
#rownames(bsEstimations) <- c('lower','mean','upper')
#iterResults <- read.csv('iterResults.csv')

# plot bootstrapping results

model_labels <- c()
for(i in 1:length(models)){
  for(j in 1:length(accuracy_metrics)){
    model_labels = c(model_labels, names(models)[i])
  }
}

metric_labels <- c()
for(i in 1:length(models)){
  for(j in 1:length(accuracy_metrics)){
    metric_labels = c(metric_labels, accuracy_metrics[j])
  }
}

df <- data.frame(t(bsEstimations))
df$labels <- rownames(df)
df$model_labels <- model_labels
df$metric_labels <- metric_labels

print(metric_labels)


for (i in 1:length(accuracy_metrics)){
    g = metric_labels[i]
    g_label <- toupper(g)
    selectedData <- df[df$metric_labels == g,]
    p <- ggplot(selectedData, aes(x = labels, y = mean)) + 
    geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
    geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
    xlab('MODEL') +
    ylab(g_label) +
    scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
    ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
    print(p)
}


# for (i in 1:length(metric_labels)){
#     g = metric_labels[i]
#     selectedData <- df[df$metric_labels == g,]
#     p <- ggplot(selectedData, aes(x = labels, y = mean, ymin = lower, ymax = upper, fill = metric_labels)) +
#     geom_crossbar(width = 0.5, position = "dodge") +
#     #coord_flip() +
#     scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
#     xlab('model') +
#     ylab(g) +
#     ggtitle(g)
#     print(p)
# }

#rank the cv results of different metric

cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels

cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")

for (i in 1:length(accuracy_metrics)){
  g = metric_labels[i]
  selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
  selectedData <- selectedData[,-3]#delete the metric_labels
  names(selectedData)<-c(g, "model_labels")
  if(g == "mmre" || g == "mdmre" || g == "mae"){
    selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
  }else{
    selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
  }
  
  cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}

#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
  selectedData <- cvRankResults[i,]
  for(j in 1:length(accuracy_metrics)){
    rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
  }
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum

cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]

#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]

print(round(cvRankResults,2))

# family-wise hypothesis test
source('familywiseHypoTest2.R')
foldResults <- cvResults$foldResults
# sig_cv <- familywiseHypoTest(iterationResults=foldResults, accuracy_metrics, model_names)

# head(sig_cv)

sig_bs <- familywiseHypoTest(iterationResults=iterResults, accuracy_metrics, model_names, "boot")


round_df <- function(df, digits) {
  nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))

  df[,nums] <- round(df[,nums], digits = digits)

  (df)
}
sig_bs_f = sig_bs[which(sig_bs$BH_p_value < 0.05),]
sig_bs_f = sig_bs_f[which(sig_bs_f$metric %in% c("mmre", "mdmre", "pred25", "mae")),]
sig_bs_f = sig_bs_f[which(sig_bs_f$model1 %in% c("tm3") | sig_bs_f$model2 %in% c("tm3")),]
sig_bs_f = sig_bs_f[order(sig_bs_f$bonferroni_p_value),]
sig_bs_f = round_df(sig_bs_f, 2)
print(sig_bs_f)


# Using the "sig_bs" results, create a graph to represent the direct graph for the models.
library(igraph)
for(metric_i in 1:length(accuracy_metrics)){
  selectedData <- sig_bs[sig_bs$metric == metric_labels[metric_i],]
  m <- matrix(0, nrow = length(models), ncol = length(models), byrow = FALSE)
  colnames(m) <- names(models)
  rownames(m) <- names(models)
  for(i in 1:nrow(selectedData)){
    if(selectedData$BH_p_value[i] < 0.05){
    if(selectedData$direction[i] == "+"){
      m[as.character(selectedData$model1[i]), as.character(selectedData$model2[i])] = 1
    }else if(selectedData$direction[i] == "-"){
      m[as.character(selectedData$model2[i]), as.character(selectedData$model1[i])] = 1
    }
    }
  }
  # if A -> B -> C, remove edge A -> C
  for(i in 1:length(models)){
    for(j in 1:length(models)){
      if(m[i,j] == 1){
        for(k in 1:length(models)){
          if(m[j,k] == 1)
            m[i,k] = 0
        }
      }
    }
  }
  #plot the directed graph
  net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
  plot.igraph(net,vertex.label=V(net)$name, layout=layout.fruchterman.reingold, 
              vertex.color="white", vertex.label.color="black", vertex.size=25, 
              edge.color="black",edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
  title(main = metric_labels[metric_i])
}


# filter for 

```

```{r stratified analysis, warning = FALSE}
projectCategories <- as.matrix(read.csv("./project_categorization.csv"))
print(projectCategories)
nCategories <- nrow(projectCategories)
benchmarkLabels <- paste("b", seq(1, nCategories, by=1), sep="")
categorizedBenchmarkResults = list()

#i=7
for(i in 1:nCategories){
  #print(projectCategories[i, ])
  categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=projectCategories[i,])

  #categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("initialDate","early"))
  
  categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("type","Productivity"))
  
  print(categorizedDataset)
  categorizedBenchmarkResults[[benchmarkLabels[i]]] <- modelBenchmark(models, categorizedDataset)
}

```