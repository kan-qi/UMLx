#npoints <- nrow(model_eval_fit)
#grouping <- c(1:npoints, 1:npoints)
#anova <- aov( c(model_eval_fit$predicted, model_eval_fit$actual) ~ grouping )
#eta_squared <- etaSquared(anova)[1,1]
#print(eta_squared)
if("R2" %in% fit_metrics){
meanActual <- mean(model_eval_fit$actual)
eval_metric_results[[modelName]]$r_squared <- 1-sum((model_eval_fit$actual - model_eval_fit$predicted)^2)/sum((model_eval_fit$actual - meanActual)^2)
#print(r_squared)
}
#f-test
if("f-test" %in% fit_metrics){
eval_metric_results[[modelName]]$f_test = var.test(model_eval_fit$actual - model_eval_fit$predicted, model_eval_fit$actual - meanActual)
}
#LM <- lm( c(model_eval_fit$predicted, model_eval_fit$actual) ~ grouping );
#r_squared <- summary(LM)$r.squared
#print(r_squared)
#add the evaluation results to the "eval_metric_results"
#eval_metric_results[[modelName]] = list(eta_squared = eta_squared,
#                                        r_squared = r_squared,
#                                        model_eval_fit = model_eval_fit,
#                                        f_test = f_test)
}
eval_metric_results
}
cv <- function(models, dataset, accuracy_metrics){
#dataset = modelData
nfold = 2
folds <- cut(seq(1,nrow(dataset)),breaks=nfold,labels=FALSE)
modelNames = names(models)
nmodels <- length(modelNames)
nmetrics <- length(accuracy_metrics)
predRange <- 50
#data structure to hold the data for 10 fold cross validation
model_accuracy_indice <- c()
for(i in 1:length(modelNames)){
modelName = modelNames[i]
model_accuracy_indice <- cbind(model_accuracy_indice, paste(modelName, accuracy_metrics, sep="_"));
}
foldResults <- matrix(nrow=nfold,ncol=nmodels*nmetrics)
colnames(foldResults) <- model_accuracy_indice
foldResults1 <- array(0,dim=c(predRange,nmodels,nfold))
#Perform 10 fold cross validation
for(i in 1:nfold){
print("iter:")
print(i)
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- dataset[testIndexes, ]
trainData <- dataset[-testIndexes, ]
eval_metrics = c()
eval_pred = c()
print(i)
for(j in 1:nmodels){
#j = 2
modelName <- modelNames[j]
print(modelName)
model = fit(trainData, modelNames[j], models[[j]])
predicted = as.vector(m_predict(model, testData))
names(predicted) <- rownames(testData)
print(predicted)
actual = testData$Effort
names(actual) <- rownames(testData)
print(actual)
intersectNames <- intersect(names(predicted), names(actual))
model_eval_predict = data.frame(predicted = predicted[intersectNames],actual=actual[intersectNames])
#print(model_eval_predict)
eval_metric_results = list()
model_eval_mre = apply(model_eval_predict, 1, mre)
print("mre")
print(model_eval_mre)
model_eval_mre <- na.omit(model_eval_mre)
#print(model_eval_mre)
if("mmre" %in% accuracy_metrics){
foldResults[i, paste(modelName,"mmre", sep="_")] = mmre(model_eval_mre)
}
if("pred15" %in% accuracy_metrics){
foldResults[i, paste(modelName, "pred15", sep="_")] = pred15(model_eval_mre)
}
if("pred25" %in% accuracy_metrics){
foldResults[i, paste(modelName, "pred25", sep="_")] = pred25(model_eval_mre)
}
if("pred50" %in% accuracy_metrics){
foldResults[i, paste(modelName, "pred50", sep="_")] = pred50(model_eval_mre)
}
if("mdmre" %in% accuracy_metrics){
foldResults[i, paste(modelName, "mdmre", sep="_")] = mdmre(model_eval_mre)
}
if("mae" %in% accuracy_metrics){
foldResults[i, paste(modelName, "mae", sep="_")] = sum(model_eval_mre)/length(model_eval_predict)
}
#eval_metrics <- c(
#  eval_metrics, model_eval_mmre,model_eval_pred15,model_eval_pred25,model_eval_pred50, model_eval_mdmre, model_eval_mae
#)
#print(eval_metrics)
if("predRange" %in% accuracy_metrics){
foldResults1[, j, i] = predR(model_eval_mre, predRange)
}
}
#foldResults[i,] = eval_metrics
#foldResults1[,,i] = eval_pred
}
#print(foldResults)
accuracyResults <- apply(foldResults, 2, mean);
names(accuracyResults) <- model_accuracy_indice
#print(cvResults)
avgPreds <- matrix(nrow=predRange,ncol=nmodels+1)
colnames(avgPreds) <- c("Pred",modelNames)
for(i in 1:predRange)
{
avgPreds[i,] <- c(i, rep(0, length(modelNames)))
for(j in 1:length(modelNames)){
model_fold_mean = mean(foldResults1[i,j,]);
avgPreds[i,j+1] <- model_fold_mean
}
}
ret <-list(accuracyResults = accuracyResults, avgPreds = avgPreds, foldResults = foldResults, foldResults1 = foldResults1)
}
bootstrappingSE <- function(models, dataset, accuracy_metrics){
#bootstrapping the sample and run the run the output sample testing.
#bootstrappingSE <- function(SWTIIIModelData, otherSizeMetricsData, model, niters, confidence_interval){
set.seed(42)
# create 10000 samples of size 50
N <- nrow(dataset)
#niters <- 10
#sample_size <- as.integer(0.83*N)
sample_size <- N
niters <- 100
confidence_interval <- 0.83
nfold = 2
folds <- cut(seq(1,nrow(dataset)),breaks=nfold,labels=FALSE)
modelNames = names(models)
nmodels <- length(modelNames)
#accuracy_metrics <- c('mmre','pred15','pred25','pred50', 'mdmre', 'mae')
nmetrics <- length(accuracy_metrics)
predRange <- 50
#data structure to hold the data for 10 fold cross validation
model_accuracy_indice <- c()
for(i in 1:length(modelNames)){
modelName = modelNames[i]
model_accuracy_indice <- cbind(model_accuracy_indice, paste(modelName, accuracy_metrics, sep="_"));
}
iterResults <- matrix(nrow=niters, ncol=nmodels*nmetrics)
colnames(iterResults) <- model_accuracy_indice
iterResults1 <- array(0,dim=c(predRange,nmodels,niters+1))
for (i in 1:niters){
if(i == 1){
resample = dataset
}
else{
resampleIndexes <- sample(1:N, size=sample_size, replace=TRUE)
resample = dataset[resampleIndexes,]
}
#sampleIndexes <- sample(1:N, size=sample_size)
# train:test = 40:10
train_data_size = as.integer(0.8*sample_size)
trainIndexes <- sample(1:N, size=train_data_size)
trainData <- resample[trainIndexes, ]
testData <- resample[-trainIndexes, ]
eval_metrics = c()
eval_pred = c()
for(j in 1:nmodels){
modelName <- modelNames[j]
model = fit(trainData, modelNames[j], models[[j]])
predicted = m_predict(model, testData)
#print(predicted)
names(predicted) <- rownames(testData)
print(predicted)
actual = testData$Effort
names(actual) <- rownames(testData)
#print(actual)
intersectNames <- intersect(names(predicted), names(actual))
model_eval_predict = data.frame(predicted = predicted[intersectNames],actual=actual[intersectNames] )
print(model_eval_predict)
eval_metric_results = list()
model_eval_mre = apply(model_eval_predict, 1, mre)
#print(modelName)
model_eval_mre <- na.omit(model_eval_mre)
#print(model_eval_mre)
if("mmre" %in% accuracy_metrics){
iterResults[i, paste(modelName,"mmre", sep="_")] = mmre(model_eval_mre)
}
if("pred15" %in% accuracy_metrics){
iterResults[i, paste(modelName, "pred15", sep="_")] = pred15(model_eval_mre)
}
if("pred25" %in% accuracy_metrics){
iterResults[i, paste(modelName, "pred25", sep="_")] = pred25(model_eval_mre)
}
if("pred50" %in% accuracy_metrics){
iterResults[i, paste(modelName, "pred50", sep="_")] = pred50(model_eval_mre)
}
if("mdmre" %in% accuracy_metrics){
iterResults[i, paste(modelName, "mdmre", sep="_")] = mdmre(model_eval_mre)
}
if("mae" %in% accuracy_metrics){
iterResults[i, paste(modelName, "mae", sep="_")] = sum(model_eval_mre)/length(model_eval_predict)
}
#eval_metrics <- c(
#  eval_metrics, model_eval_mmre,model_eval_pred15,model_eval_pred25,model_eval_pred50, model_eval_mdmre, model_eval_mae
#)
#print(eval_metrics)
if("predRange" %in% accuracy_metrics){
iterResults1[,j,i] = predR(model_eval_mre, predRange)
}
}
if (i%%500 == 0){
print(i)
}
#iterResults[i,] = eval_metrics
#iterResults1[,,i] = eval_pred
}
#confidence_interval <- 0.83
t <- confidence_interval/2
# estimatied value falls in [mean(x) - t * se, mean(m) + t * se]
calEstimation <- function(x){
return(c(mean(x)-t*sd(x), mean(x), mean(x)+t*sd(x)))
}
bsEstimations <- apply(iterResults, 2, calEstimation)  # 3*54 matrix
colnames(bsEstimations) <- model_accuracy_indice
rownames(bsEstimations) <- c('lower','mean','upper')
ret <- list(bsEstimations = bsEstimations, iterResults = iterResults, iterResults1=iterResults1)
}
benchmarkResults <- modelBenchmark(models, modelData)
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_labels
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- metric_labels
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
print(round(cvRankResults,2))
#evaluate bootstrap results
bsRet <- benchmarkResults$bsResults
#bootstrappingSE(SWTIIIModelData, otherSizeMetricsData, model3, 10000, 0.83)
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
#save as csv
#write.csv(bsEstimations, file='bsEstimations.csv', quote=F, row.names = F)
#write.csv(iterResults, file='iterResults.csv', quote=F, row.names = F)
#read from csv
#bsEstimations <- read.csv('bsEstimations.csv')
#rownames(bsEstimations) <- c('lower','mean','upper')
#iterResults <- read.csv('iterResults.csv')
# plot bootstrapping results
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
df <- data.frame(t(bsEstimations))
df$labels <- rownames(df)
df$model_labels <- model_labels
df$metric_labels <- metric_labels
print(metric_labels)
# calculate non-overlapping intervals.
nonOverlappingPairs <- list()
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
ii <- 1
nonOverlap <- list()
for (j in 1:(nrow(selectedData)-1)){
for (k in (j+1):nrow(selectedData)){
if(selectedData[j,]$lower>selectedData[k,]$upper | selectedData[j,]$upper<selectedData[k,]$lower){
#selectedData[j,] and selectedData[k,] non-overlap
nonOverlap[[ii]] <- c(selectedData[j,]$model_labels, selectedData[k,]$model_labels)
ii <- ii+1
}
}
}
nonOverlappingPairs[[g]] <- nonOverlap
}
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
}
# draw a partially ordered graph based on non-overlapping pairs
# for (i in 1:length(metric_labels)){
#     g = metric_labels[i]
#     selectedData <- df[df$metric_labels == g,]
#     p <- ggplot(selectedData, aes(x = labels, y = mean, ymin = lower, ymax = upper, fill = metric_labels)) +
#     geom_crossbar(width = 0.5, position = "dodge") +
#     #coord_flip() +
#     scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
#     xlab('model') +
#     ylab(g) +
#     ggtitle(g)
#     print(p)
# }
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
install.packages("classInt")
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
install.packages("jsonlite")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
install.packages("reshape")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
install.packages("fitdistrplus")
install.packages("egg")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
