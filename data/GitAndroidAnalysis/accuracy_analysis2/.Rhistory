ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum
print(rank_sum)
View(cvRankResults)
View(cvRankResults)
View(cvRankResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvResults)
View(cvResults)
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
View(cvRankResults)
View(cvRankResults)
View(cvRankResults)
View(cvRankResults)
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
View(selectedData)
View(selectedData)
print(g)
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
View(selectedData)
View(selectedData)
names(selectedData)<-c(g, "model_labels")
names(selectedData)<-c(g, "model_labels", "metric")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
View(cvRankResults)
View(cvRankResults)
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
View(models)
View(models)
View(benchmarkResults)
View(benchmarkResults)
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)
modelData <- selectData("../android_analysis_datasets/android_dataset_7_21.csv")
models = list()
#register the model into the models list with the hyper parameters returned from  the "trainsaction_based_model" function
transaction_models <- trainsaction_based_model(modelData)
load("~/D/ResearchSpace/ResearchProejcts/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/7-22.RData")
View(cachedTransactionFiles)
View(cachedTransactionFiles)
View(models)
View(models)
models = list()
models = append(models, transaction_models)
#register the list of the size metric based models.
models = append(models, size_models)
View(models)
View(models)
models = list()
models = append(models, transaction_models)
View(models)
View(models)
models = list()
View(models)
View(models)
models = list()
View(transaction_models)
View(transaction_models)
View(transaction_models)
models = append(models, transaction_models)
View(models)
View(models)
benchmarkResults <- modelBenchmark(models, modelData)
View(models)
View(models)
load("~/D/ResearchSpace/ResearchProejcts/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/7-23.RData")
View(models)
View(models)
View(benchmarkResults)
View(benchmarkResults)
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels", "metric")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
View(models)
View(models)
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
View(cvRankResults)
View(cvRankResults)
View(cvResults)
View(cvResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
View(cvRankResults)
View(cvRankResults)
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
View(selectedData)
View(selectedData)
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
View(cvRankResults)
View(cvRankResults)
View(cvRankResults)
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvRankResults)
View(cvRankResults)
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
View(selectedData)
View(selectedData)
colnames(selectedData)<-c(g, "model_labels")
g = accuracy_metrics[i]
View(selectedData)
View(selectedData)
View(cvAccuracyResults)
View(cvAccuracyResults)
View(cvResults)
View(cvResults)
