m_predict.step_lnr <- function(step_lnr, testData){
# numeric data only
testData <- testData[, step_lnr$dims]
numeric_columns <- unlist(lapply(testData, is.numeric))
data.numeric <- testData[, numeric_columns]
data.numeric <- data.frame(apply(testData, 2, as.numeric))
#cols_removed = step_lnr$cols_removed
predicted <- predict(step_lnr$m, testData)
}
stepwise_linear_model <- function(modelData, regression_cols=c()){
#modelData <- selectData("dsets/android_dataset_5_15.csv")
#rownames(modelData) <- modelData$Project
#c1<-c1[-c(5,9,18,19,20,21,25,33,36,38,50,51,52,53,54,55,56,60,61,70,75,79,80,81,83,84,85,89,90,91,92,102,103.106,107,112,114,120,124,125,128,129,130,131,134,135,136,137)]
#c_name <- list("Tran_Num","Activity_Num","Component_Num","Precedence_Num","Stimulus_Num","Response_Num")
step_lnr <- list(
regression_cols=regression_cols
)
#str_frm <- gsub("[\r\n]", "", str_frm)
#frm <- as.formula(str_frm)
}
# Preprocess dataset
clean_step <- function(dataset){
# numeric data only
numeric_columns <- unlist(lapply(dataset, is.numeric))
data.numeric <- dataset[, numeric_columns]
data.numeric <- data.frame(apply(dataset, 2, as.numeric))
# remove near zero variance columns
library(caret)
nzv_cols <- nearZeroVar(data.numeric)
if(length(nzv_cols) > 0) data <- data.numeric[, -nzv_cols]
sapply(data, function(x) sum(is.na(x)))
## Impute
library(mice)
library(randomForest)
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- mice::complete(miceMod)
# remove collinear columns
#need to consider the number of columns after colinearity analysis to make sure that the column number is fewer than row number.
descrCorr <- cor(data.imputed)
data.done = data.imputed
#cor_limits <- c(0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1)
cor_limits <- seq(0.99, 0.01, by = -0.02)
for(i in 1:length(cor_limits)){
highCorr <- findCorrelation(descrCorr, cor_limits[i])
data.removed <- data.imputed[, -highCorr]
#coli <- findLinearCombos(data.imputed1)
print("data removed frame")
print(c(ncol = ncol(data.removed), nrow = nrow(data.removed)))
if(is.null(data.removed) || is.null(dim(data.removed)) || dim(data.removed)[2] < 2){
break
}
if(nrow(data.removed) > ncol(data.removed)){
data.done = data.removed
break
}
}
#coli <- findLinearCombos(data.imputed)
#print(coli)
#data.done <- data.imputed[, -coli$remove]
return(data.done)
}
projectCategories <- as.matrix(read.csv("./project_categorization.csv"))
print(projectCategories)
nCategories <- nrow(projectCategories)
benchmarkLabels <- paste("b", seq(1, nCategories, by=1), sep="")
categorizedBenchmarkResults = list()
for(i in 1:nCategories){
print(projectCategories[i, ])
categorizedDataset <- selectData("dsets/combined_data_set_4.csv", selector=projectCategories[i,])
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("initialDate","early"))
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("type","Productivity"))
#print(categorizedDataset)
benchmarkResults <- modelBenchmark(models, categorizedDataset)
categorizedBenchmarkResults[[benchmarkLabels[i]]] <- benchmarkResults
# calculate the rank information
cvResults <- benchmarkResults$cvResults
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
accuracy_metrics <- accuracy_metrics[! accuracy_metrics %in% c("predRange")]
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle("Overall Ranking Result for Cross Validation") +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
p_cvAllRank
}
categorizedDataset <- selectData("dsets/combined_data_set_4.csv", "Game")
View(categorizedDataset)
View(categorizedDataset)
View(categorizedDataset)
print(categorizedDataset$Type)
View(cvRankResults)
print(categorizedDataset$Category)
categorizedDataset <- selectData("dsets/combined_data_set_4.csv", c("type", "Game"))
print(categorizedDataset$Category)
View(categorizedDataset)
View(categorizedDataset)
projectCategories <- as.matrix(read.csv("./project_categorization.csv"))
print(projectCategories)
nCategories <- nrow(projectCategories)
benchmarkLabels <- paste("b", seq(1, nCategories, by=1), sep="")
categorizedBenchmarkResults = list()
for(i in 1:nCategories){
print(projectCategories[i, ])
#categorizedDataset <- selectData("dsets/combined_data_set_4.csv", selector=projectCategories[i,])
categorizedDataset <- selectData("dsets/combined_data_set_4.csv", c("type", "Game"))
print(categorizedDataset$Category)
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("initialDate","early"))
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("type","Productivity"))
#print(categorizedDataset)
benchmarkResults <- modelBenchmark(models, categorizedDataset)
categorizedBenchmarkResults[[benchmarkLabels[i]]] <- benchmarkResults
# calculate the rank information
cvResults <- benchmarkResults$cvResults
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
accuracy_metrics <- accuracy_metrics[! accuracy_metrics %in% c("predRange")]
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle("Overall Ranking Result for Cross Validation") +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
p_cvAllRank
projectCategories <- as.matrix(read.csv("./project_categorization.csv"))
print(projectCategories)
nCategories <- nrow(projectCategories)
benchmarkLabels <- paste("b", seq(1, nCategories, by=1), sep="")
categorizedBenchmarkResults = list()
for(i in 1:nCategories){
print(projectCategories[i, ])
categorizedDataset <- selectData("dsets/combined_data_set_4.csv", selector=projectCategories[i,])
#categorizedDataset <- selectData("dsets/combined_data_set_4.csv", c("type", "Game"))
#print(categorizedDataset$Category)
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("initialDate","early"))
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("type","Productivity"))
#print(categorizedDataset)
benchmarkResults <- modelBenchmark(models, categorizedDataset)
categorizedBenchmarkResults[[benchmarkLabels[i]]] <- benchmarkResults
# calculate the rank information
cvResults <- benchmarkResults$cvResults
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
accuracy_metrics <- accuracy_metrics[! accuracy_metrics %in% c("predRange")]
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle("Overall Ranking Result for Cross Validation") +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
p_cvAllRank
projectCategories <- as.matrix(read.csv("./project_categorization.csv"))
print(projectCategories)
nCategories <- nrow(projectCategories)
benchmarkLabels <- paste("b", seq(1, nCategories, by=1), sep="")
categorizedBenchmarkResults = list()
for(i in 1:nCategories){
print(projectCategories[i, ])
categorizedDataset <- selectData("dsets/combined_data_set_4.csv", selector=projectCategories[i,])
#categorizedDataset <- selectData("dsets/combined_data_set_4.csv", c("type", "Game"))
#print(categorizedDataset$Category)
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("initialDate","early"))
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("type","Productivity"))
#print(categorizedDataset)
benchmarkResults <- modelBenchmark(models, categorizedDataset)
categorizedBenchmarkResults[[benchmarkLabels[i]]] <- benchmarkResults
# calculate the rank information
cvResults <- benchmarkResults$cvResults
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
accuracy_metrics <- accuracy_metrics[! accuracy_metrics %in% c("predRange")]
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle("Overall Ranking Result for Cross Validation") +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
p_cvAllRank
projectCategories <- as.matrix(read.csv("./project_categorization.csv"))
print(projectCategories)
nCategories <- nrow(projectCategories)
benchmarkLabels <- paste("b", seq(1, nCategories, by=1), sep="")
categorizedBenchmarkResults = list()
View(projectCategories)
View(projectCategories)
View(projectCategories)
View(projectCategories)
View(projectCategories)
View(projectCategories)
#define the stepwise linear model
m_fit.step_lnr <- function(step_lnr,dataset){
#step_lnr <- models$step_lnr
#dataset = modelData
cols = step_lnr$regression_cols
dataset <- dataset[, cols]
cleanData <- clean_step(dataset[, colnames(dataset) != "Effort"])
#str_frm <- paste("Effort ~",step_lnr$formula)
dims <- colnames(cleanData)
step_lnr$dims <- dims
str_frm <- paste("Effort ~", paste(dims, collapse="+"))
print(str_frm)
regressionData <- cleanData
regressionData$Effort <- dataset$Effort
frm <- as.formula(str_frm)
#step_m <- lm(frm, data=dataset)
step_m <- lm(frm, data=regressionData)
step_lnr$m <- step_m
tryCatch( { step_lnr$m <- stepAIC(step_m, direction = "both", trace = FALSE) }
, error = function(e) {
print("-infinite AIC")
})
#step_lnr$cols_removed = c()
#print(step_lnr$m)
step_lnr
}
m_predict.step_lnr <- function(step_lnr, testData){
# numeric data only
testData <- testData[, step_lnr$dims]
numeric_columns <- unlist(lapply(testData, is.numeric))
data.numeric <- testData[, numeric_columns]
data.numeric <- data.frame(apply(testData, 2, as.numeric))
#cols_removed = step_lnr$cols_removed
predicted <- predict(step_lnr$m, testData)
}
stepwise_linear_model <- function(modelData, regression_cols=c()){
#modelData <- selectData("dsets/android_dataset_5_15.csv")
#rownames(modelData) <- modelData$Project
#c1<-c1[-c(5,9,18,19,20,21,25,33,36,38,50,51,52,53,54,55,56,60,61,70,75,79,80,81,83,84,85,89,90,91,92,102,103.106,107,112,114,120,124,125,128,129,130,131,134,135,136,137)]
#c_name <- list("Tran_Num","Activity_Num","Component_Num","Precedence_Num","Stimulus_Num","Response_Num")
step_lnr <- list(
regression_cols=regression_cols
)
#str_frm <- gsub("[\r\n]", "", str_frm)
#frm <- as.formula(str_frm)
}
# Preprocess dataset
clean_step <- function(dataset){
# numeric data only
numeric_columns <- unlist(lapply(dataset, is.numeric))
data.numeric <- dataset[, numeric_columns]
data.numeric <- data.frame(apply(dataset, 2, as.numeric))
# remove near zero variance columns
library(caret)
nzv_cols <- nearZeroVar(data.numeric)
if(length(nzv_cols) > 0) data <- data.numeric[, -nzv_cols]
sapply(data, function(x) sum(is.na(x)))
## Impute
library(mice)
library(randomForest)
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- mice::complete(miceMod)
# remove collinear columns
#need to consider the number of columns after colinearity analysis to make sure that the column number is fewer than row number.
descrCorr <- cor(data.imputed)
data.done = data.imputed
#cor_limits <- c(0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1)
cor_limits <- seq(0.99, 0.01, by = -0.02)
for(i in 1:length(cor_limits)){
highCorr <- findCorrelation(descrCorr, cor_limits[i])
data.removed <- data.imputed[, -highCorr]
#coli <- findLinearCombos(data.imputed1)
print("data removed frame")
print(c(ncol = ncol(data.removed), nrow = nrow(data.removed)))
if(is.null(data.removed) || is.null(dim(data.removed)) || dim(data.removed)[2] < 2){
break
}
if(nrow(data.removed) > ncol(data.removed)){
data.done = data.removed
break
}
}
#coli <- findLinearCombos(data.imputed)
#print(coli)
#data.done <- data.imputed[, -coli$remove]
return(data.done)
}
categorizedDataset <- selectData("dsets/combined_data_set_4.csv", selector=projectCategories[i,])
