miyazaki94.lm = lm(KLOC ~ MM, data=miyazaki94_data)
print(summary(miyazaki94.lm)$r.squared)
#evaluate bootstrap results
bsRet <- benchmarkResults$bsResults
#bootstrappingSE(SWTIIIModelData, otherSizeMetricsData, model3, 10000, 0.83)
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
#save as csv
#write.csv(bsEstimations, file='bsEstimations.csv', quote=F, row.names = F)
#write.csv(iterResults, file='iterResults.csv', quote=F, row.names = F)
#read from csv
#bsEstimations <- read.csv('bsEstimations.csv')
#rownames(bsEstimations) <- c('lower','mean','upper')
#iterResults <- read.csv('iterResults.csv')
# plot bootstrapping results
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
df <- data.frame(t(bsEstimations))
df$labels <- rownames(df)
df$model_labels <- model_labels
df$metric_labels <- metric_labels
print(metric_labels)
# calculate non-overlapping intervals.
nonOverlappingPairs <- list()
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
ii <- 1
nonOverlap <- list()
for (j in 1:(nrow(selectedData)-1)){
for (k in (j+1):nrow(selectedData)){
if(selectedData[j,]$lower>selectedData[k,]$upper | selectedData[j,]$upper<selectedData[k,]$lower){
#selectedData[j,] and selectedData[k,] non-overlap
nonOverlap[[ii]] <- c(selectedData[j,]$model_labels, selectedData[k,]$model_labels)
ii <- ii+1
}
}
}
nonOverlappingPairs[[g]] <- nonOverlap
}
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
}
# draw a partially ordered graph based on non-overlapping pairs
# for (i in 1:length(metric_labels)){
#     g = metric_labels[i]
#     selectedData <- df[df$metric_labels == g,]
#     p <- ggplot(selectedData, aes(x = labels, y = mean, ymin = lower, ymax = upper, fill = metric_labels)) +
#     geom_crossbar(width = 0.5, position = "dodge") +
#     #coord_flip() +
#     scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
#     xlab('model') +
#     ylab(g) +
#     ggtitle(g)
#     print(p)
# }
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
print(round(cvRankResults,2))
# draw histogram base on ranking
library(ggplot2)
modelsName = rownames(cvRankResults)
p <- list()
for(i in 1:length(accuracy_metrics)){
g = paste("rank", i, sep = "")
selectedData <- cvRankResults[names(cvRankResults) == g]
names(selectedData) <- c("rank");
p[[i]] <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 1)) +
geom_text(aes(label = rank, vjust = -0.8, hjust = 0.5)) +
#ggtitle(accuracy_metrics[i]) +
labs(caption=toupper(accuracy_metrics[i])) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank())
}
library("cowplot")
prow <- plot_grid( p[[1]] + theme(legend.position="none"),
p[[2]] + theme(legend.position="none"),
p[[3]] + theme(legend.position="none"),
p[[4]] + theme(legend.position="none"),
p[[5]] + theme(legend.position="none"),
p[[6]] + theme(legend.position="none"),
align = 'vh',
hjust = -1,
nrow = 1
)
legend_b <- get_legend(p[[1]] + theme(legend.position="bottom", legend.justification="center"))
title <- ggdraw() + draw_label("Ranking Result for Cross Validation", fontface='bold')
p_cvRank <- plot_grid(title, prow, legend_b, ncol = 1, rel_heights = c(.2 , 1, .1))
p_cvRank
bsRet <- benchmarkResults$bsResults
#bootstrappingSE(SWTIIIModelData, otherSizeMetricsData, model3, 10000, 0.83)
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
#save as csv
#write.csv(bsEstimations, file='bsEstimations.csv', quote=F, row.names = F)
#write.csv(iterResults, file='iterResults.csv', quote=F, row.names = F)
#read from csv
#bsEstimations <- read.csv('bsEstimations.csv')
#rownames(bsEstimations) <- c('lower','mean','upper')
#iterResults <- read.csv('iterResults.csv')
# plot bootstrapping results
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
df <- data.frame(t(bsEstimations))
df$labels <- rownames(df)
df$model_labels <- model_labels
df$metric_labels <- metric_labels
print(metric_labels)
# calculate non-overlapping intervals.
nonOverlappingPairs <- list()
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
ii <- 1
nonOverlap <- list()
for (j in 1:(nrow(selectedData)-1)){
for (k in (j+1):nrow(selectedData)){
if(selectedData[j,]$lower>selectedData[k,]$upper | selectedData[j,]$upper<selectedData[k,]$lower){
#selectedData[j,] and selectedData[k,] non-overlap
nonOverlap[[ii]] <- c(selectedData[j,]$model_labels, selectedData[k,]$model_labels)
ii <- ii+1
}
}
}
nonOverlappingPairs[[g]] <- nonOverlap
}
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
}
# draw a partially ordered graph based on non-overlapping pairs
# for (i in 1:length(metric_labels)){
#     g = metric_labels[i]
#     selectedData <- df[df$metric_labels == g,]
#     p <- ggplot(selectedData, aes(x = labels, y = mean, ymin = lower, ymax = upper, fill = metric_labels)) +
#     geom_crossbar(width = 0.5, position = "dodge") +
#     #coord_flip() +
#     scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
#     xlab('model') +
#     ylab(g) +
#     ggtitle(g)
#     print(p)
# }
View(models)
View(models)
View(models)
View(models)
View(modelData)
View(modelData)
View(modelData)
modelData <- selectData("../android_analysis_datasets/android_dataset_7_21.csv")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)
modelData <- selectData("../android_analysis_datasets/android_dataset_7_21.csv")
View(modelData)
View(modelData)
models = list()
#initialize the size metric based models
size_models <- size_metric_models(modelData)
#register the list of the size metric based models.
models = append(models, size_models)
benchmarkResults <- modelBenchmark(models, modelData)
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
print(round(cvRankResults,2))
# draw histogram base on ranking
library(ggplot2)
modelsName = rownames(cvRankResults)
p <- list()
for(i in 1:length(accuracy_metrics)){
g = paste("rank", i, sep = "")
selectedData <- cvRankResults[names(cvRankResults) == g]
names(selectedData) <- c("rank");
p[[i]] <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 1)) +
geom_text(aes(label = rank, vjust = -0.8, hjust = 0.5)) +
#ggtitle(accuracy_metrics[i]) +
labs(caption=toupper(accuracy_metrics[i])) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank())
}
library("cowplot")
prow <- plot_grid( p[[1]] + theme(legend.position="none"),
p[[2]] + theme(legend.position="none"),
p[[3]] + theme(legend.position="none"),
p[[4]] + theme(legend.position="none"),
p[[5]] + theme(legend.position="none"),
p[[6]] + theme(legend.position="none"),
align = 'vh',
hjust = -1,
nrow = 1
)
legend_b <- get_legend(p[[1]] + theme(legend.position="bottom", legend.justification="center"))
title <- ggdraw() + draw_label("Ranking Result for Cross Validation", fontface='bold')
p_cvRank <- plot_grid(title, prow, legend_b, ncol = 1, rel_heights = c(.2 , 1, .1))
p_cvRank
View(models)
View(models)
View(models)
#register the model into the models list with the hyper parameters returned from  the "trainsaction_based_model" function
transaction_models <- trainsaction_based_model(modelData)
models = append(models, transaction_models)
benchmarkResults
View(modelData)
View(modelData)
View(modelData)
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum
View(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
benchmarkResults <- modelBenchmark(models, modelData)
save.image("D:/ResearchSpace/ResearchProjects/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/7-22.RData")
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)
modelData <- selectData("../android_analysis_datasets/android_dataset_7_21.csv")
models = list()
#register the model into the models list with the hyper parameters returned from  the "trainsaction_based_model" function
transaction_models <- trainsaction_based_model(modelData)
View(transaction_models)
View(transaction_models)
View(transaction_models)
models = append(models, transaction_models)
#initialize the size metric based models
size_models <- size_metric_models(modelData)
#register the list of the size metric based models.
models = append(models, size_models)
View(models)
View(models)
benchmarkResults <- modelBenchmark(models, modelData)
save.image("D:/ResearchSpace/ResearchProjects/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/7-23.RData")
View(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
View(benchmarkResults)
