sapply(data, function(x) sum(is.na(x)))
## Impute
library(mice)
library(randomForest)
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- complete(miceMod)
# remove collinear columns
coli <- findLinearCombos(data.imputed)
data.done <- data.imputed[, -coli$remove]
data.done$Effort <- dataset$Effort
return(data.done)
}
#define the lasso model
m_fit.lasso <- function(lasso,dataset){
#dataset = modelData
#lasso = list()
#ind_variables = c("Activity_Num", "Component_Num", "Precedence_Num",	"Stimulus_Num",	"Response_Num",	"Tran_Num",	"Boundary_Num")
cleanData <- clean(dataset)
#keep the columns that have been changed and set into the ind_variables
#ind_variables <- setdiff(names(dataset), names(cleanData))
#colNames <- names(cleanData)
#for(i in 1:length(colNames)){
#  if(!identical(dataset[, colNames[i]], cleanData[, colNames[i]])){
#    ind_variables <- c(ind_variables, colNames[i])
#  }
#}
ind_variables <- colnames(cleanData[, colnames(cleanData)!="Effort"])
x_data <- data.matrix(cleanData[, ind_variables])
y_data <- data.matrix(cleanData[, c("Effort")])
#lasso_lm <- glmnet(x = x_data, y = y_data, alpha = 1, standardize = T)
set.seed(2)
lambda_list <- Lasso_range(x_data,y_data,100)
cvfit = cv.glmnet(x_data,y_data,
standardize = T, lambda = lambda_list, type.measure = 'mse', nfolds = 5, alpha = 1)
lasso_lm = cvfit$glmnet.fit
#print(lasso_lm$lambda)
#plot(lasso_lm)
#for 10 biggest final features
#plot_glmnet(lasso_lm)                             # default colors
#plot_glmnet(lasso_lm, label=10)
lasso$m = lasso_lm
lasso$m$cv_lambda = min(cvfit$cvm)
lasso$m$cvfit = cvfit
lasso$m$ind_variables = ind_variables
lasso$m$lambda_list = lambda_list
lasso
}
m_predict.lasso <- function(lasso, testData){
#testData = modelData[2, ]
#testData[,lasso$m$ind_variables]
predicted <- predict(lasso$m,newx=data.matrix(testData[,lasso$m$ind_variables]),s=lasso$m$cv_lambda)
predicted_names <- rownames(predicted)
predicted <- as.vector(predicted[,1])
names(predicted) <- predicted_names
predicted
}
lasso_model <- function(dataset){
parameters = list()
}
benchmarkResults <- modelBenchmark(models, modelData)
library(glmnet)
library(plotmo) # for plot_glmnet
library(mice)
library(randomForest)
library(caret)
Lasso_range = function(x, y, k){
# inputs:
# x_matrix, a matrix containing independent variables
# y: vector of dependent varaibles
# k: the length of sequence
# output:
# seq: a sequence of lambdaa from high to low
x = x_data
y = y_data
k = 100
# define my own scale function to simulate that in glmnet
myscale = function(x) sqrt(sum((x - mean(x)) ^ 2) / length(x))
# normalize x
sx = as.matrix(scale(x, scale = apply(x, 2, myscale)))
# sy = as.vector(scale(y, scale = myscale(y)))
max_lambda = max(abs(colSums(sx * as.vector(y)))) / dim(sx)[1]
# The default depends on the sample size nobs relative to the number of variables nvars.
# If nobs > nvars, the default is 0.0001, close to zero.
# If nobs < nvars, the default is 0.01.
# A very small value of lambda.min.ratio will lead to a saturated fit in the nobs < nvars case.
ratio = 0
if(dim(sx)[1] > dim(sx)[2]){
ratio = 0.0001
}else{
ratio = 0.01
}
min_lambda = max_lambda * ratio
log_seq = seq(from  = log(min_lambda), to = log(max_lambda), length.out = k)
seq = sort(exp(log_seq), decreasing = T)
#print(seq)
return(seq)
}
#Lasso_range(x_data,y_data, 100)
cv_lasso_model = function(x_data,y_data){
set.seed(2)
lambda_list <- Lasso_range(x_data,y_data,100)
percent = 50
cvfit = cv.glmnet(x_data,y_data,
standardize = T, type.measure = 'mse', nfolds = 5, alpha = 1)
#print(cvfit$lambda)
# # 5 fold cross validation
k <- 5
#
# function to calculate MMRE
calcMMRE <- function(testData,pred){
mmre <- abs(testData - pred)/testData
mean_value <- mean(mmre)
mean_value
}
# # function to calculate PRED
calcPRED <- function(testData,pred,percent){
value <- abs(testData - pred)/testData
percent_value <- percent/100
pred_value <- value <= percent_value
mean(pred_value)
}
#
folds <- cut(seq(1,nrow(x_data)),breaks=k,labels=FALSE)
mean_mmre <- vector("list",k)
mean_pred <- vector("list",k)
overall_mean_mmre <- vector("list",100)
overall_mean_pred <- vector("list",100)
for(iterator in seq(1,100)){
for(i in 1:k){
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- y_data[testIndexes]
pred <- predict(cvfit,newx=x_data,s=lambda_list[[iterator]])
mean_mmre[[i]] <- calcMMRE(testData,pred[testIndexes])
mean_pred[[i]] <- calcPRED(testData,pred[testIndexes],percent)
}
overall_mean_mmre[[iterator]] <- mean(as.numeric(mean_mmre))
overall_mean_pred[[iterator]] <- mean(as.numeric(mean_pred))
}
plot(log(lambda_list),overall_mean_mmre,xlab="log(Lambda)",ylab="MMRE")
lines(log(lambda_list),overall_mean_mmre,xlim=range(log(lambda_list)), ylim=range(overall_mean_mmre), pch=16)
plot(log(lambda_list),overall_mean_pred,xlab="log(Lambda)",ylab = "PRED")
lines(log(lambda_list),overall_mean_pred,xlim=range(log(lambda_list)), ylim=range(overall_mean_pred), pch=16)
}
# Preprocess dataset
clean <- function(dataset){
# numeric data only
numeric_columns <- unlist(lapply(dataset, is.numeric))
data.numeric <- dataset[, numeric_columns]
data.numeric <- data.frame(apply(dataset, 2, as.numeric))
# remove near zero variance columns
nzv_cols <- nearZeroVar(data.numeric)
if(length(nzv_cols) > 0) {
data <- data.numeric[, -nzv_cols]
}
sapply(data, function(x) sum(is.na(x)))
## Impute
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- complete(miceMod)
# remove collinear columns
coli <- findLinearCombos(data.imputed)
data.done <- data.imputed[, -coli$remove]
data.done$Effort <- dataset$Effort
return(data.done)
}
#define the lasso model
m_fit.lasso <- function(lasso,dataset){
#dataset = modelData
#lasso = list()
#ind_variables = c("Activity_Num", "Component_Num", "Precedence_Num",	"Stimulus_Num",	"Response_Num",	"Tran_Num",	"Boundary_Num")
cleanData <- clean(dataset)
#keep the columns that have been changed and set into the ind_variables
#ind_variables <- setdiff(names(dataset), names(cleanData))
#colNames <- names(cleanData)
#for(i in 1:length(colNames)){
#  if(!identical(dataset[, colNames[i]], cleanData[, colNames[i]])){
#    ind_variables <- c(ind_variables, colNames[i])
#  }
#}
ind_variables <- colnames(cleanData[, colnames(cleanData)!="Effort"])
x_data <- data.matrix(cleanData[, ind_variables])
y_data <- data.matrix(cleanData[, c("Effort")])
#lasso_lm <- glmnet(x = x_data, y = y_data, alpha = 1, standardize = T)
set.seed(2)
lambda_list <- Lasso_range(x_data,y_data,100)
cvfit = cv.glmnet(x_data,y_data,
standardize = T, lambda = lambda_list, type.measure = 'mse', nfolds = 5, alpha = 1)
lasso_lm = cvfit$glmnet.fit
#print(lasso_lm$lambda)
#plot(lasso_lm)
#for 10 biggest final features
#plot_glmnet(lasso_lm)                             # default colors
#plot_glmnet(lasso_lm, label=10)
lasso$m = lasso_lm
lasso$m$cv_lambda = min(cvfit$cvm)
lasso$m$cvfit = cvfit
lasso$m$ind_variables = ind_variables
lasso$m$lambda_list = lambda_list
lasso
}
m_predict.lasso <- function(lasso, testData){
#testData = modelData[2, ]
#testData[,lasso$m$ind_variables]
predicted <- predict(lasso$m,newx=data.matrix(testData[,lasso$m$ind_variables]),s=lasso$m$cv_lambda)
predicted_names <- rownames(predicted)
predicted <- as.vector(predicted[,1])
names(predicted) <- predicted_names
predicted
}
lasso_model <- function(dataset){
parameters = list()
}
benchmarkResults <- modelBenchmark(models, modelData)
library(glmnet)
library(plotmo) # for plot_glmnet
library(mice)
library(randomForest)
library(caret)
Lasso_range = function(x, y, k){
# inputs:
# x_matrix, a matrix containing independent variables
# y: vector of dependent varaibles
# k: the length of sequence
# output:
# seq: a sequence of lambdaa from high to low
x = x_data
y = y_data
k = 100
# define my own scale function to simulate that in glmnet
myscale = function(x) sqrt(sum((x - mean(x)) ^ 2) / length(x))
# normalize x
sx = as.matrix(scale(x, scale = apply(x, 2, myscale)))
# sy = as.vector(scale(y, scale = myscale(y)))
max_lambda = max(abs(colSums(sx * as.vector(y)))) / dim(sx)[1]
# The default depends on the sample size nobs relative to the number of variables nvars.
# If nobs > nvars, the default is 0.0001, close to zero.
# If nobs < nvars, the default is 0.01.
# A very small value of lambda.min.ratio will lead to a saturated fit in the nobs < nvars case.
ratio = 0
if(dim(sx)[1] > dim(sx)[2]){
ratio = 0.0001
}else{
ratio = 0.01
}
min_lambda = max_lambda * ratio
log_seq = seq(from  = log(min_lambda), to = log(max_lambda), length.out = k)
seq = sort(exp(log_seq), decreasing = T)
#print(seq)
return(seq)
}
#Lasso_range(x_data,y_data, 100)
cv_lasso_model = function(x_data,y_data){
set.seed(2)
lambda_list <- Lasso_range(x_data,y_data,100)
percent = 50
cvfit = cv.glmnet(x_data,y_data,
standardize = T, type.measure = 'mse', nfolds = 5, alpha = 1)
#print(cvfit$lambda)
# # 5 fold cross validation
k <- 5
#
# function to calculate MMRE
calcMMRE <- function(testData,pred){
mmre <- abs(testData - pred)/testData
mean_value <- mean(mmre)
mean_value
}
# # function to calculate PRED
calcPRED <- function(testData,pred,percent){
value <- abs(testData - pred)/testData
percent_value <- percent/100
pred_value <- value <= percent_value
mean(pred_value)
}
#
folds <- cut(seq(1,nrow(x_data)),breaks=k,labels=FALSE)
mean_mmre <- vector("list",k)
mean_pred <- vector("list",k)
overall_mean_mmre <- vector("list",100)
overall_mean_pred <- vector("list",100)
for(iterator in seq(1,100)){
for(i in 1:k){
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- y_data[testIndexes]
pred <- predict(cvfit,newx=x_data,s=lambda_list[[iterator]])
mean_mmre[[i]] <- calcMMRE(testData,pred[testIndexes])
mean_pred[[i]] <- calcPRED(testData,pred[testIndexes],percent)
}
overall_mean_mmre[[iterator]] <- mean(as.numeric(mean_mmre))
overall_mean_pred[[iterator]] <- mean(as.numeric(mean_pred))
}
plot(log(lambda_list),overall_mean_mmre,xlab="log(Lambda)",ylab="MMRE")
lines(log(lambda_list),overall_mean_mmre,xlim=range(log(lambda_list)), ylim=range(overall_mean_mmre), pch=16)
plot(log(lambda_list),overall_mean_pred,xlab="log(Lambda)",ylab = "PRED")
lines(log(lambda_list),overall_mean_pred,xlim=range(log(lambda_list)), ylim=range(overall_mean_pred), pch=16)
}
# Preprocess dataset
clean <- function(dataset){
# numeric data only
numeric_columns <- unlist(lapply(dataset, is.numeric))
data.numeric <- dataset[, numeric_columns]
data.numeric <- data.frame(apply(dataset, 2, as.numeric))
# remove near zero variance columns
nzv_cols <- nearZeroVar(data.numeric)
if(length(nzv_cols) > 0) {
data <- data.numeric[, -nzv_cols]
}
sapply(data, function(x) sum(is.na(x)))
## Impute
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- mice::complete(miceMod)
# remove collinear columns
coli <- findLinearCombos(data.imputed)
data.done <- data.imputed[, -coli$remove]
data.done$Effort <- dataset$Effort
return(data.done)
}
#define the lasso model
m_fit.lasso <- function(lasso,dataset){
#dataset = modelData
#lasso = list()
#ind_variables = c("Activity_Num", "Component_Num", "Precedence_Num",	"Stimulus_Num",	"Response_Num",	"Tran_Num",	"Boundary_Num")
cleanData <- clean(dataset)
#keep the columns that have been changed and set into the ind_variables
#ind_variables <- setdiff(names(dataset), names(cleanData))
#colNames <- names(cleanData)
#for(i in 1:length(colNames)){
#  if(!identical(dataset[, colNames[i]], cleanData[, colNames[i]])){
#    ind_variables <- c(ind_variables, colNames[i])
#  }
#}
ind_variables <- colnames(cleanData[, colnames(cleanData)!="Effort"])
x_data <- data.matrix(cleanData[, ind_variables])
y_data <- data.matrix(cleanData[, c("Effort")])
#lasso_lm <- glmnet(x = x_data, y = y_data, alpha = 1, standardize = T)
set.seed(2)
lambda_list <- Lasso_range(x_data,y_data,100)
cvfit = cv.glmnet(x_data,y_data,
standardize = T, lambda = lambda_list, type.measure = 'mse', nfolds = 5, alpha = 1)
lasso_lm = cvfit$glmnet.fit
#print(lasso_lm$lambda)
#plot(lasso_lm)
#for 10 biggest final features
#plot_glmnet(lasso_lm)                             # default colors
#plot_glmnet(lasso_lm, label=10)
lasso$m = lasso_lm
lasso$m$cv_lambda = min(cvfit$cvm)
lasso$m$cvfit = cvfit
lasso$m$ind_variables = ind_variables
lasso$m$lambda_list = lambda_list
lasso
}
m_predict.lasso <- function(lasso, testData){
#testData = modelData[2, ]
#testData[,lasso$m$ind_variables]
predicted <- predict(lasso$m,newx=data.matrix(testData[,lasso$m$ind_variables]),s=lasso$m$cv_lambda)
predicted_names <- rownames(predicted)
predicted <- as.vector(predicted[,1])
names(predicted) <- predicted_names
predicted
}
lasso_model <- function(dataset){
parameters = list()
}
benchmarkResults <- modelBenchmark(models, modelData)
fitResults <- benchmarkResults$fitResults
goodness_fit_metrics = benchmarkResults$goodness_fit_metrics
goodnessRankResults = data.frame(matrix(ncol=length(goodness_fit_metrics), nrow=length(fitResults)))
colnames(goodnessRankResults) <- goodness_fit_metrics
goodnessRankResults$model_labels = names(fitResults)
for (i in 1:length(goodness_fit_metrics)){
g = goodness_fit_metrics[i]
selectData = data.frame(matrix(ncol=0, nrow=length(fitResults)))
selectData$model_labels <- names(fitResults)
selectData[, g] <- c()
for(j in 1:length(fitResults)){
m = fitResults[[j]]
m_name = names(fitResults)[j]
selectData[m_name, g] <- m[[g]]
}
selectData[paste("rank", i, sep = "")] <- rank(selectData[,g], ties.method = "min")
goodnessRankResults <- merge(goodnessRankResults, selectData, by = "model_labels", all=FALSE)
}
write.csv(goodnessRankResults, "goodness_rank_results.csv")
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_labels
print(model_labels)
cvAccuracyResults$metric_labels <- accuracy_metrics
#calculate ranking results
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
print(round(cvRankResults,2))
write.csv(round(cvRankResults,2), "model_ranking_results.csv")
# draw histogram base on ranking
library(ggplot2)
modelsName = rownames(cvRankResults)
p <- list()
for(i in 1:length(accuracy_metrics)){
g = paste("rank", i, sep = "")
selectedData <- cvRankResults[names(cvRankResults) == g]
names(selectedData) <- c("rank");
p[[i]] <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black") +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 1)) +
geom_text(aes(label = rank, vjust = -0.3, hjust = 0.5)) +
#ggtitle(accuracy_metrics[i]) +
labs(caption=toupper(accuracy_metrics[i])) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(), panel.background = element_blank())
}
library("cowplot")
prow <- plot_grid( p[[1]] + theme(legend.position="none"),
p[[2]] + theme(legend.position="none"),
p[[3]] + theme(legend.position="none"),
p[[4]] + theme(legend.position="none"),
p[[5]] + theme(legend.position="none"),
p[[6]] + theme(legend.position="none"),
align = 'vh',
hjust = -1,
nrow = 1
)
legend_b <- get_legend(p[[1]] + theme(legend.position="bottom", legend.justification="center"))
title <- ggdraw() + draw_label("Ranking Result for Cross Validation", fontface='bold')
p_cvRank <- plot_grid(title, prow, legend_b, ncol = 1, rel_heights = c(.2 , 1, .1))
p_cvRank
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 1)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle("Overall Ranking Result for Cross Validation") +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
p_cvAllRank
