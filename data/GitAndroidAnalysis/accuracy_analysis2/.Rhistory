for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, model_names[i])
}
}
metric_labels <- c()
for(i in 1:length(model_names)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
accuracy_labels <- paste(model_labels, metric_labels, sep="_")
print(accuracy_labels)
save.image("D:/ResearchSpace/ResearchProjects/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/8_18_models_full_categorized_tests.RData")
projectCategoryLabels <- as.data.frame(as.matrix(read.csv("./project_categorization1.csv")))
projectCategoryLabels <- paste(projectCategoryLabels$Selection, projectCategoryLabels$Type, sep=": ")
#print the categorized benchmark results
for(i in 1: length(model_names)){
benchmarkResults = categorizedBenchmarkResults[[model_names[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(model_names)
names(cvRankResults)<-c("model_labels")
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvRankResults$model_labels = c("COSMIC", "IFPUG", "LASSO", "LOG SLOC", "NEURALNET", "REG TREE", "SLOC", "STEP LNR", "SWTI", "SWTII","SWTIII", "UCP")
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(model_names))
for (j in 1:length(model_names)){
selectedData <- cvRankResults[j,]
for(k in 1:length(accuracy_metrics)){
rank_sum[j] <- rank_sum[j] + selectedData[,2*k+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
#print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
modelsName <- rownames(selectedData)
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle(projectCategoryLabels[i]) +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
print(p_cvAllRank)
}
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
print(projectCategories[i, ])
categorizedDataset <- selectData(datasheet, selector=projectCategories[i,])
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(model_names))
for (j in 1:length(model_names)){
selectedData <- cvRankResults[j,]
for(k in 1:length(accuracy_metrics)){
rank_sum[j] <- rank_sum[j] + selectedData[,2*k+1]
}
}
for(i in 1: length(model_names)){
benchmarkResults = categorizedBenchmarkResults[[model_names[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(model_names)
names(cvRankResults)<-c("model_labels")
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvRankResults$model_labels = c("COSMIC", "IFPUG", "LASSO", "LOG SLOC", "NEURALNET", "REG TREE", "SLOC", "STEP LNR", "SWTI", "SWTII","SWTIII", "UCP")
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(model_names))
for (j in 1:length(model_names)){
selectedData <- cvRankResults[j,]
for(k in 1:length(accuracy_metrics)){
rank_sum[j] <- rank_sum[j] + selectedData[,2*k+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
#print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
modelsName <- rownames(selectedData)
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle(projectCategoryLabels[i]) +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
print(p_cvAllRank)
}
benchmarkResults = categorizedBenchmarkResults[[model_names[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(model_names)
names(cvRankResults)<-c("model_labels")
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
load("D:/ResearchSpace/ResearchProjects/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/8_17_12_models_full_categorized_tests_1.RData")
#print the categorized benchmark results
for(i in 1: length(model_names)){
benchmarkResults = categorizedBenchmarkResults[[model_names[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(model_names)
names(cvRankResults)<-c("model_labels")
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvRankResults$model_labels = c("COSMIC", "IFPUG", "LASSO", "LOG SLOC", "NEURALNET", "REG TREE", "SLOC", "STEP LNR", "SWTI", "SWTII","SWTIII", "UCP")
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(model_names))
for (j in 1:length(model_names)){
selectedData <- cvRankResults[j,]
for(k in 1:length(accuracy_metrics)){
rank_sum[j] <- rank_sum[j] + selectedData[,2*k+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
#print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
modelsName <- rownames(selectedData)
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle(projectCategoryLabels[i]) +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
print(p_cvAllRank)
}
benchmarkResults = categorizedBenchmarkResults[[model_names[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
View(cvAccuracyResults)
View(cvAccuracyResults)
View(categorizedBenchmarkResults)
View(categorizedBenchmarkResults)
View(categorizedBenchmarkResults)
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
print(cvResults)
print(benchmarkResults)
print(model_names)
print(model_names[1])
print the categorized benchmark results
#print the categorized benchmark results
for(i in 1: nCategories){
benchmarkResults = categorizedBenchmarkResults[[names(categorizedBenchmarkResults)[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(model_names)
names(cvRankResults)<-c("model_labels")
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvRankResults$model_labels = c("COSMIC", "IFPUG", "LASSO", "LOG SLOC", "NEURALNET", "REG TREE", "SLOC", "STEP LNR", "SWTI", "SWTII","SWTIII", "UCP")
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(model_names))
for (j in 1:length(model_names)){
selectedData <- cvRankResults[j,]
for(k in 1:length(accuracy_metrics)){
rank_sum[j] <- rank_sum[j] + selectedData[,2*k+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
#print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
modelsName <- rownames(selectedData)
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle(projectCategoryLabels[i]) +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
print(p_cvAllRank)
}
ts$metric_labels <- metric_labels
projectCategoryLabels <- as.data.frame(as.matrix(read.csv("./project_categorization1.csv")))
projectCategoryLabels <- paste(projectCategoryLabels$Selection, projectCategoryLabels$Type, sep=": ")
#print the categorized benchmark results
for(i in 1: nCategories){
benchmarkResults = categorizedBenchmarkResults[[names(categorizedBenchmarkResults)[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(model_names)
names(cvRankResults)<-c("model_labels")
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvRankResults$model_labels = c("COSMIC", "IFPUG", "LASSO", "LOG SLOC", "NEURALNET", "REG TREE", "SLOC", "STEP LNR", "SWTI", "SWTII","SWTIII", "UCP")
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(model_names))
for (j in 1:length(model_names)){
selectedData <- cvRankResults[j,]
for(k in 1:length(accuracy_metrics)){
rank_sum[j] <- rank_sum[j] + selectedData[,2*k+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
#print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
modelsName <- rownames(selectedData)
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle(projectCategoryLabels[i]) +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
print(p_cvAllRank)
}
save.image("D:/ResearchSpace/ResearchProjects/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/8_19_models_full_categorized_tests.RData")
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model.R")
install.packages("maptree")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model.R")
install.packages("tree")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model.R")
projectCategoryLabels <- as.data.frame(as.matrix(read.csv("./project_categorization1.csv")))
projectCategoryLabels <- paste(projectCategoryLabels$Selection, projectCategoryLabels$Type, sep=": ")
for(i in 1:nCategories){
print(projectCategories[i, ])
categorizedDataset <- selectData(datasheet, selector=projectCategories[i,])
#categorizedDataset <- selectData("dsets/combined_data_set_4_1.csv", c("type", "Entertainment"))
#print(rownames(categorizedDataset))
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("initialDate","early"))
#categorizedDataset <- selectData("../android_analysis_datasets/android_dataset_6_29.csv", selector=c("type","Productivity"))
#print(categorizedDataset)
benchmarkResults <- modelBenchmark(models, categorizedDataset)
categorizedBenchmarkResults[[benchmarkLabels[i]]] <- benchmarkResults
# calculate the rank information
}
projectCategoryLabels <- as.data.frame(as.matrix(read.csv("./project_categorization1.csv")))
projectCategoryLabels <- paste(projectCategoryLabels$Selection, projectCategoryLabels$Type, sep=": ")
#print the categorized benchmark results
for(i in 1: nCategories){
benchmarkResults = categorizedBenchmarkResults[[names(categorizedBenchmarkResults)[i]]]
cvResults <- benchmarkResults$cvResults
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$label <- rownames(cvAccuracyResults)
cvAccuracyResults <- cvAccuracyResults[- grep("predRange", cvAccuracyResults$label),]
cvAccuracyResults$label <- NULL
cvAccuracyResults$model_labels <- model_labels
cvAccuracyResults$metric_labels <- metric_labels
#calculate ranking results
cvRankResults <- data.frame(model_names)
names(cvRankResults)<-c("model_labels")
for (j in 1:length(accuracy_metrics)){
g = accuracy_metrics[j]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
colnames(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
cvRankResults$model_labels = c("COSMIC", "IFPUG", "LASSO", "LOG SLOC", "NEURALNET", "REG TREE", "SLOC", "STEP LNR", "SWTI", "SWTII","SWTIII", "UCP")
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(model_names))
for (j in 1:length(model_names)){
selectedData <- cvRankResults[j,]
for(k in 1:length(accuracy_metrics)){
rank_sum[j] <- rank_sum[j] + selectedData[,2*k+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
#print(rank_sum)
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
#print(round(cvRankResults,2))
# draw overall ranking histogram
selectedData <- cvRankResults[names(cvRankResults) == "rank*"]
names(selectedData) <- c("rank");
modelsName <- rownames(selectedData)
p_cvAllRank <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 2)) +
geom_text(aes(label = rank, vjust = -0.4, hjust = 0.5)) +
ggtitle(projectCategoryLabels[i]) +
#labs(caption=toupper("Total Rank")) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)), legend.position = "bottom",
plot.title = element_text(hjust = 0.5),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(),  panel.background = element_blank())
print(p_cvAllRank)
}
View(models)
View(models)
View(models)
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model.R")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
R
install.packages("ggplot2")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
R -version
version
