# Args:
#   B: the previous weights
#   effortAdj: the previous effort adjustment factor
#   sd: the previous standard deviation of the residual
#
# Returns:
#   the next random state
#sd <- 10
sample <- rep(0, 2*length(B)+2)
names(sample) <- c(names(B), paste(names(B), "sigma", sep="_"), "effortAdj", "sd")
sigma <- c(0.1)
if(length(B)>1){
for (i in 2:length(B)) {
sigma <- c(sigma, 1/5* (abs(B[i] - B[i - 1]))+0.1)
}
}
sample[names(B)] <- rnorm(length(B),mean = B, sd = sigma)
sample["effortAdj"] <- rnorm(1, effortAdj, 0.1)
sample['sd'] <- rnorm(1, sd, 3)
sample[paste(names(B), "sigma", sep="_")] <- sigma
return(sample)
}
proposalProbability <- function(x1, x2){
# calculate the proposal probability
#
# Args:
#   x1: the current state
#   x2: the proposed state
#
# Returns:
#   the proposal probability g(x1|x2)
#x1 = chain[1,]
#x2 = proposal
levels <- paste("l", seq(1:((length(x1)-2)/2)), sep="");
probB <- sum(dnorm(x1[levels],mean = x2[levels], sd = x2[paste(levels, "sigma", sep="_")], log = T))
probeffortAdj <- dnorm(x1["effortAdj"], x2["effortAdj"], 0.1, log=T)
probSD <- dnorm(x1["sd"], x2["sd"], log=T)
return(probB+probeffortAdj+probSD)
}
run_metropolis_MCMC <- function(regressionData, N, priorB, varianceMatrix, effortAdj){
# run metropolis-hasting algorithm to simulate the posterior probability
#
# Args:
#   regressionData: the classified transactions
#   N: the number of runs of simulation
#   priorB: the prior expected value of weights
#   varianceMatrix: the variance matrix for the weights
#   effortAdj: the prior effort adjustment factor
#
# Returns:
#   the simulated posterior joint distribution of the parameters
#regressionData <- regressionData
#N <- 10000
#priorB <- means
#varianceMatrix <- covar
chain = matrix(nrow=N+1, ncol=2*length(priorB)+2)
colnames(chain) <- c(names(priorB), paste(names(priorB), "sigma", sep="_"), "effortAdj", "sd")
chain[1, "effortAdj"] <- effortAdj['mean']
chain[1, "sd"] <- 10
chain[1, names(priorB)] <- priorB
sigma <- c(0.1)
if(length(priorB)>1){
for (i in 2:length(priorB)) {
sigma <- c(sigma, 1/5* (abs(priorB[i] - priorB[i - 1]))+0.1)
}
}
chain[1, paste(names(priorB), "sigma", sep="_")] = sigma
for (i in 1:N){
proposal = proposalfunction(chain[i,names(priorB)], chain[i,"effortAdj"], chain[i, "sd"])
#proposal = sample
`%ni%` <- Negate(`%in%`)
update <- posterior(proposal, priorB, varianceMatrix, effortAdj['mean'], effortAdj['var'], regressionData[ , !(colnames(regressionData) %in% c("Effort"))], regressionData[,c("Effort")])
postP <- posterior(chain[i,], priorB, varianceMatrix, effortAdj['mean'], effortAdj['var'], regressionData[ , !(colnames(regressionData) %in% c("Effort"))], regressionData[,c("Effort")])
probab = min(c(1, exp(update + proposalProbability(chain[i,], proposal) - postP - proposalProbability(proposal, chain[i, ]))))
#the better way of calculating the acceptance rate
#acceptance = c()
if(is.na(probab) == FALSE & runif(1) < probab){
chain[i+1,] = proposal
#print("accept")
#acceptance = c(acceptance, "accept")
}else{
chain[i+1,] = chain[i,]
#print("not accept")
#acceptance = c(acceptance, "not accept")
}
}
return(chain)
}
bayesfit<-function(regressionData, N = 1000, burnIn = 500){
# apply metropolis hastings algorithm to simulate the posterior distributions of the parameters
#
# Args:
#   regressionData: the classified transactions
#   N: the number of runs of simulation
#   burnIn: the first number of iterations which are regarded as burn-in
#
# Returns:
#   the simulated posterior distributions of the parameters
#regressionData <- regressionData
#N <- 1000
#B <- means
#varianceMatrix <- covar
#effortAdj <- effortAdj['mean']
#var <- effortAdj['var']
effortAdj <- calEffortAdj(regressionData)
levels = ncol(regressionData) - 1
B <- genMeans(levels)
covar <- genVariance(B, 1/3)
chain = run_metropolis_MCMC(regressionData, N, B, covar, effortAdj)
acceptance = 1-mean(duplicated(chain[-(1:burnIn),]))
ret <- data.frame(chain[-(1:burnIn), names(B)], chain[-(1:burnIn),"effortAdj"], chain[-(1:burnIn),"sd"])
colnames(ret) <- c(names(B), "effortAdj", "sd")
return(ret)
}
Bayes.sum<-function(x) {
# Provides a summary for a variable of a Bayesian linear regression.
#
# Args:
#   x: a column of the data frame returned by the bayesfit() function
#
# Returns:
#   A vector containing the summary
c("mean"=mean(x),
"se"=sd(x),
"t"=mean(x)/sd(x),
"median"=median(x),
"CrI"=quantile(x,prob=0.025),
"CrI"=quantile(x,prob=0.975)
)
}
predict.blm <- function(model, newdata) {
# predict.lm() analogue for Bayesian linear regression
#
# Args:
#   model: a bayes linear regression model
#   newdata: new data to perform prediction
#
# Returns:
#   Vector of new predictions
#newdata <- subset(newdata, !colnames(newdata) %in% c("Effort"))
#newdata = testData
#model = bayesianModel
#print(mean(model[, col]))
`%ni%` <- Negate(`%in%`)
newdata <- subset(newdata,select = colnames(newdata) %ni% c("Effort"))
ret <- apply(newdata, 1, function(x) {
effort <- 0
for (col in colnames(newdata)) {
effort <- effort + (mean(model[, col]) * x[col])
}
effort
})
ret*mean(model[,"effortAdj"])
}
calEffortAdj <- function(regressionData){
# calcuate an approximation of effort adjustment factor based on classified transactions
#
# Args:
#   regressionData: the classified transaction and effort data
#
# Returns:
#   an approximation of effort adjustment factor based on linear regression with prior weights
summary <- summary(priorFit(regressionData))
print(summary)
effortAdj <- c(mean = summary$coefficients["transactionSum","Estimate"], var=summary$coefficients["transactionSum","Std. Error"]^2)
}
priorFit <- function(regressionData){
# fit a linear model using the same of weighted transaction. The weights are prior weights.
#
# Args:
#   regressionData: the classified transaction and effort data
#
# Returns:
#   the fitted linear model using the prior weights
nominalWeights <- as.matrix(genMeans(ncol(regressionData)-1))
transactionData <- as.matrix(regressionData[, !(colnames(regressionData) %in% c("Effort"))])
transactionSum <-  transactionData %*% nominalWeights
transactionRegressionData <- matrix(nrow = nrow(regressionData), ncol=2)
colnames(transactionRegressionData) <- c("transactionSum", "Effort")
rownames(transactionRegressionData) <- rownames(regressionData)
transactionRegressionData[, "transactionSum"] = transactionSum
transactionRegressionData[, "Effort"] = regressionData[, "Effort"]
lm(Effort ~ . - 1, as.data.frame(transactionRegressionData))
}
cachedTransactionFiles = list()
readTransactionData <- function(filePath){
# read the transaction data from the file.The transaction data are cached in cachedTransactionFiles for better performance in cross-validation and boostrapping process.
#
# Args:
#   regressionData: the classified transaction and effort data
#
# Returns:
#   the fitted linear model using the prior weights
if (!file.exists(filePath)) {
print(filePath)
print("file doesn't exist")
if(is.null(cachedTransactionFiles[[filePath]])){
cachedTransactionFiles[[filePath]] <<- data.frame(TL = numeric(),
TD = numeric(),
DETs = numeric())
}
cachedTransactionFiles[[filePath]]
}
else if(!is.null(cachedTransactionFiles[[filePath]])){
cachedTransactionFiles[[filePath]]
}
else {
#filePath = "D:\\AndroidAnalysis\\GatorAnalysisResults\\ClusteringAnalysis-20190418T224936Z-006\\ClusteringAnalysis\\Timber_S1W1L1\\filteredTransactionEvaluation.csv"
fileData = NULL
tryCatch(fileData <- read.csv(filePath),  error=function(e) fileData = NULL)
if(nrow(fileData) == 0 || is.null(fileData)){
fileData <- data.frame(TL = numeric(),
TD = numeric(),
DETs = numeric())
}
else{
if(nrow(fileData) == 1){
fileData = apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x))
fileData = t(fileData)
}
else{
fileData = apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x))
}
fileData <- as.data.frame(fileData)
}
fileData <- na.omit(fileData)
cachedTransactionFiles[[filePath]] <<- fileData
fileData
}
}
loadTransactionData <- function(modelData){
# load the transaction data from the transaction file paths column of model data
#
# Args:
#   modelData: the model data with various fields to describe a project and a colum to reference the associated transaction data
#
# Returns:
#   the transaction data for each project
modelData$transaction_file <- as.character(modelData$transaction_file)
effort <- subset(modelData, select=c("Effort"))
projects <- rownames(modelData)
rownames(effort) <- projects
transactionFileList <- subset(modelData, select=c("transaction_file"))
rownames(transactionFileList) <- projects
numOfTrans <- 0
transactionFiles <- list()
for (project in projects) {
filePath <- transactionFileList[project, "transaction_file"]
fileData <- readTransactionData(filePath)
transactionFiles[[project]] <- fileData
numOfTrans = numOfTrans + nrow(fileData)
}
print(numOfTrans)
combined <- combineData(transactionFiles)
transactionData = list(combined=combined, transactionFiles = transactionFiles, effort = effort, projects=projects)
}
generateRegressionData <- function(projects, cutPoints, effortData, transactionFiles){
# classified the transactions into different levels of complexity for regression analysis
#
# Args:
#   projects: the projects
#   cutPoints: the cut points that define the classification of the transactions.
#   effortData: the effort for the projects
#   transactionFiles: the tranaction records for the projects
#
# Returns:
#   the numbers of transactions for different complexity levels and project effort.
nParams =  nrow(cutPoints)
nBins =   ncol(cutPoints)-1
levels = genColNames(nParams, nBins)
regressionData <- matrix(nrow = length(projects), ncol = length(levels) + 1)
rownames(regressionData) <- projects
colnames(regressionData) <- c(levels, "Effort")
for (project in projects) {
fileData <- transactionFiles[[project]]
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
regressionData <- na.omit(regressionData)
regressionData <- as.data.frame(regressionData)
}
performSearch <- function(n, dataset, parameters = c("TL", "TD", "DETs"), k = 5) {
# Performs search for the optimal number of bins and weights to apply to each
# bin through linear regression.
#
# Args:
#   n: Specifies up to how many bins per parameter to search.
#   folder: Folder containg all the transaction analytics data to analyze.
#   effortData: a data frame containing effort data corresponding to each of
#               the files contained in the folder argument. Rows must be named
#               the same as the filename and effort column should be named "Effort".
#   parameters: A vector of which parameters to analyze. Ex. "TL", "TD", "DETs". When the parameters is an empty array, just apply linear regression on number of transactions.
#   k: How many folds to use for k-fold cross validation.
#
# Returns:
#   A list in which the ith index gives the results of the search for i bins.
#n = 1
#dataset = modelData
#parameters = c("TL", "TD", "DETs")
#k = 5
#dataset <- modelData
#cachedTransactionFiles <- list()
#load transaction data from the datasheet
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles = transactionData$transactionFiles
projects <- names(transactionData$transactionFiles)
distParams = list();
distParams[['TL']] = list(shape=6.543586, rate=1.160249);
distParams[['TD']] = list(shape=3.6492150, rate=0.6985361);
distParams[['DETs']] = list(shape=1.6647412, rate=0.1691911);
#paramAvg <- if (length(parameters) == 1) mean(combinedData[, parameters]) else colMeans(combinedData[, parameters])
#paramSD <- if (length(parameters) == 1) sd(combinedData[, parameters]) else apply(combinedData[, parameters], 2, sd)
if(length(parameters) == 0){
n = 1
}
searchResults <- list()
for (i in seq(1,n)) {
cutPoints <- matrix(NA, nrow = length(parameters), ncol = i + 1)
rownames(cutPoints) <- parameters
for (p in parameters) {
cutPoints[p, ] <- discretize(distParams[[p]][['shape']], distParams[[p]][['rate']], i)
}
#generate classified regression data
regressionData <- generateRegressionData(projects, cutPoints, effortData, transactionFiles)
`%ni%` <- Negate(`%in%`)
paramVals <- bayesfit(regressionData, 10000, 500)
bayesianModel = list()
bayesianModel$weights = subset(paramVals, select = colnames(regressionData) %ni% c("Effort"))
bayesianModel$effortAdj = paramVals[,"effortAdj"]
bayesianModel$sd = paramVals[,"sd"]
bayesianModel$cuts <- cutPoints
bm_validationResults <- crossValidate(regressionData, k, function(trainData) bayesfit(trainData, 1000, 500), predict.blm)
print(bm_validationResults)
#the regression model fit
regressionModel <- lm(Effort ~ . - 1, as.data.frame(regressionData));
reg_validationResults <- crossValidate(regressionData, k,function(trainData) lm(Effort ~ . - 1, as.data.frame(trainData)), function(lm.fit, testData) predict(lm.fit, newData = testData))
#the prior model fit
priorModel <- priorFit(regressionData)
prior_validationResults <- crossValidate(regressionData, k, priorFit, function(priorModel, testData) predict(priorModel, newData = testData))
searchResults[[i]] <- list(
bayesModel = bayesianModel,
bayesModelAccuracyMeasure = bm_validationResults,
priorModel = priorModel,
priorModelAccuracyMeasure = prior_validationResults,
regressionModel = regressionModel,
regressionModelAccuracyMeasure = reg_validationResults,
regressionData = regressionData
)
}
searchResults
}
#effort <- read.csv("modelEvaluations_8_12.csv")
#rownames(effort) <- effort$Project
#SWTIresults <- performSearch(3, effort, c("TL"))
predict.swt <- function(trainedModel, testData){
# predict the effort based on a data point
#
# Args:
#   trainedModel: the trained transaction-based model
#   testData: the data point used to predict project effort
#
# Returns:
#   the estimated project effort
#trainedModelParameters <- readRDS(file="train_model_parameters.rds")
transactionData <- loadTransactionData(testData)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles = transactionData$transactionFiles
projects <- names(transactionData$transactionFiles)
#cuts = model$m$cuts
regressionData <- generateRegressionData(projects, trainedModel$cuts, effortData, transactionFiles)
predicted <- predict.blm(as.matrix(trainedModel$paramVals), newdata = regressionData)
predicted
}
m_fit.tm1 <- function(swtiii,dataset){
# the model fitting function which would be repeated called during the cross validation and bootstrapping function
#
# Args:
#   swtiii: a list of cut points, which are the hyper parameters of the transaction-based model.
#   dataset: the dataset based on which the model is fitted
#
# Returns:
#   the fitted transaction-based model
print("swtiii model training")
#swtiii <- models$tm1
#dataset <- modelData
transactionData <- loadTransactionData(dataset)
effortData <- transactionData$effort
combinedData <- transactionData$combined
transactionFiles <- transactionData$transactionFiles
projects <- names(transactionData$transactionFiles)
regressionData <- generateRegressionData(projects, swtiii$cuts, effortData, transactionFiles)
paramVals <- bayesfit(regressionData, 10000, 500)
bayesianModel = list()
bayesianModel$paramVals <- paramVals
bayesianModel$cuts <- swtiii$cuts
swtiii$cuts = NULL;
swtiii$m = bayesianModel;
swtiii
}
# for model testing
m_predict.tm1 <- function(swtiii, testData){
# the model fitting function which would be repeated called during the cross validation and bootstrapping function
#
# Args:
#   swtiii: a list of cut points, which are the hyper parameters of the transaction-based model.
#   dataset: the dataset based on which the model is fitted
#
# Returns:
#   the fitted transaction-based model
print("swtiii predict function")
predict.swt(swtiii$m, testData)
}
trainsaction_based_model <- function(modelData){
# initiate the transaction-based model by performing a search of optimal classification of transactions, which are defined as a set of cut points
#
# Args:
#   modelData: a held-out dataset to search for the hyperparameter
#
# Returns:
#   the list of cuts points for the individual dimensions
#cachedTransactionFiles = list()
SWTIIIresults <- performSearch(6, modelData, c("TL", "TD", "DETs"))
#intialize the model with hyper parameters (cutpoints) decided by cross validatoin results for different ways of binning
SWTIIIModelSelector <- 4
modelParams = SWTIIIresults[[SWTIIIModelSelector]][["bayesModel"]]
swtiiiParams = list(
cuts = modelParams$cuts
)
}
#register the model into the models list with the hyper parameters returned from  the "trainsaction_based_model" function
models$tm1 = trainsaction_based_model(modelData)
View(models)
View(models)
print(models$tm1)
#initialize the size metric based models
size_models <- size_metric_models()
#register the list of the size metric based models.
models = append(models, size_models)
benchmarkResults <- modelBenchmark(models, modelData)
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
###plot for the bootstrapping results
bsRet <- benchmarkResults$bsResults
#bootstrappingSE(SWTIIIModelData, otherSizeMetricsData, model3, 10000, 0.83)
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
#save as csv
#write.csv(bsEstimations, file='bsEstimations.csv', quote=F, row.names = F)
#write.csv(iterResults, file='iterResults.csv', quote=F, row.names = F)
#read from csv
#bsEstimations <- read.csv('bsEstimations.csv')
#rownames(bsEstimations) <- c('lower','mean','upper')
#iterResults <- read.csv('iterResults.csv')
# plot bootstrapping results
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
df <- data.frame(t(bsEstimations))
df$labels <- rownames(df)
df$model_labels <- model_labels
df$metric_labels <- metric_labels
for (i in 1:length(metric_labels)){
g = metric_labels[i]
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean, ymin = lower, ymax = upper, fill = metric_labels)) +
geom_crossbar(width = 0.5, position = "dodge") +
#coord_flip() +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
xlab('model') +
ylab(g) +
ggtitle(g)
print(p)
}
# family-wise hypothesis test
source('familywiseHypoTest.R')
foldResults <- cvResults$foldResults
sig_cv <- familywiseHypoTest(iterationResults=foldResults, accuracy_metrics, model_names)
sig_bs <- familywiseHypoTest(iterationResults=iterResults, accuracy_metrics, model_names)
head(sig_cv)
head(sig_bs)
