#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
install.packages("jsonlite")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
install.packages("reshape")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
install.packages("fitdistrplus")
install.packages("egg")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
install.packages("rlang")
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
install.packages("lsr")
require(MASS)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model_1.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model_1.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
print(round(cvRankResults,2))
# draw histogram base on ranking
library(ggplot2)
modelsName = rownames(cvRankResults)
p <- list()
for(i in 1:length(accuracy_metrics)){
g = paste("rank", i, sep = "")
selectedData <- cvRankResults[names(cvRankResults) == g]
names(selectedData) <- c("rank");
p[[i]] <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 1)) +
geom_text(aes(label = rank, vjust = -0.8, hjust = 0.5)) +
#ggtitle(accuracy_metrics[i]) +
labs(caption=toupper(accuracy_metrics[i])) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank())
}
library("cowplot")
install.packages("cowplot")
#evaluate cross-validation
model_names <- benchmarkResults$model_names
accuracy_metrics <- benchmarkResults$accuracy_metrics
#plot for the cross validation results
cvResults <- benchmarkResults$cvResults
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info")
ggplot(meltAvgPreds) + theme_bw() + geom_point(aes(x=Pred, y=Value, group=Method,color=Method),size=3)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as lines and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_line(aes(y=Value, x=Pred, group=Method,color=Method)) +
stat_smooth(aes(y=Value, x=Pred, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
cvAccuracyResults <- data.frame(cvResults$accuracyResults)
cvAccuracyResults$model_labels <- model_names
cvAccuracyResults$metric_labels <- accuracy_metrics
cvRankResults <- data.frame(names(models))
names(cvRankResults)<-c("model_labels")
for (i in 1:length(accuracy_metrics)){
g = accuracy_metrics[i]
selectedData <- cvAccuracyResults[cvAccuracyResults$metric_labels == g,]
selectedData <- selectedData[,-3]#delete the metric_labels
names(selectedData)<-c(g, "model_labels")
if(g == "mmre" || g == "mdmre" || g == "mae"){
selectedData[paste("rank", i, sep = "")] <- rank(selectedData[,1], ties.method = "min")
}else{
selectedData[paste("rank", i, sep = "")] <- rank(-selectedData[,1], ties.method = "min")
}
cvRankResults <- merge(cvRankResults, selectedData, by = "model_labels", all=FALSE)
}
#make a total rank(rank*) base on the ranki
rank_sum <- vector(mode = "integer",length = length(models))
for (i in 1:length(models)){
selectedData <- cvRankResults[i,]
for(j in 1:length(accuracy_metrics)){
rank_sum[i] <- rank_sum[i] + selectedData[,2*j+1]
}
}
rank_sum <- rank(rank_sum, ties.method = "min")
cvRankResults["rank*"] <- rank_sum
cvRankResults <- cvRankResults[order(cvRankResults$'rank*'),]
#change the first line as the row name
rownames(cvRankResults) = cvRankResults[,1]
cvRankResults <- cvRankResults[,-1]
print(round(cvRankResults,2))
# draw histogram base on ranking
library(ggplot2)
modelsName = rownames(cvRankResults)
p <- list()
for(i in 1:length(accuracy_metrics)){
g = paste("rank", i, sep = "")
selectedData <- cvRankResults[names(cvRankResults) == g]
names(selectedData) <- c("rank");
p[[i]] <- ggplot(selectedData, aes(x=modelsName, y=rank, fill=modelsName)) +
geom_bar(stat="identity", colour = "black", width = 0.7) +
#scale_y_discrete(expand = c(0, 0)) +
guides(fill = guide_legend(title = "MODEL", nrow = 1)) +
geom_text(aes(label = rank, vjust = -0.8, hjust = 0.5)) +
#ggtitle(accuracy_metrics[i]) +
labs(caption=toupper(accuracy_metrics[i])) +
theme(plot.caption = element_text(hjust=0.5, size=rel(1)),
#axis.line=element_blank(),
axis.title=element_blank(), axis.text=element_blank(),axis.ticks=element_blank())
}
library("cowplot")
prow <- plot_grid( p[[1]] + theme(legend.position="none"),
p[[2]] + theme(legend.position="none"),
p[[3]] + theme(legend.position="none"),
p[[4]] + theme(legend.position="none"),
p[[5]] + theme(legend.position="none"),
p[[6]] + theme(legend.position="none"),
align = 'vh',
hjust = -1,
nrow = 1
)
legend_b <- get_legend(p[[1]] + theme(legend.position="bottom", legend.justification="center"))
title <- ggdraw() + draw_label("Ranking Result for Cross Validation", fontface='bold')
p_cvRank <- plot_grid(title, prow, legend_b, ncol = 1, rel_heights = c(.2 , 1, .1))
p_cvRank
#evaluate bootstrap results
bsRet <- benchmarkResults$bsResults
#bootstrappingSE(SWTIIIModelData, otherSizeMetricsData, model3, 10000, 0.83)
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
#save as csv
#write.csv(bsEstimations, file='bsEstimations.csv', quote=F, row.names = F)
#write.csv(iterResults, file='iterResults.csv', quote=F, row.names = F)
#read from csv
#bsEstimations <- read.csv('bsEstimations.csv')
#rownames(bsEstimations) <- c('lower','mean','upper')
#iterResults <- read.csv('iterResults.csv')
# plot bootstrapping results
model_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, names(models)[i])
}
}
metric_labels <- c()
for(i in 1:length(models)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
df <- data.frame(t(bsEstimations))
df$labels <- rownames(df)
df$model_labels <- model_labels
df$metric_labels <- metric_labels
print(metric_labels)
# calculate non-overlapping intervals.
nonOverlappingPairs <- list()
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
ii <- 1
nonOverlap <- list()
for (j in 1:(nrow(selectedData)-1)){
for (k in (j+1):nrow(selectedData)){
if(selectedData[j,]$lower>selectedData[k,]$upper | selectedData[j,]$upper<selectedData[k,]$lower){
#selectedData[j,] and selectedData[k,] non-overlap
nonOverlap[[ii]] <- c(selectedData[j,]$model_labels, selectedData[k,]$model_labels)
ii <- ii+1
}
}
}
nonOverlappingPairs[[g]] <- nonOverlap
}
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))
print(p)
}
# draw a partially ordered graph based on non-overlapping pairs
# for (i in 1:length(metric_labels)){
#     g = metric_labels[i]
#     selectedData <- df[df$metric_labels == g,]
#     p <- ggplot(selectedData, aes(x = labels, y = mean, ymin = lower, ymax = upper, fill = metric_labels)) +
#     geom_crossbar(width = 0.5, position = "dodge") +
#     #coord_flip() +
#     scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
#     xlab('model') +
#     ylab(g) +
#     ggtitle(g)
#     print(p)
# }
source('familywiseHypoTest2.R')
foldResults <- cvResults$foldResults
# sig_cv <- familywiseHypoTest(iterationResults=foldResults, accuracy_metrics, model_names)
# head(sig_cv)
sig_bs <- familywiseHypoTest(iterationResults=iterResults, accuracy_metrics, model_names, "boot")
round_df <- function(df, digits) {
nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
df[,nums] <- round(df[,nums], digits = digits)
(df)
}
sig_bs_f = sig_bs[which(sig_bs$BH_p_value < 0.05),]
sig_bs_f = sig_bs_f[which(sig_bs_f$metric %in% c("mmre", "mdmre", "pred25", "mae")),]
sig_bs_f = sig_bs_f[which(sig_bs_f$model1 %in% c("tm3") | sig_bs_f$model2 %in% c("tm3")),]
sig_bs_f = sig_bs_f[order(sig_bs_f$bonferroni_p_value),]
sig_bs_f = round_df(sig_bs_f, 2)
print(sig_bs_f)
# Using the "sig_bs" results, create a graph to represent the direct graph for the models.
library(igraph)
install.packages("igraph")
# family-wise hypothesis test
source('familywiseHypoTest2.R')
foldResults <- cvResults$foldResults
# sig_cv <- familywiseHypoTest(iterationResults=foldResults, accuracy_metrics, model_names)
# head(sig_cv)
sig_bs <- familywiseHypoTest(iterationResults=iterResults, accuracy_metrics, model_names, "boot")
round_df <- function(df, digits) {
nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
df[,nums] <- round(df[,nums], digits = digits)
(df)
}
sig_bs_f = sig_bs[which(sig_bs$BH_p_value < 0.05),]
sig_bs_f = sig_bs_f[which(sig_bs_f$metric %in% c("mmre", "mdmre", "pred25", "mae")),]
sig_bs_f = sig_bs_f[which(sig_bs_f$model1 %in% c("tm3") | sig_bs_f$model2 %in% c("tm3")),]
sig_bs_f = sig_bs_f[order(sig_bs_f$bonferroni_p_value),]
sig_bs_f = round_df(sig_bs_f, 2)
print(sig_bs_f)
# Using the "sig_bs" results, create a graph to represent the direct graph for the models.
library(igraph)
for(metric_i in 1:length(accuracy_metrics)){
selectedData <- sig_bs[sig_bs$metric == metric_labels[metric_i],]
m <- matrix(0, nrow = length(models), ncol = length(models), byrow = FALSE)
colnames(m) <- names(models)
rownames(m) <- names(models)
for(i in 1:nrow(selectedData)){
if(selectedData$BH_p_value[i] < 0.05){
if(selectedData$direction[i] == "+"){
m[as.character(selectedData$model1[i]), as.character(selectedData$model2[i])] = round(selectedData$bonferroni_p_value[i], 3)
}else if(selectedData$direction[i] == "-"){
m[as.character(selectedData$model2[i]), as.character(selectedData$model1[i])] = round(selectedData$bonferroni_p_value[i], 3)
}
}
}
edge_val <- c()
# if A -> B -> C, remove edge A -> C
for(i in 1:length(models)){
for(j in 1:length(models)){
if(m[i,j] != 0){
edge_val <- c(edge_val, m[i,j])
for(k in 1:length(models)){
if(m[j,k] != 0)
m[i,k] = 0
}
}
}
}
#plot the directed graph
model_mean <- matrix(0, nrow = length(models), byrow = FALSE)
rownames(model_mean) <- names(models)
colnames(model_mean) <- "mean"
for(i in 1:nrow(selectedData)){
model_mean[which(rownames(model_mean) == selectedData[i,]$model1)] = round(selectedData[i,]$model1_mean, 3)
model_mean[which(rownames(model_mean) == selectedData[i,]$model2)] = round(selectedData[i,]$model2_mean, 3)
}
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold,
vertex.color="white", vertex.label.color="black", vertex.size=25,
edge.color="black", edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
title(main = metric_labels[metric_i])
}
# filter for
foldResults <- cvResults$foldResults
# sig_cv <- familywiseHypoTest(iterationResults=foldResults, accuracy_metrics, model_names)
# head(sig_cv)
sig_bs <- familywiseHypoTest(iterationResults=iterResults, accuracy_metrics, model_names, "boot")
round_df <- function(df, digits) {
nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
df[,nums] <- round(df[,nums], digits = digits)
(df)
}
sig_bs_f = sig_bs[which(sig_bs$BH_p_value < 0.05),]
sig_bs_f = sig_bs_f[which(sig_bs_f$metric %in% c("mmre", "mdmre", "pred25", "mae")),]
sig_bs_f = sig_bs_f[which(sig_bs_f$model1 %in% c("tm3") | sig_bs_f$model2 %in% c("tm3")),]
sig_bs_f = sig_bs_f[order(sig_bs_f$bonferroni_p_value),]
sig_bs_f = round_df(sig_bs_f, 2)
print(sig_bs_f)
# Using the "sig_bs" results, create a graph to represent the direct graph for the models.
library(igraph)
for(metric_i in 1:length(accuracy_metrics)){
selectedData <- sig_bs[sig_bs$metric == metric_labels[metric_i],]
m <- matrix(0, nrow = length(models), ncol = length(models), byrow = FALSE)
colnames(m) <- names(models)
rownames(m) <- names(models)
for(i in 1:nrow(selectedData)){
if(selectedData$BH_p_value[i] < 0.05){
if(selectedData$direction[i] == "+"){
m[as.character(selectedData$model1[i]), as.character(selectedData$model2[i])] = round(selectedData$bonferroni_p_value[i], 3)
}else if(selectedData$direction[i] == "-"){
m[as.character(selectedData$model2[i]), as.character(selectedData$model1[i])] = round(selectedData$bonferroni_p_value[i], 3)
}
}
}
edge_val <- c()
# if A -> B -> C, remove edge A -> C
for(i in 1:length(models)){
for(j in 1:length(models)){
if(m[i,j] != 0){
edge_val <- c(edge_val, m[i,j])
for(k in 1:length(models)){
if(m[j,k] != 0)
m[i,k] = 0
}
}
}
}
#plot the directed graph
model_mean <- matrix(0, nrow = length(models), byrow = FALSE)
rownames(model_mean) <- names(models)
colnames(model_mean) <- "mean"
for(i in 1:nrow(selectedData)){
model_mean[which(rownames(model_mean) == selectedData[i,]$model1)] = round(selectedData[i,]$model1_mean, 3)
model_mean[which(rownames(model_mean) == selectedData[i,]$model2)] = round(selectedData[i,]$model2_mean, 3)
}
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold,
vertex.color="white", vertex.label.color="black", vertex.size=25,
edge.color="black", edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
title(main = metric_labels[metric_i])
}
