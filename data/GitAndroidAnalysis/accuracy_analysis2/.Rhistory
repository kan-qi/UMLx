#model_names <- c("neuralnet", "lasso", "reg_tree", "tm1", "ucp", "fp", "cosmic", "sloc", "ln_sloc", "step_lnr")
#model_texts <- c("NEURAL_NET", "LASSO", "REG_TREE", "TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC", "STEP_REG")
#model_estimator <- c("ANN", "LASSO", "REG TREE", "LSR", "LSR", "LSR", "LSR", "LSR", "LSR", "STEP")
#test 3
#model_names <- c("tm1", "ucp", "fp", "cosmic", "sloc", "ln_sloc")
#model_texts <- c("TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC")
#model_estimator <- c("LSR", "LSR", "LSR", "LSR", "LSR", "LSR")
model_mapping <- data.frame(model_names=model_names, model_texts=model_texts, model_estimator=model_estimator)
#accuracy_metrics <- c('mmre','pred15','pred25','pred50', 'mdmre', 'mae', 'predRange')
#accuracy_metric_texts <- c('MMRE','PRED15','PRED25','PRED50', 'MDMRE', 'MAE', 'PREDRANGE')
accuracy_metrics <- c('mmre', 'pred25', 'mdmre', 'mae')
accuracy_metric_texts <- c('MMRE', 'PRED25', 'MDMRE', 'MAE')
accuracy_mapping <- data.frame(accuracy_metrics = accuracy_metrics, accuracy_metric_texts=accuracy_metric_texts)
#accuracy_metrics <- benchmarkResults$accuracy_metric
#accuracy_metrics <- accuracy_metrics[!accuracy_metrics %in% c("pred15", "pred50")]
goodness_fit_metrics = benchmarkResults$goodness_fit_metrics
fitResults <- benchmarkResults$fitResults
cvResults <- benchmarkResults$cvResults
bsRet <- benchmarkResults$bsResults
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
model_labels <- c()
for(i in 1:length(model_names)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, model_names[i])
}
}
print(model_labels)
metric_labels <- c()
for(i in 1:length(model_names)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
accuracy_labels <- paste(model_labels, metric_labels, sep="_")
#print(accuracy_labels)
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
avgPreds <- avgPreds[,names(avgPreds) %in% c(model_names, "Pred")]
names(avgPreds) <- model_mapping$model_texts[match(names(avgPreds), model_mapping$model_names)]
names(avgPreds)[1] = c("Pred")
#apply new labels
#colnames(avgPreds) <- c("Pred", "SWT", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "SWT-I", "SWT-II", "SWT-III", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "SWT-I", "SWT-II", "SWT-III", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "NEURAL_NET", "LASSO", "REG_TREE", "TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC", "STEP_REG")
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#rank the cv results of different metric
benchmarkResults <- modelBenchmark(models, modelData)
#model_names <- benchmarkResults$model_names
#test 1
model_names <- c("ucp", "fp", "cosmic", "sloc", "ln_sloc")
model_texts <- c("UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC")
model_estimator <- c("LSR", "LSR", "LSR", "LSR", "LSR")
#test 2
#model_names <- c("neuralnet", "lasso", "reg_tree", "tm1", "ucp", "fp", "cosmic", "sloc", "ln_sloc", "step_lnr")
#model_texts <- c("NEURAL_NET", "LASSO", "REG_TREE", "TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC", "STEP_REG")
#model_estimator <- c("ANN", "LASSO", "REG TREE", "LSR", "LSR", "LSR", "LSR", "LSR", "LSR", "STEP")
#test 3
#model_names <- c("tm1", "ucp", "fp", "cosmic", "sloc", "ln_sloc")
#model_texts <- c("TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC")
#model_estimator <- c("LSR", "LSR", "LSR", "LSR", "LSR", "LSR")
model_mapping <- data.frame(model_names=model_names, model_texts=model_texts, model_estimator=model_estimator)
#accuracy_metrics <- c('mmre','pred15','pred25','pred50', 'mdmre', 'mae', 'predRange')
#accuracy_metric_texts <- c('MMRE','PRED15','PRED25','PRED50', 'MDMRE', 'MAE', 'PREDRANGE')
accuracy_metrics <- c('mmre', 'pred25', 'mdmre', 'mae')
accuracy_metric_texts <- c('MMRE', 'PRED25', 'MDMRE', 'MAE')
accuracy_mapping <- data.frame(accuracy_metrics = accuracy_metrics, accuracy_metric_texts=accuracy_metric_texts)
#accuracy_metrics <- benchmarkResults$accuracy_metric
#accuracy_metrics <- accuracy_metrics[!accuracy_metrics %in% c("pred15", "pred50")]
goodness_fit_metrics = benchmarkResults$goodness_fit_metrics
fitResults <- benchmarkResults$fitResults
cvResults <- benchmarkResults$cvResults
bsRet <- benchmarkResults$bsResults
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
model_labels <- c()
for(i in 1:length(model_names)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, model_names[i])
}
}
print(model_labels)
metric_labels <- c()
for(i in 1:length(model_names)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
accuracy_labels <- paste(model_labels, metric_labels, sep="_")
#print(accuracy_labels)
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
avgPreds <- avgPreds[,names(avgPreds) %in% c(model_names, "Pred")]
names(avgPreds) <- model_mapping$model_texts[match(names(avgPreds), model_mapping$model_names)]
names(avgPreds)[1] = c("Pred")
#apply new labels
#colnames(avgPreds) <- c("Pred", "SWT", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "SWT-I", "SWT-II", "SWT-III", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "SWT-I", "SWT-II", "SWT-III", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "NEURAL_NET", "LASSO", "REG_TREE", "TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC", "STEP_REG")
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
knitr::opts_chunk$set(echo = TRUE)
source("utils/feature_selection.R")
source("utils/data_selection.R")
source("utils/model_funcs.R")
source("accuracy_confidence_evaluation.R")
source("transaction_based_model.R")
source("size_metric_based_models.R")
source("neuralnet_model.R")
source("stepwise_linear_model.R")
source("lasso_regression_model.R")
source("regression_tree_model.R")
source('familywiseHypoTest.R')
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
library(lsr)
require(MASS)
#modelData <- selectData("../android_analysis_datasets/android_dataset_7_21.csv")
#modelData <- selectData("dsets/combined_data_set_4.csv")
#dataset <- read.csv("dsets/combined_data_set_4_rufus.csv")
#dataset <- read.csv("dsets/combined_data_set_8_22.csv")
dataset <- read.csv("dsets/combined_data_set_8_23.csv")
#dataset <- read.csv("dsets/combined_data_set_4_1.csv")
modelData <- selectData(dataset)
models = list()
#initialize the size metric based models
size_models <- size_metric_models(modelData)
#register the list of the size metric based models.
models = append(models, size_models)
benchmarkResults <- modelBenchmark(models, modelData)
#model_names <- benchmarkResults$model_names
#test 1
model_names <- c("ucp", "fp", "cosmic", "sloc", "ln_sloc")
model_texts <- c("UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC")
model_estimator <- c("LSR", "LSR", "LSR", "LSR", "LSR")
#test 2
#model_names <- c("neuralnet", "lasso", "reg_tree", "tm1", "ucp", "fp", "cosmic", "sloc", "ln_sloc", "step_lnr")
#model_texts <- c("NEURAL_NET", "LASSO", "REG_TREE", "TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC", "STEP_REG")
#model_estimator <- c("ANN", "LASSO", "REG TREE", "LSR", "LSR", "LSR", "LSR", "LSR", "LSR", "STEP")
#test 3
#model_names <- c("tm1", "ucp", "fp", "cosmic", "sloc", "ln_sloc")
#model_texts <- c("TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC")
#model_estimator <- c("LSR", "LSR", "LSR", "LSR", "LSR", "LSR")
model_mapping <- data.frame(model_names=model_names, model_texts=model_texts, model_estimator=model_estimator)
#accuracy_metrics <- c('mmre','pred15','pred25','pred50', 'mdmre', 'mae', 'predRange')
#accuracy_metric_texts <- c('MMRE','PRED15','PRED25','PRED50', 'MDMRE', 'MAE', 'PREDRANGE')
accuracy_metrics <- c('mmre', 'pred25', 'mdmre', 'mae')
accuracy_metric_texts <- c('MMRE', 'PRED25', 'MDMRE', 'MAE')
accuracy_mapping <- data.frame(accuracy_metrics = accuracy_metrics, accuracy_metric_texts=accuracy_metric_texts)
#accuracy_metrics <- benchmarkResults$accuracy_metric
#accuracy_metrics <- accuracy_metrics[!accuracy_metrics %in% c("pred15", "pred50")]
goodness_fit_metrics = benchmarkResults$goodness_fit_metrics
fitResults <- benchmarkResults$fitResults
cvResults <- benchmarkResults$cvResults
bsRet <- benchmarkResults$bsResults
bsEstimations <- bsRet[['bsEstimations']]
iterResults <- bsRet[['iterResults']]
model_labels <- c()
for(i in 1:length(model_names)){
for(j in 1:length(accuracy_metrics)){
model_labels = c(model_labels, model_names[i])
}
}
print(model_labels)
metric_labels <- c()
for(i in 1:length(model_names)){
for(j in 1:length(accuracy_metrics)){
metric_labels = c(metric_labels, accuracy_metrics[j])
}
}
accuracy_labels <- paste(model_labels, metric_labels, sep="_")
#print(accuracy_labels)
avgPreds <- cvResults[["avgPreds"]]
avgPreds <- data.frame(avgPreds)
avgPreds <- avgPreds[,names(avgPreds) %in% c(model_names, "Pred")]
names(avgPreds) <- model_mapping$model_texts[match(names(avgPreds), model_mapping$model_names)]
names(avgPreds)[1] = c("Pred")
#apply new labels
#colnames(avgPreds) <- c("Pred", "SWT", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "SWT-I", "SWT-II", "SWT-III", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "SWT-I", "SWT-II", "SWT-III", "UUCP", "AFP", "COSMIC", "SLOC", "LOG SLOC")
#colnames(avgPreds) <- c("Pred", "NEURAL_NET", "LASSO", "REG_TREE", "TRAN", "UCP", "IFPUG", "COSMIC", "SLOC", "LOG_SLOC", "STEP_REG")
meltAvgPreds = melt(avgPreds, id.vars="Pred")
colnames(meltAvgPreds) <- c("Pred", "Method", "Value")
print("melt avg preds info as dots and smooth function")
ggplot(meltAvgPreds) + theme_bw() +
geom_point(aes(x=Pred, y=Value, group=Method,color=Method,shape=Method),size=1.5) +
scale_shape_manual(values=c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15))+
stat_smooth(aes(x=Pred, y=Value, group=Method,color=Method), method = lm, formula = y ~ poly(x, 10), se = FALSE)+ xlab("Relative Deviation (%)") +
ylab("Percentage of Estimates <= x%")+ theme(legend.position="bottom")
#evaluate bootstrap results
#bootstrappingSE(SWTIIIModelData, otherSizeMetricsData, model3, 10000, 0.83)
#save as csv
#write.csv(bsEstimations, file='bsEstimations.csv', quote=F, row.names = F)
#write.csv(iterResults, file='iterResults.csv', quote=F, row.names = F)
#read from csv
#bsEstimations <- read.csv('bsEstimations.csv')
#rownames(bsEstimations) <- c('lower','mean','upper')
#iterResults <- read.csv('iterResults.csv')
# plot bootstrapping results
df <- data.frame(t(bsEstimations))
df$labels <- rownames(df)
#df <- df[- grep("predRange", df$labels),]
#df <- df[- grep("pred15", df$labels),]
#df <- df[- grep("pred50", df$labels),]
#df <- df[- grep("tm2", df$labels),]
#df <- df[- grep("tm3", df$labels),]
df <- df[df$label %in% accuracy_labels,]
df$model_labels <- model_labels
df$metric_labels <- metric_labels
print(metric_labels)
nonOverlappingPairs <- data.frame(matrix(ncol = 8, nrow = 0))
overlappingPairAttrs <- c("model1", "model2", "metric", "direction", "mean1", "mean2", "84% CI1", "84% CI2")
colnames(nonOverlappingPairs) <- overlappingPairAttrs
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
selectedData <- df[df$metric_labels == g,]
for (j in 1:(nrow(selectedData)-1)){
for (k in (j+1):nrow(selectedData)){
if(selectedData[j,]$lower>selectedData[k,]$upper | selectedData[j,]$upper<selectedData[k,]$lower){
#selectedData[j,] and selectedData[k,] non-overlap
direction = "="
if(selectedData[j,]$mean > selectedData[k,]$mean){
direction = "+"
}
else if(selectedData[j,]$mean < selectedData[k,]$mean){
direction = "-"
}
if(selectedData[j,]$metric %in% c("mae", "mdmre", "mmre")){
if(direction == "+"){
direction = "-"
}
else if(direction == "-"){
direction = "+"
}
}
nonOverlap <- data.frame(
selectedData[j,]$model_labels,
selectedData[k,]$model_labels,
g,
direction,
round(selectedData[j,]$mean, 3),
round(selectedData[k,]$mean, 3),
paste0("[", as.character(round(selectedData[j,]$lower, 3)), ", ", as.character(round(selectedData[j,]$upper, 3)), "]"),
paste0("[", as.character(round(selectedData[k,]$lower, 3)), ", ", as.character(round(selectedData[k,]$upper, 3)), "]")
)
colnames(nonOverlap) <- overlappingPairAttrs
nonOverlappingPairs <- rbind(nonOverlappingPairs, nonOverlap)
}
}
}
}
print(nonOverlappingPairs)
# apply filters on the overlapping results.
filteredNonOverlappingPairs = nonOverlappingPairs[which(nonOverlappingPairs$metric %in% accuracy_metrics),]
filteredNonOverlappingPairs = nonOverlappingPairs[which((nonOverlappingPairs$model1 %in% c("tm3", "tm2", "tm1") | nonOverlappingPairs$model2 %in% c("tm3", "tm2", "tm1"))) ,]
nonOverlappingPairsIndices <- paste(filteredNonOverlappingPairs$model1, filteredNonOverlappingPairs$model2, filteredNonOverlappingPairs$metric, sep="-")
write.csv(filteredNonOverlappingPairs,'nonOverlappingPairs.csv')
print(filteredNonOverlappingPairs)
print(nonOverlappingPairsIndices)
for (i in 1:length(accuracy_metrics)){
g = metric_labels[i]
g_label <- toupper(g)
selectedData <- df[df$metric_labels == g,]
p <- ggplot(selectedData, aes(x = labels, y = mean)) +
geom_errorbar(aes(ymin=lower, ymax=upper), colour="black", width=.1) +
geom_point(size=2, shape=21, fill="black") + # 21 is filled circle
xlab('MODEL') +
ylab(g_label) +
scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
ggtitle(paste(g_label, "- 84% Confidence Intervals", setp=""))+
theme_bw()
print(p)
}
# draw a partially ordered graph based on non-overlapping pairs
# for (i in 1:length(metric_labels)){
#     g = metric_labels[i]
#     selectedData <- df[df$metric_labels == g,]
#     p <- ggplot(selectedData, aes(x = labels, y = mean, ymin = lower, ymax = upper, fill = metric_labels)) +
#     geom_crossbar(width = 0.5, position = "dodge") +
#     #coord_flip() +
#     scale_x_discrete(breaks=selectedData$label, labels=as.vector(selectedData$model_labels)) +
#     xlab('model') +
#     ylab(g) +
#     ggtitle(g)
#     print(p)
# }
# Using the "sig_bs" results, create a graph to represent the direct graph for the models.
library(igraph)
for(metric_i in 1:length(accuracy_metrics)){
#print(accuracy_metrics[metric_i])
#if(accuracy_metrics[metric_i] == "predRange"){
#  next
#}
#print(accuracy_metrics)
selectedData <- nonOverlappingPairs[nonOverlappingPairs$metric == metric_labels[metric_i],]
m <- matrix(0, nrow = length(model_names), ncol = length(model_names), byrow = FALSE)
#colnames(m) <- names(models)
#rownames(m) <- names(models)
colnames(m) <- model_names
rownames(m) <- model_names
if(nrow(selectedData) > 0){
for(i in 1:nrow(selectedData)){
if(selectedData$direction[i] == "+"){
m[as.character(selectedData$model1[i]), as.character(selectedData$model2[i])] = 1
}else if(selectedData$direction[i] == "-"){
m[as.character(selectedData$model2[i]), as.character(selectedData$model1[i])] = 1
}
}
}
edge_val <- c()
# if A -> B -> C, remove edge A -> C
for(i in 1:length(model_names)){
for(j in 1:length(model_names)){
if(m[i,j] != 0){
edge_val <- c(edge_val, m[i,j])
for(k in 1:length(model_names)){
if(m[j,k] != 0)
m[i,k] = 0
}
}
}
}
#plot the directed graph
model_mean <- matrix(0, nrow = length(model_names), byrow = FALSE)
rownames(model_mean) <- model_names
colnames(model_mean) <- "mean"
if(nrow(selectedData) > 0){
for(i in 1:nrow(selectedData)){
model_mean[which(rownames(model_mean) == selectedData[i,]$model1)] = round(selectedData[i,]$mean1, 3)
model_mean[which(rownames(model_mean) == selectedData[i,]$model2)] = round(selectedData[i,]$mean2, 3)
}
}
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
lo <- layout.fruchterman.reingold(net, niter = 1000)
plot(net, vertex.label=paste(model_mapping$model_texts[match(V(net)$name, model_mapping$model_names)], model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 5, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5, edge.label.cex=0.7)
#plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold(net)*30.0, vertex.color="white", vertex.label.color="black", vertex.size=3, edge.color="black", vertex.label.dist = 0.5, edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
title(main = accuracy_mapping[accuracy_mapping$accuracy_metrics == metric_labels[metric_i], "accuracy_metric_texts"])
sig_bs <- familywiseHypoTest(iterationResults=iterResults, accuracy_metrics, model_names, "boot")
sig_bs = sig_bs[which(sig_bs$model1 %in% model_names | sig_bs$model2 %in% model_names),]
#sig_bs$model1 <- model_mapping$model_texts[match(sig_bs_f$model1, model_mapping$model_names)]
#sig_bs$model2 <- model_mapping$model_texts[match(sig_bs_f$model2, model_mapping$model_names)]
round_df <- function(df, digits) {
nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
df[,nums] <- round(df[,nums], digits = digits)
(df)
}
#print only pair comparisons that are related to transaction models
sig_bs_f = sig_bs[which(sig_bs$BH_p_value < 0.05),]
sig_bs_f = sig_bs_f[which(sig_bs_f$metric %in% accuracy_metrics),]
sig_bs_f = sig_bs_f[which(sig_bs_f$model1 %in% c("tm1", "tm2", "tm3") | sig_bs_f$model2 %in% c("tm1", "tm2", "tm3")),]
sig_bs_f = sig_bs_f[order(sig_bs_f$bonferroni_p_value),]
sig_bs_f = round_df(sig_bs_f, 2)
print(sig_bs_f)
#identify the parirs that are not identified from the 84% overlaps.
sig_bs_f_indices <- paste(sig_bs_f$model1, sig_bs_f$model2, sig_bs_f$metric, sep="-")
print(sig_bs_f_indices)
sig_bs_f$index <- sig_bs_f_indices
`%ni%` <- Negate(`%in%`)
filtered_sig_bs_f = sig_bs_f[which(sig_bs_f$index %ni% nonOverlappingPairsIndices), ]
write.csv(filtered_sig_bs_f,'sig_bs_f.csv')
# Using the "sig_bs" results, create a graph to represent the direct graph for the models.
library(igraph)
for(metric_i in 1:length(accuracy_metrics)){
#print(accuracy_metrics[metric_i])
#if(accuracy_metrics[metric_i] == "predRange"){
#  next
#}
#print(accuracy_metrics)
selectedData <- sig_bs[sig_bs$metric == metric_labels[metric_i],]
m <- matrix(0, nrow = length(model_names), ncol = length(model_names), byrow = FALSE)
#colnames(m) <- names(models)
#rownames(m) <- names(models)
colnames(m) <- model_names
rownames(m) <- model_names
for(i in 1:nrow(selectedData)){
if(selectedData$BH_p_value[i] < 0.05){
if(selectedData$direction[i] == "+"){
m[as.character(selectedData$model1[i]), as.character(selectedData$model2[i])] = round(selectedData$BH_p_value[i], 3)
}else if(selectedData$direction[i] == "-"){
m[as.character(selectedData$model2[i]), as.character(selectedData$model1[i])] = round(selectedData$BH_p_value[i], 3)
}
}
}
edge_val <- c()
# if A -> B -> C, remove edge A -> C
for(i in 1:length(model_names)){
for(j in 1:length(model_names)){
if(m[i,j] != 0){
edge_val <- c(edge_val, m[i,j])
for(k in 1:length(model_names)){
if(m[j,k] != 0)
m[i,k] = 0
}
}
}
}
#plot the directed graph
model_mean <- matrix(0, nrow = length(model_names), byrow = FALSE)
rownames(model_mean) <- model_names
colnames(model_mean) <- "mean"
for(i in 1:nrow(selectedData)){
model_mean[which(rownames(model_mean) == selectedData[i,]$model1)] = round(selectedData[i,]$model1_mean, 3)
model_mean[which(rownames(model_mean) == selectedData[i,]$model2)] = round(selectedData[i,]$model2_mean, 3)
}
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
lo <- layout.fruchterman.reingold(net, niter = 1000)
plot(net, vertex.label=paste(model_mapping$model_texts[match(V(net)$name, model_mapping$model_names)], model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 5, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5, edge.label = edge_val, edge.label.cex=0.7)
#plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold(net)*30.0, vertex.color="white", vertex.label.color="black", vertex.size=3, edge.color="black", vertex.label.dist = 0.5, edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
title(main = accuracy_mapping[accuracy_mapping$accuracy_metrics == metric_labels[metric_i], "accuracy_metric_texts"])
}
# filter for
sig_bs <- familywiseHypoTest(iterationResults=iterResults, accuracy_metrics, model_names, "boot")
sig_bs = sig_bs[which(sig_bs$model1 %in% model_names | sig_bs$model2 %in% model_names),]
#sig_bs$model1 <- model_mapping$model_texts[match(sig_bs_f$model1, model_mapping$model_names)]
#sig_bs$model2 <- model_mapping$model_texts[match(sig_bs_f$model2, model_mapping$model_names)]
round_df <- function(df, digits) {
nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))
df[,nums] <- round(df[,nums], digits = digits)
(df)
}
#print only pair comparisons that are related to transaction models
sig_bs_f = sig_bs[which(sig_bs$BH_p_value < 0.05),]
sig_bs_f = sig_bs_f[which(sig_bs_f$metric %in% accuracy_metrics),]
sig_bs_f = sig_bs_f[which(sig_bs_f$model1 %in% c("tm1", "tm2", "tm3") | sig_bs_f$model2 %in% c("tm1", "tm2", "tm3")),]
sig_bs_f = sig_bs_f[order(sig_bs_f$bonferroni_p_value),]
sig_bs_f = round_df(sig_bs_f, 2)
print(sig_bs_f)
#identify the parirs that are not identified from the 84% overlaps.
sig_bs_f_indices <- paste(sig_bs_f$model1, sig_bs_f$model2, sig_bs_f$metric, sep="-")
print(sig_bs_f_indices)
sig_bs_f$index <- sig_bs_f_indices
`%ni%` <- Negate(`%in%`)
filtered_sig_bs_f = sig_bs_f[which(sig_bs_f$index %ni% nonOverlappingPairsIndices), ]
write.csv(filtered_sig_bs_f,'sig_bs_f.csv')
# Using the "sig_bs" results, create a graph to represent the direct graph for the models.
library(igraph)
for(metric_i in 1:length(accuracy_metrics)){
#print(accuracy_metrics[metric_i])
#if(accuracy_metrics[metric_i] == "predRange"){
#  next
#}
#print(accuracy_metrics)
selectedData <- sig_bs[sig_bs$metric == metric_labels[metric_i],]
m <- matrix(0, nrow = length(model_names), ncol = length(model_names), byrow = FALSE)
#colnames(m) <- names(models)
#rownames(m) <- names(models)
colnames(m) <- model_names
rownames(m) <- model_names
for(i in 1:nrow(selectedData)){
if(selectedData$BH_p_value[i] < 0.05){
if(selectedData$direction[i] == "+"){
m[as.character(selectedData$model1[i]), as.character(selectedData$model2[i])] = round(selectedData$BH_p_value[i], 3)
}else if(selectedData$direction[i] == "-"){
m[as.character(selectedData$model2[i]), as.character(selectedData$model1[i])] = round(selectedData$BH_p_value[i], 3)
}
}
}
edge_val <- c()
# if A -> B -> C, remove edge A -> C
for(i in 1:length(model_names)){
for(j in 1:length(model_names)){
if(m[i,j] != 0){
edge_val <- c(edge_val, m[i,j])
for(k in 1:length(model_names)){
if(m[j,k] != 0)
m[i,k] = 0
}
}
}
}
#plot the directed graph
model_mean <- matrix(0, nrow = length(model_names), byrow = FALSE)
rownames(model_mean) <- model_names
colnames(model_mean) <- "mean"
for(i in 1:nrow(selectedData)){
model_mean[which(rownames(model_mean) == selectedData[i,]$model1)] = round(selectedData[i,]$model1_mean, 3)
model_mean[which(rownames(model_mean) == selectedData[i,]$model2)] = round(selectedData[i,]$model2_mean, 3)
}
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
lo <- layout.fruchterman.reingold(net, niter = 1000)
plot(net, vertex.label=paste(model_mapping$model_texts[match(V(net)$name, model_mapping$model_names)], model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 5, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5, edge.label = edge_val, edge.label.cex=0.7)
#plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold(net)*30.0, vertex.color="white", vertex.label.color="black", vertex.size=3, edge.color="black", vertex.label.dist = 0.5, edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
title(main = accuracy_mapping[accuracy_mapping$accuracy_metrics == metric_labels[metric_i], "accuracy_metric_texts"])
}
# filter for
