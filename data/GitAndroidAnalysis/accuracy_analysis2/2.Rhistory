vertex.label.dist = 0.5, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.width = 0.5)
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
lo <- layout.fruchterman.reingold(net, repulserad = vcount(net)^2.8,
area = vcount(net)^2.3, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 3, vertex.frame.color = NULL,
vertex.label.dist = 0.5, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", , edge.arrow.size=0.5, edge.width = 0.5)
lo <- layout.fruchterman.reingold(net, repulserad = vcount(net)^2.8,
area = vcount(net)^2.3, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 3, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5)
lo <- layout.fruchterman.reingold(net, repulserad = vcount(net)^2.8,
area = vcount(net)^2.3, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 3, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5)
title(main = metric_labels[metric_i])
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
lo <- layout.fruchterman.reingold(net, repulserad = vcount(net)^2.8,
area = vcount(net)^2.3, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 3, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5)
#  plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold(net)*30.0, vertex.color="white", vertex.label.color="black", vertex.size=3, edge.color="black", vertex.label.dist = 0.5, edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
title(main = metric_labels[metric_i])
plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold(net)*30.0, vertex.color="white", vertex.label.color="black", vertex.size=3, edge.color="black", vertex.label.dist = 0.5, edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
lo <- layout.fruchterman.reingold(net, repulserad = vcount(net)^2.8,
area = vcount(net)^2.3, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 5, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5, edge.label = edge_val)
lo <- layout.fruchterman.reingold(net, repulserad = vcount(net)^2.8,
area = vcount(net)^2.3, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 5, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5, edge.label = edge_val, edge.label.cex=0.7)
library(igraph)
for(metric_i in 1:length(accuracy_metrics)){
print(accuracy_metrics[metric_i])
if(accuracy_metrics[metric_i] == "predRange"){
next
}
selectedData <- sig_bs[sig_bs$metric == metric_labels[metric_i],]
m <- matrix(0, nrow = length(models), ncol = length(models), byrow = FALSE)
colnames(m) <- names(models)
rownames(m) <- names(models)
for(i in 1:nrow(selectedData)){
if(selectedData$BH_p_value[i] < 0.05){
if(selectedData$direction[i] == "+"){
m[as.character(selectedData$model1[i]), as.character(selectedData$model2[i])] = round(selectedData$BH_p_value[i], 3)
}else if(selectedData$direction[i] == "-"){
m[as.character(selectedData$model2[i]), as.character(selectedData$model1[i])] = round(selectedData$BH_p_value[i], 3)
}
}
}
edge_val <- c()
# if A -> B -> C, remove edge A -> C
for(i in 1:length(models)){
for(j in 1:length(models)){
if(m[i,j] != 0){
edge_val <- c(edge_val, m[i,j])
for(k in 1:length(models)){
if(m[j,k] != 0)
m[i,k] = 0
}
}
}
}
#plot the directed graph
model_mean <- matrix(0, nrow = length(models), byrow = FALSE)
rownames(model_mean) <- names(models)
colnames(model_mean) <- "mean"
for(i in 1:nrow(selectedData)){
model_mean[which(rownames(model_mean) == selectedData[i,]$model1)] = round(selectedData[i,]$model1_mean, 3)
model_mean[which(rownames(model_mean) == selectedData[i,]$model2)] = round(selectedData[i,]$model2_mean, 3)
}
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
lo <- layout.fruchterman.reingold(net, repulserad = vcount(net)^2.8,
area = vcount(net)^2.3, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 5, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5, edge.label = edge_val, edge.label.cex=0.7)
#plot.igraph(net,vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout=layout.fruchterman.reingold(net)*30.0, vertex.color="white", vertex.label.color="black", vertex.size=3, edge.color="black", vertex.label.dist = 0.5, edge.label = edge_val, edge.width=3, edge.arrow.size=0.5, edge.arrow.width=1.2)
title(main = metric_labels[metric_i])
}
net=graph.adjacency(m,mode="directed",weighted=TRUE,diag=FALSE)
lo <- layout.fruchterman.reingold(net, niter = 1000)
plot(net, vertex.label=paste(V(net)$name, model_mean[which(rownames(model_mean) == V(net)$name)], sep = " : "), layout = lo, vertex.size = 5, vertex.frame.color = NULL,
vertex.label.dist = 1, vertex.label.cex = 0.7,  vertex.label.color="black",
edge.color="black", edge.arrow.size=0.5, edge.width = 0.5, edge.label = edge_val, edge.label.cex=0.7)
save.image("D:/ResearchSpace/ResearchProjects/UMLx/data/GitAndroidAnalysis/accuracy_analysis2/7-27.RData")
=======
>>>>>>> 0ee63dc8d7f46ced13faa9d30cddee827ccacaed
source("neuralnet_model_2.R")
source("neuralnet_model_2.R")
source("stepwise_linear_model.R")
models = list()
#initialize the neuralnet model
models$neuralnet = neuralnet_model(modelData)
#initialize the neuralnet model
neuralnetModel = models$neuralnet
#initialize the neuralnet model
neuralnetModel = neuralnet_model(modelData)
benchmarkResults <- modelBenchmark(models, modelData)
source("neuralnet_model_2.R")
benchmarkResults <- modelBenchmark(models, modelData)
install.packages("RANN")
benchmarkResults <- modelBenchmark(models, modelData)
source("stepwise_linear_model.R")
benchmarkResults <- modelBenchmark(models, modelData)
models = list()
stepwise_linear_model
models$step_lnr <- step_model
models$step_lnr <- step_model
#intialize the step-wise learning model
step_model <- stepwise_linear_model(modelData)
models$step_lnr <- step_model
benchmarkResults <- modelBenchmark(models, modelData)
step_lnr$m <- stepAIC(step_m, direction = "both", trace = FALSE)
require(MASS)
step_lnr$m <- stepAIC(step_m, direction = "both", trace = FALSE)
benchmarkResults <- modelBenchmark(models, modelData)
#define the stepwise linear model
m_fit.step_lnr <- function(step_lnr,dataset){
cleanData <- clean(dataset)
#str_frm <- paste("Effort ~",step_lnr$formula)
str_frm <- paste("Effort ~", paste(colnames(cleanData), collapse="+"))
frm <- as.formula(str_frm)
#step_m <- lm(frm, data=dataset)
step_m <- lm(frm, data=cleanData)
step_lnr$m <- stepAIC(step_m, direction = "both", trace = FALSE)
#step_lnr$cols_removed = c()
step_lnr
}
m_predict.step_lnr <- function(step_lnr, testData){
# numeric data only
numeric_columns <- unlist(lapply(testData, is.numeric))
data.numeric <- testData[, numeric_columns]
data.numeric <- data.frame(apply(testData, 2, as.numeric))
#cols_removed = step_lnr$cols_removed
predicted <- predict(step_lnr$m, testData)
}
stepwise_linear_model <- function(modelData){
#modelData <- selectData("dsets/android_dataset_5_15.csv")
#rownames(modelData) <- modelData$Project
#c1<-c1[-c(5,9,18,19,20,21,25,33,36,38,50,51,52,53,54,55,56,60,61,70,75,79,80,81,83,84,85,89,90,91,92,102,103.106,107,112,114,120,124,125,128,129,130,131,134,135,136,137)]
#c_name <- list("Tran_Num","Activity_Num","Component_Num","Precedence_Num","Stimulus_Num","Response_Num")
step_lnr <- list()
#str_frm <- gsub("[\r\n]", "", str_frm)
#frm <- as.formula(str_frm)
}
# Preprocess dataset
clean <- function(dataset){
# numeric data only
numeric_columns <- unlist(lapply(dataset, is.numeric))
data.numeric <- dataset[, numeric_columns]
data.numeric <- data.frame(apply(dataset, 2, as.numeric))
# remove near zero variance columns
library(caret)
nzv_cols <- nearZeroVar(data.numeric)
if(length(nzv_cols) > 0) data <- data.numeric[, -nzv_cols]
sapply(data, function(x) sum(is.na(x)))
## Impute
library(mice)
library(randomForest)
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- complete(miceMod)
# remove collinear columns
#descrCorr <- cor(data.imputed)
#highCorr <- findCorrelation(descrCorr, 0.90)
#data.imputed1 <- data.imputed[, -highCorr]
#coli <- findLinearCombos(data.imputed1)
coli <- findLinearCombos(data.imputed)
data.done <- data.imputed[, -coli$remove]
data.done$Effort <- dataset$Effort
return(data.done)
}
benchmarkResults <- modelBenchmark(models, modelData)
install.packages(c("maptree", "rpart.plot"))
# Regression Tree Example
library(rpart)
library(rpart.plot)
library(maptree)
library(tree)
#select the most appropriate cp value
cp.select <- function(big.tree) {
min.x <- which.min(big.tree$cptable[, 4]) #column 4 is xerror
for(i in 1:nrow(big.tree$cptable)) {
if(big.tree$cptable[i, 4] < big.tree$cptable[min.x, 4] + big.tree$cptable[min.x, 5]) {
cp = big.tree$cptable[i, 1] big.tree$cptable #column 5: xstd, column 1: cp
# prevent overfitting
if (cp > 0.1 && i != nrow(big.tree$cptable)) {
print("CP VALUE is:")
print(big.tree$cptable[i + 1, 1])
return(big.tree$cptable[i + 1, 1])
}
else {
return(cp) #column 5: xstd, column 1: cp
}
}
}
}
#define the regression tree model
m_fit.reg_tree <- function(reg_tree,dataset){
#reg_tree$m = lm(Effort~reg_tree, data=dataset)
#fit <- rpart(Effort~UseCase_Num + Tran_Num + Activity_Num + Component_Num + Precedence_Num + Stimulus_Num + Tran_Num,
#             method="anova", data=android_dataset_4_26)
#if (features == 'default') {
# 51 features left after dropping: 1. cols containing all missing values (all zeros); 2. duplucated cols
#  features <- c('Effort', 'NT', 'NORT', 'EUCP', 'DM', 'Tran_Num', 'EXUCP', 'EXT', 'ANPC', 'class_num', 'Attribute_num', 'real_num', 'MPC', 'COSMIC', 'Complex_UC', 'ANAPUC', 'EF', 'NEM', 'Personnel', 'TRAN_NA', 'EXTIVK', 'Avg_TD', 'avg_attribute', 'objectdata_num', 'NOP', 'SWTIII', 'NOR', 'NOUC', 'ControlNum', 'IFPUG', 'Average_UC', 'Component_num', 'WMC', 'RR', 'Arch_Diff', 'UCP', 'Type', 'Activity_Num', 'ANWMC', 'Actor_Num', 'MKII', 'Priori_COCOMO_Estimate', 'Avg_TL', 'INT', 'UseCase_Num', 'avg_real', 'NT.1', 'EXTCLL', 'avg_usage', 'Boundary_Num', 'Simple_UC', 'COCOMO_Estimate')
#}
#train_df <- dataset[,names(dataset)%in%features]
# train_df['Type'] <- apply(train_df['Type'], 1, function(x) if(x == 'Website') 1 else if (x =='Mobile App') 2 else if (x=='Information System') 3 else 4)
#features = 'default'
prune = TRUE
plot_tree = FALSE
train_df <- clean(data)
dims <- colnames(train_df)
reg_tree$dims <- dims
# rt = rpart(Effort~., method="class", data=train_df)
# define the regression model with formula Effort ~ x1 + x2 +..
rt = rpart(str_frm <- paste("Effort ~", paste(dims, collapse="+")), control = rpart.control(minsplit = 2), data=train_df)
# show the result of the regression tree
# draw.tree(rt)
# show a better looking tree
# prp(rt,box.col=c("Grey", "Orange")[rt$frame$yval],varlen=0, type=1,under=TRUE)
# show the relation between Complexity Parameter and x-error
# For getting higher mmre and lower pre
plotcp(rt)
printcp(rt)
reg_tree$m <- rt
# plotcp(rt)
#cross validation to check where to stop pruning
# set.seed(3)
#
# cv_tree = cv.tree(rt, FUN = prune.misclass)
#
# name(cv_tree)
#
# plot(cv_tree$size,
#      cv_tree$dev,
#      type = "b"
#      )
# rt$cptable
#pruning
if (prune == TRUE) {
# cp_id <- which.min(rt$cptable[,"xerror"]) #id of min xerror
# cp <- rt$cptable[cp_id,"CP"] #cp threshold
# rt_pruned <- prune(rt,cp=0.05)
getCP = cp.select(rt)
if (getCP <= 0.05) {
rt_pruned <- prune(rt, cp = getCP)
# rt_pruned=prune(rt,cp=rt$cptable[which.min(rt$cptable[,"xerror"]),"CP"])
}
else {
rt_pruned <- prune(rt,cp=0.05)
}
reg_tree$m <- rt_pruned
}
if (plot_tree == TRUE) {
print(reg_tree$m)
plot(reg_tree$m)
text(reg_tree$m)
}
reg_tree
}
m_predict.reg_tree <- function(reg_tree, testData){
#if (predictors == 'default') {
# predict variable names (no target variable)
#  predictors <- c('NT', 'NORT', 'EUCP', 'DM', 'Tran_Num', 'EXUCP', 'EXT', 'ANPC', 'class_num', 'Attribute_num', 'real_num', 'MPC', 'COSMIC', 'Complex_UC', 'ANAPUC', 'EF', 'NEM', 'Personnel', 'TRAN_NA', 'EXTIVK', 'Avg_TD', 'avg_attribute', 'objectdata_num', 'NOP', 'SWTIII', 'NOR', 'NOUC', 'ControlNum', 'IFPUG', 'Average_UC', 'Component_num', 'WMC', 'RR', 'Arch_Diff', 'UCP', 'Type', 'Activity_Num', 'ANWMC', 'Actor_Num', 'MKII', 'Priori_COCOMO_Estimate', 'Avg_TL', 'INT', 'UseCase_Num', 'avg_real', 'NT.1', 'EXTCLL', 'avg_usage', 'Boundary_Num', 'Simple_UC', 'COCOMO_Estimate')
#}
test_df <- testData[, reg_tree$dims]
#test_df <- testData[,names(testData)%in%predictors]
# test_df['Type'] <- apply(test_df['Type'], 1, function(x) if(x == 'Website') 1 else if (x =='Mobile App') 2 else if (x=='Information System') 3 else 4)
predict(reg_tree$m, test_df)
}
regression_tree_model <- function(modelData){
models = list()
models$reg_tree = list()
models
}
# Preprocess dataset
clean <- function(dataset){
# numeric data only
#numeric_columns <- unlist(lapply(dataset, is.numeric))
#data.numeric <- dataset[, numeric_columns]
#data.numeric <- data.frame(apply(dataset, 2, as.numeric))
# remove near zero variance columns
library(caret)
nzv_cols <- nearZeroVar(data.numeric)
if(length(nzv_cols) > 0) data <- data.numeric[, -nzv_cols]
sapply(data, function(x) sum(is.na(x)))
## Impute
library(mice)
library(randomForest)
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- complete(miceMod)
# remove collinear columns
#descrCorr <- cor(data.imputed)
#highCorr <- findCorrelation(descrCorr, 0.90)
#data.imputed1 <- data.imputed[, -highCorr]
#coli <- findLinearCombos(data.imputed1)
coli <- findLinearCombos(data.imputed)
data.done <- data.imputed[, -coli$remove]
data.done$Effort <- dataset$Effort
return(data.done)
}
#
# printcp(fit) # display the results
# plotcp(fit) # visualize cross-validation results
# summary(fit) # detailed summary of splits
#
# # create additional plots
# par(mfrow=c(1,2)) # two plots on one page
# rsq.rpart(fit) # visualize cross-validation results
#
# # plot tree
# plot(fit, uniform=TRUE,
#      main="Regression Tree for Mileage ")
# text(fit, use.n=TRUE, all=TRUE, cex=.8)
#
# # create attractive postcript plot of tree
# post(fit, file = "tree2.ps",
#      title = "Regression Tree for Mileage ")
# # prune the tree
# pfit<- prune(fit, cp=0.01160389) # from cptable
#
# # plot the pruned tree
# plot(pfit, uniform=TRUE,
#      main="Pruned Regression Tree for Mileage")
# text(pfit, use.n=TRUE, all=TRUE, cex=.8)
# post(pfit, file = "ptree2.ps",
#      title = "Pruned Regression Tree for Mileage")
#
source("regression_tree_model_8_7.R")
# Regression Tree Example
library(rpart)
library(rpart.plot)
library(maptree)
library(tree)
#select the most appropriate cp value
cp.select <- function(big.tree) {
min.x <- which.min(big.tree$cptable[, 4]) #column 4 is xerror
for(i in 1:nrow(big.tree$cptable)) {
if(big.tree$cptable[i, 4] < big.tree$cptable[min.x, 4] + big.tree$cptable[min.x, 5]) {
cp = big.tree$cptable[i, 1] #column 5: xstd, column 1: cp
# prevent overfitting
if (cp > 0.1 && i != nrow(big.tree$cptable)) {
print("CP VALUE is:")
print(big.tree$cptable[i + 1, 1])
return(big.tree$cptable[i + 1, 1])
}
else {
return(cp) #column 5: xstd, column 1: cp
}
}
}
}
#define the regression tree model
m_fit.reg_tree <- function(reg_tree,dataset){
#reg_tree$m = lm(Effort~reg_tree, data=dataset)
#fit <- rpart(Effort~UseCase_Num + Tran_Num + Activity_Num + Component_Num + Precedence_Num + Stimulus_Num + Tran_Num,
#             method="anova", data=android_dataset_4_26)
#if (features == 'default') {
# 51 features left after dropping: 1. cols containing all missing values (all zeros); 2. duplucated cols
#  features <- c('Effort', 'NT', 'NORT', 'EUCP', 'DM', 'Tran_Num', 'EXUCP', 'EXT', 'ANPC', 'class_num', 'Attribute_num', 'real_num', 'MPC', 'COSMIC', 'Complex_UC', 'ANAPUC', 'EF', 'NEM', 'Personnel', 'TRAN_NA', 'EXTIVK', 'Avg_TD', 'avg_attribute', 'objectdata_num', 'NOP', 'SWTIII', 'NOR', 'NOUC', 'ControlNum', 'IFPUG', 'Average_UC', 'Component_num', 'WMC', 'RR', 'Arch_Diff', 'UCP', 'Type', 'Activity_Num', 'ANWMC', 'Actor_Num', 'MKII', 'Priori_COCOMO_Estimate', 'Avg_TL', 'INT', 'UseCase_Num', 'avg_real', 'NT.1', 'EXTCLL', 'avg_usage', 'Boundary_Num', 'Simple_UC', 'COCOMO_Estimate')
#}
#train_df <- dataset[,names(dataset)%in%features]
# train_df['Type'] <- apply(train_df['Type'], 1, function(x) if(x == 'Website') 1 else if (x =='Mobile App') 2 else if (x=='Information System') 3 else 4)
#features = 'default'
prune = TRUE
plot_tree = FALSE
train_df <- clean(data)
dims <- colnames(train_df)
reg_tree$dims <- dims
# rt = rpart(Effort~., method="class", data=train_df)
# define the regression model with formula Effort ~ x1 + x2 +..
rt = rpart(str_frm <- paste("Effort ~", paste(dims, collapse="+")), control = rpart.control(minsplit = 2), data=train_df)
# show the result of the regression tree
# draw.tree(rt)
# show a better looking tree
# prp(rt,box.col=c("Grey", "Orange")[rt$frame$yval],varlen=0, type=1,under=TRUE)
# show the relation between Complexity Parameter and x-error
# For getting higher mmre and lower pre
plotcp(rt)
printcp(rt)
reg_tree$m <- rt
# plotcp(rt)
#cross validation to check where to stop pruning
# set.seed(3)
#
# cv_tree = cv.tree(rt, FUN = prune.misclass)
#
# name(cv_tree)
#
# plot(cv_tree$size,
#      cv_tree$dev,
#      type = "b"
#      )
# rt$cptable
#pruning
if (prune == TRUE) {
# cp_id <- which.min(rt$cptable[,"xerror"]) #id of min xerror
# cp <- rt$cptable[cp_id,"CP"] #cp threshold
# rt_pruned <- prune(rt,cp=0.05)
getCP = cp.select(rt)
if (getCP <= 0.05) {
rt_pruned <- prune(rt, cp = getCP)
# rt_pruned=prune(rt,cp=rt$cptable[which.min(rt$cptable[,"xerror"]),"CP"])
}
else {
rt_pruned <- prune(rt,cp=0.05)
}
reg_tree$m <- rt_pruned
}
if (plot_tree == TRUE) {
print(reg_tree$m)
plot(reg_tree$m)
text(reg_tree$m)
}
reg_tree
}
m_predict.reg_tree <- function(reg_tree, testData){
#if (predictors == 'default') {
# predict variable names (no target variable)
#  predictors <- c('NT', 'NORT', 'EUCP', 'DM', 'Tran_Num', 'EXUCP', 'EXT', 'ANPC', 'class_num', 'Attribute_num', 'real_num', 'MPC', 'COSMIC', 'Complex_UC', 'ANAPUC', 'EF', 'NEM', 'Personnel', 'TRAN_NA', 'EXTIVK', 'Avg_TD', 'avg_attribute', 'objectdata_num', 'NOP', 'SWTIII', 'NOR', 'NOUC', 'ControlNum', 'IFPUG', 'Average_UC', 'Component_num', 'WMC', 'RR', 'Arch_Diff', 'UCP', 'Type', 'Activity_Num', 'ANWMC', 'Actor_Num', 'MKII', 'Priori_COCOMO_Estimate', 'Avg_TL', 'INT', 'UseCase_Num', 'avg_real', 'NT.1', 'EXTCLL', 'avg_usage', 'Boundary_Num', 'Simple_UC', 'COCOMO_Estimate')
#}
test_df <- testData[, reg_tree$dims]
#test_df <- testData[,names(testData)%in%predictors]
# test_df['Type'] <- apply(test_df['Type'], 1, function(x) if(x == 'Website') 1 else if (x =='Mobile App') 2 else if (x=='Information System') 3 else 4)
predict(reg_tree$m, test_df)
}
regression_tree_model <- function(modelData){
models = list()
models$reg_tree = list()
models
}
# Preprocess dataset
clean <- function(dataset){
# numeric data only
#numeric_columns <- unlist(lapply(dataset, is.numeric))
#data.numeric <- dataset[, numeric_columns]
#data.numeric <- data.frame(apply(dataset, 2, as.numeric))
# remove near zero variance columns
library(caret)
nzv_cols <- nearZeroVar(data.numeric)
if(length(nzv_cols) > 0) data <- data.numeric[, -nzv_cols]
sapply(data, function(x) sum(is.na(x)))
## Impute
library(mice)
library(randomForest)
# perform mice imputation, based on random forests.
# print(md.pattern(data))
miceMod <- mice(data, method="rf", print=FALSE, remove_collinear = TRUE)
# generate the completed data.
data.imputed <- complete(miceMod)
# remove collinear columns
#descrCorr <- cor(data.imputed)
#highCorr <- findCorrelation(descrCorr, 0.90)
#data.imputed1 <- data.imputed[, -highCorr]
#coli <- findLinearCombos(data.imputed1)
coli <- findLinearCombos(data.imputed)
data.done <- data.imputed[, -coli$remove]
data.done$Effort <- dataset$Effort
return(data.done)
}
#
# printcp(fit) # display the results
# plotcp(fit) # visualize cross-validation results
# summary(fit) # detailed summary of splits
#
# # create additional plots
# par(mfrow=c(1,2)) # two plots on one page
# rsq.rpart(fit) # visualize cross-validation results
#
# # plot tree
# plot(fit, uniform=TRUE,
#      main="Regression Tree for Mileage ")
# text(fit, use.n=TRUE, all=TRUE, cex=.8)
#
# # create attractive postcript plot of tree
# post(fit, file = "tree2.ps",
#      title = "Regression Tree for Mileage ")
# # prune the tree
# pfit<- prune(fit, cp=0.01160389) # from cptable
#
# # plot the pruned tree
# plot(pfit, uniform=TRUE,
#      main="Pruned Regression Tree for Mileage")
# text(pfit, use.n=TRUE, all=TRUE, cex=.8)
# post(pfit, file = "ptree2.ps",
#      title = "Pruned Regression Tree for Mileage")
#
source("regression_tree_model_8_7.R")
install.packages("tree")
source("regression_tree_model_8_7.R")
source('familywiseHypoTest2.R')
source("regression_tree_model_8_7.R")
install.package("tree")
install.packages("tree")
View(regression_tree_model)
View(plot.nnet)
View(plot.nnet)
View(plot.nnet)
