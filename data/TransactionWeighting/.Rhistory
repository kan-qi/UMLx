<<<<<<< HEAD
<<<<<<< HEAD
=======
library(fitdistrplus)
library(egg)
library(gridExtra)
cutsAsVec <- function(x) {
ret <- c("Cut Points:")
for (row in rownames(x)) {
=======
>>>>>>> 1fe44868607a8dc76e3327a35b3a002751dfc82b
cutPoints <- round(x[row, ], digits = 1)
ret <- c(ret, paste(row,":", paste(cutPoints, collapse = ", ")))
}
ret
}
modelData <- read.csv("modelEvaluations-8-30-2.csv")
modelData$Project <- as.character(modelData$Project)
modelData$transaction_file <- as.character(modelData$transaction_file)
effort <- subset(modelData, select=c("Effort"))
transactionFiles <- subset(modelData, select=c("transaction_file"))
rownames(effort) <- modelData$Project
rownames(transactionFiles) <- modelData$Project
combined <- combineData(transactionFiles)
=======
theme_bw(base_size=15)
<<<<<<< HEAD
projects <- rownames(effortData)
View(effort)
#n = 6
effortData = effort
projects <- rownames(effortData)
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numofTrans = nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
View(transactionFiles)
View(transactionFiles)
projects <- rownames(effortData)
print(projects)
projects <- rownames(effortData)
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numofTrans = nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numofTrans = nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration4.R")
source("comparison_between_size_metrics.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
cutsAsVec <- function(x) {
ret <- c("Cut Points:")
for (row in rownames(x)) {
cutPoints <- round(x[row, ], digits = 1)
ret <- c(ret, paste(row,":", paste(cutPoints, collapse = ", ")))
}
ret
}
modelData <- read.csv("modelEvaluations-9-13-1.csv")
modelData$Project <- as.character(modelData$Project)
modelData$transaction_file <- as.character(modelData$transaction_file)
effort <- subset(modelData, select=c("Effort"))
transactionFiles <- subset(modelData, select=c("transaction_file"))
rownames(effort) <- modelData$Project
rownames(transactionFiles) <- modelData$Project
combined <- combineData(transactionFiles)
modelData <- read.csv("modelEvaluations-9-13-1.csv")
modelData$Project <- as.character(modelData$Project)
modelData$transaction_file <- as.character(modelData$transaction_file)
effort <- subset(modelData, select=c("Effort"))
transactionFiles <- subset(modelData, select=c("transaction_file"))
rownames(effort) <- modelData$Project
rownames(transactionFiles) <- modelData$Project
combined <- combineData(transactionFiles)
modelData <- read.csv("modelEvaluations-9-13-1.csv")
modelData$Project <- as.character(modelData$Project)
modelData$transaction_file <- as.character(modelData$transaction_file)
effort <- subset(modelData, select=c("Effort"))
transactionFiles <- subset(modelData, select=c("transaction_file"))
rownames(effort) <- modelData$Project
rownames(transactionFiles) <- modelData$Project
combined <- combineData(transactionFiles)
#n = 6
effortData = effort
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numofTrans = nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numofTrans)
numOfTrans <- 0
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numofTrans = nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numofTrans)
numOfTrans <- 0
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numOfTrans = nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numofTrans)
numOfTrans <- 0
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numOfTrans = nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numOfTrans)
numOfTrans <- 0
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
numOfTrans = numOfTrans + nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numOfTrans)
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
if(nrow(fileData) < 1){
next
}
View(fileData)
View(fileData)
numOfTrans <- 0
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
>>>>>>> f98318d8abf4bdfd4a0e02cac1ce966d2d1c13b7
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
print(nrow(fileData))
if(nrow(fileData) < 1){
next
}
numOfTrans = numOfTrans + nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numOfTrans)
numOfTrans <- 0
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
fileData <- na.omit(fileData)
print(fileData)
if(nrow(fileData) < 1){
next
}
numOfTrans = numOfTrans + nrow(fileData)
classifiedData <- classify(fileData, cutPoints)
regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numOfTrans)
numOfTrans <- 0
for (project in projects) {
filePath <- transactionFiles[project, "transaction_file"]
print(filePath)
if (!file.exists(filePath)) {
print("file doesn't exist")
next
}
fileData <- read.csv(filePath)
fileData <- data.frame(apply(subset(fileData, select = c("TL", "TD", "DETs")), 2, function(x) as.numeric(x)))
#fileData <- na.omit(fileData)
print(fileData)
if(nrow(fileData) < 1){
next
}
numOfTrans = numOfTrans + nrow(fileData)
#classifiedData <- classify(fileData, cutPoints)
#regressionData[project, ] <- c(classifiedData, effortData[project, "Effort"])
}
print(numOfTrans)
<<<<<<< HEAD
regressionData <- na.omit(regressionData)
#regressionData <- rbind(regressionData, "Aggregate" = colSums(regressionData))
regressionData <- as.data.frame(regressionData)
print("regressionData")
print(regressionData)
#}
lm.fit <- lm(Effort ~ .-1, regressionData)
coefficients <- summary(lm.fit)$coefficients
predicts <- as.data.frame(predict(lm.fit, newData = regressionData))
rownames(predicts) <- projects
results = list()
results[["predicts"]] <- predicts
results[["coefficients"]] <- coefficients
results
}
#EUCP
weights <- c(1)
names(weights) <- c("L1")
cutpoints <- NULL
results <- prodByLinearRegression(effort1, transactionFiles1, cutpoints, weights)
print(results)
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration4.R")
source("comparison_between_size_metrics.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
cutsAsVec <- function(x) {
ret <- c("Cut Points:")
for (row in rownames(x)) {
cutPoints <- round(x[row, ], digits = 1)
ret <- c(ret, paste(row,":", paste(cutPoints, collapse = ", ")))
}
ret
}
#Ks parameteric test
parametricKStest(combined[, "TL"])
#Ks parameteric test
parametricKStest(combined[, "TL"])
dist <- combined[, "TL"]
fit.gamma <- fitdist(dist, distr = "gamma", method = "mle", lower = c(0, 0))
# Check result
shape = coefficients(fit.gamma)["shape"]
rate = coefficients(fit.gamma)["rate"]
print(coefficients(fit.gamma))
ksResult <- ks.test(dist, "pgamma", shape, rate)
print("gamma goodness of fit")
print(ksResult)
dist <- combined[, "TD"]
#tableValues <- table(dist)
#reduce sizes for fitting with gamma curve
#print(as.integer(tableValues/10))
#dist <- rep(as.numeric(names(tableValues)), as.integer(tableValues/10))
fit.gamma <- fitdist(dist, distr = "gamma", method = "mle", lower = c(0, 0))
# Check result
shape = coefficients(fit.gamma)["shape"]
rate = coefficients(fit.gamma)["rate"]
print(coefficients(fit.gamma))
ksResult <- ks.test(dist, "pgamma", shape, rate)
print("gamma goodness of fit")
print(ksResult)
ks = ksResult[['statistic']]
parametricKStest <- function(dist){
#dist <- combined[, "TD"]
#tableValues <- table(dist)
#reduce sizes for fitting with gamma curve
#print(as.integer(tableValues/10))
#dist <- rep(as.numeric(names(tableValues)), as.integer(tableValues/10))
fit.gamma <- fitdist(dist, distr = "gamma", method = "mle", lower = c(0, 0))
=======
=======
>>>>>>> eeb31ae77ab5fc8e2547c2ca89a88d15da914ad8
>>>>>>> bb9adaac326be892bfca7effc49b75a635e0b119
require(graphics)
require(dgof)
set.seed(1)
x <- rnorm(50)
y <- runif(30)
# Do x and y come from the same distribution?
ks.test(x, y)
# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")
# test if x is stochastically larger than x2
x2 <- rnorm(50, -1)
plot(ecdf(x), xlim=range(c(x, x2)))
plot(ecdf(x2), add=TRUE, lty="dashed")
t.test(x, x2, alternative="g")
wilcox.test(x, x2, alternative="g")
ks.test(x, x2, alternative="l")
#########################################################
# TBA, JWE new examples added for discrete distributions:
x3 <- sample(1:10, 25, replace=TRUE)
# Using ecdf() to specify a discrete distribution:
ks.test(x3, ecdf(1:10))
# Using step() to specify the same discrete distribution:
myfun <- stepfun(1:10, cumsum(c(0, rep(0.1, 10))))
ks.test(x3, myfun)
# The previous R ks.test() does not correctly calculate the
# test statistic for discrete distributions (gives warning):
# stats::ks.test(c(0, 1), ecdf(c(0, 1)))
# ks.test(c(0, 1), ecdf(c(0, 1)))
# Even when the correct test statistic is given, the
# previous R ks.test() gives conservative p-values:
stats::ks.test(rep(1, 3), ecdf(1:3))
ks.test(rep(1, 3), ecdf(1:3))
ks.test(rep(1, 3), ecdf(1:3), simulate=TRUE, B=10000)
# Do x and y come from the same distribution?
ks.test(x, y)
x <- rnorm(50)
y <- runif(30)
# Do x and y come from the same distribution?
result <- ks.test(x, y)
x <- rnorm(50)
y <- runif(30)
# Do x and y come from the same distribution?
result <- ks.test(x, y)
print(result)
print(result[["p-value"]])
f(30)
# Do x and y come from the same distribution?
result <- ks.test(x, y)
print(result["p-value"])
# Do x and y come from the same distribution?
result <- ks.test(x, y)
View(result)
View(result)
result <- ks.test(x, y)
print(result[['statistic']])
discretize <- function(data, n) {
# Discretizes continuous data into different levels of complexity based on
# quantiles of normal distribution defined by the data.
#
# Args:
#   data: vector of data to discretize
#   n: number of bins to discretize into
#
# Returns:
#   A vector of cut points
#n = 6
#data = combined[, "TD"]
#print(data)
if (n <= 1) {
return(c(-Inf, Inf))
}
quantiles <- seq(1/n, 1 - (1/n), 1/n)
tableValues <- table(data)
#reduce sizes for fitting with gamma curve
#print(as.integer(tableValues/10))
vec <- rep(as.numeric(names(tableValues)), as.integer(tableValues/10))
fit.gamma <- fitdist(vec, distr = "gamma", method = "mle", lower = c(0, 0))
>>>>>>> f98318d8abf4bdfd4a0e02cac1ce966d2d1c13b7
# Check result
shape = coefficients(fit.gamma)["shape"]
rate = coefficients(fit.gamma)["rate"]
print(coefficients(fit.gamma))
# testing the goodness of fit.
<<<<<<< HEAD
#num_of_samples = length(dist)
#y <- rgamma(num_of_samples, shape = shape, rate = rate)
#result = ks.test(dist, y)
ksResult <- ks.test(dist, "pgamma", shape, rate)
print("gamma goodness of fit")
print(ksResult)
ks = ksResult[['statistic']]
# iterate 10000 samples for ks-statistics
num_of_samples = length(dist)
sample_ks <- c()
runs = 10000
for(i in 1: runs){
run.Sample <- rgamma(num_of_samples, shape = shape, rate = rate)
run.fit.gamma <- fitdist(run.Sample, distr = "gamma", method = "mle", lower = c(0, 0))
# Check result
run.shape = coefficients(run.fit.gamma)["shape"]
run.rate = coefficients(run.fit.gamma)["rate"]
result = ks.test(run.Sample, "pgamma", run.shape, run.rate)
#result = ks.test(dist, y)
#print("gamma goodness of fit")
#print(result)
sample_ks = c(sample_ks, result[['statistic']])
}
tests<-sapply(sample_ks, function(x) {
=======
num_of_samples = length(vec)
y <- rgamma(num_of_samples, shape = shape, rate = rate)
result = ks.test(vec, y)
print("gamma goodness of fit")
print(result)
parametric-test <- parametric-ks-test(vec, shape, rate, result[["statistic"]])
print("parametric test")
print(parametric-test)
cutPoints <- qgamma(quantiles, shape, rate, lower.tail = TRUE)
cutPoints <- c(-Inf, cutPoints, Inf)
#print(cutPoints)
#par(mar = rep(2, 4))
#plot(fit.gamma)
}
parametric-ks-test <- function(dist, shape, rate, ks){
# iterate 10000 samples for ks-statistics
num_of_samples = length(dist)
sample-ks = c()
for(i in 1: 10000){
y <- rgamma(num_of_samples, shape = shape, scale = rate)
result = ks.test(dist, y)
#print("gamma goodness of fit")
#print(result)
sample-ks = c(sample-ks, result[['statistic']])
}
sapply(sample-ks, mean)
tests<-sapply(sample-ks, function(x) {
>>>>>>> f98318d8abf4bdfd4a0e02cac1ce966d2d1c13b7
if(x > ks ){
1
}
else {
0
}
})
<<<<<<< HEAD
print(sample_ks)
#print(tests)
parametric_test = sum(tests)/runs
print("parametric test")
print(parametric_test)
parametric_test
}
#Ks parameteric test
parametricKStest(combined[, "TD"])
ist <- combined[, "DETs"]
#tableValues <- table(dist)
#reduce sizes for fitting with gamma curve
#print(as.integer(tableValues/10))
#dist <- rep(as.numeric(names(tableValues)), as.integer(tableValues/10))
fit.gamma <- fitdist(dist, distr = "gamma", method = "mle", lower = c(0, 0))
# Check result
shape = coefficients(fit.gamma)["shape"]
rate = coefficients(fit.gamma)["rate"]
print(coefficients(fit.gamma))
# testing the goodness of fit.
#num_of_samples = length(dist)
#y <- rgamma(num_of_samples, shape = shape, rate = rate)
#result = ks.test(dist, y)
ksResult <- ks.test(dist, "pgamma", shape, rate)
print("gamma goodness of fit")
print(ksResult)
ks = ksResult[['statistic']]
# iterate 10000 samples for ks-statistics
num_of_samples = length(dist)
sample_ks <- c()
runs = 10000
for(i in 1: runs){
run.Sample <- rgamma(num_of_samples, shape = shape, rate = rate)
run.fit.gamma <- fitdist(run.Sample, distr = "gamma", method = "mle", lower = c(0, 0))
# Check result
run.shape = coefficients(run.fit.gamma)["shape"]
run.rate = coefficients(run.fit.gamma)["rate"]
result = ks.test(run.Sample, "pgamma", run.shape, run.rate)
#result = ks.test(dist, y)
#print("gamma goodness of fit")
#print(result)
sample_ks = c(sample_ks, result[['statistic']])
}
tests<-sapply(sample_ks, function(x) {
=======
sum(tests)
}
parametricKStest <- function(dist, shape, rate, ks){
# iterate 10000 samples for ks-statistics
num_of_samples = length(dist)
sample-ks = c()
for(i in 1: 10000){
y <- rgamma(num_of_samples, shape = shape, scale = rate)
result = ks.test(dist, y)
#print("gamma goodness of fit")
#print(result)
sample-ks = c(sample-ks, result[['statistic']])
}
sapply(sample-ks, mean)
tests<-sapply(sample-ks, function(x) {
>>>>>>> f98318d8abf4bdfd4a0e02cac1ce966d2d1c13b7
if(x > ks ){
1
}
else {
0
}
})
<<<<<<< HEAD
print(sample_ks)
#print(tests)
parametric_test = sum(tests)/runs
print("parametric test")
print(parametric_test)
parametric_test
dist <- combined[, "TD"]
dist <- combined[, "TD"]
#tableValues <- table(dist)
#reduce sizes for fitting with gamma curve
#print(as.integer(tableValues/10))
#dist <- rep(as.numeric(names(tableValues)), as.integer(tableValues/10))
fit.gamma <- fitdist(dist, distr = "gamma", method = "mle", lower = c(0, 0))
=======
sum(tests)
}
discretize <- function(data, n) {
# Discretizes continuous data into different levels of complexity based on
# quantiles of normal distribution defined by the data.
#
# Args:
#   data: vector of data to discretize
#   n: number of bins to discretize into
#
# Returns:
#   A vector of cut points
#n = 6
#data = combined[, "TD"]
#print(data)
if (n <= 1) {
return(c(-Inf, Inf))
}
quantiles <- seq(1/n, 1 - (1/n), 1/n)
tableValues <- table(data)
#reduce sizes for fitting with gamma curve
#print(as.integer(tableValues/10))
vec <- rep(as.numeric(names(tableValues)), as.integer(tableValues/10))
fit.gamma <- fitdist(vec, distr = "gamma", method = "mle", lower = c(0, 0))
>>>>>>> f98318d8abf4bdfd4a0e02cac1ce966d2d1c13b7
# Check result
shape = coefficients(fit.gamma)["shape"]
rate = coefficients(fit.gamma)["rate"]
print(coefficients(fit.gamma))
# testing the goodness of fit.
<<<<<<< HEAD
#num_of_samples = length(dist)
#y <- rgamma(num_of_samples, shape = shape, rate = rate)
#result = ks.test(dist, y)
ksResult <- ks.test(dist, "pgamma", shape, rate)
print("gamma goodness of fit")
print(ksResult)
ks = ksResult[['statistic']]
# iterate 10000 samples for ks-statistics
num_of_samples = length(dist)
sample_ks <- c()
runs = 10000
for(i in 1: runs){
run.Sample <- rgamma(num_of_samples, shape = shape, rate = rate)
run.fit.gamma <- fitdist(run.Sample, distr = "gamma", method = "mle", lower = c(0, 0))
# Check result
run.shape = coefficients(run.fit.gamma)["shape"]
run.rate = coefficients(run.fit.gamma)["rate"]
result = ks.test(run.Sample, "pgamma", run.shape, run.rate)
#result = ks.test(dist, y)
#print("gamma goodness of fit")
#print(result)
sample_ks = c(sample_ks, result[['statistic']])
}
tests<-sapply(sample_ks, function(x) {
if(x > ks ){
1
}
else {
0
}
})
print(sample_ks)
#print(tests)
parametric_test = sum(tests)/runs
print("parametric test")
print(parametric_test)
parametric_test
modelData <- read.csv("modelEvaluations-9-29.csv")
=======
num_of_samples = length(vec)
y <- rgamma(num_of_samples, shape = shape, rate = rate)
result = ks.test(vec, y)
print("gamma goodness of fit")
print(result)
parametric-test <- pparametricKStest(vec, shape, rate, result[["statistic"]])
print("parametric test")
print(parametric-test)
cutPoints <- qgamma(quantiles, shape, rate, lower.tail = TRUE)
cutPoints <- c(-Inf, cutPoints, Inf)
#print(cutPoints)
#par(mar = rep(2, 4))
#plot(fit.gamma)
}
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration4.R")
source("comparison_between_size_metrics.R")
library(jsonlite)
library(reshape)
library(tidyverse)
library(fitdistrplus)
library(egg)
library(gridExtra)
library(plyr)
cutsAsVec <- function(x) {
ret <- c("Cut Points:")
for (row in rownames(x)) {
cutPoints <- round(x[row, ], digits = 1)
ret <- c(ret, paste(row,":", paste(cutPoints, collapse = ", ")))
}
ret
}
modelData <- read.csv("modelEvaluations-9-13-1.csv")
>>>>>>> f98318d8abf4bdfd4a0e02cac1ce966d2d1c13b7
modelData$Project <- as.character(modelData$Project)
modelData$transaction_file <- as.character(modelData$transaction_file)
effort <- subset(modelData, select=c("Effort"))
transactionFiles <- subset(modelData, select=c("transaction_file"))
rownames(effort) <- modelData$Project
rownames(transactionFiles) <- modelData$Project
combined <- combineData(transactionFiles)
<<<<<<< HEAD
save.image("~/Research Projects/UMLx/data/TransactionWeighting/10-2.RData")
ist <- combined[, "TD"]
x.gam.cut<-cut(dist,breaks=c(0,3,6,9,12,Inf)) ##binning data
table(x.gam.cut) ## binned data table
dist <- combined[, "TD"]
x.gam.cut<-cut(dist,breaks=c(0,3,6,9,12,Inf)) ##binning data
table(x.gam.cut) ## binned data table
fit.gamma <- fitdist(dist, distr = "gamma", method = "mle", lower = c(0, 0))
# Check result
a.est = coefficients(fit.gamma)["shape"]
l.est = coefficients(fit.gamma)["rate"]
num_of_samples = length(dist)
(pgamma(3,shape=a.est,rate=l.est)-pgamma(0,shape=a.est,rate=l.est))*num_of_samples
(pgamma(6,shape=a.est,rate=l.est)-pgamma(3,shape=a.est,rate=l.est))*num_of_samples
(pgamma(9,shape=a.est,rate=l.est)-pgamma(6,shape=a.est,rate=l.est))*num_of_samples
(pgamma(12,shape=a.est,rate=l.est)-pgamma(9,shape=a.est,rate=l.est))*num_of_samples
(pgamma(Inf,shape=a.est,rate=l.est)-pgamma(12,shape=a.est,rate=l.est))*num_of_samples
p1 = (pgamma(3,shape=a.est,rate=l.est)-pgamma(0,shape=a.est,rate=l.est))*num_of_samples
p2 = (pgamma(6,shape=a.est,rate=l.est)-pgamma(3,shape=a.est,rate=l.est))*num_of_samples
p3 = (pgamma(9,shape=a.est,rate=l.est)-pgamma(6,shape=a.est,rate=l.est))*num_of_samples
p4 = (pgamma(12,shape=a.est,rate=l.est)-pgamma(9,shape=a.est,rate=l.est))*num_of_samples
p5 = (pgamma(Inf,shape=a.est,rate=l.est)-pgamma(12,shape=a.est,rate=l.est))*num_of_samples
f.ex<-c(p1, p2, p3, p4, p5) ## expected
f.os<-vector()
for(i in 1:5) f.os[i]<- table(x.gam.cut)[[i]] ## empirical frequencies
X2<-sum(((f.os-f.ex)^2)/f.ex) ## chi-square statistic
gdl<-5-2-1 ## degrees of freedom
1-pchisq(X2,gdl) ## p-value
f.ex<-c(p1, p2, p3, p4, p5) ## expected frequencies vector
f.os<-vector()
for(i in 1:5) f.os[i]<- table(x.gam.cut)[[i]] ## empirical frequencies
dist <- combined[, "TD"]
x.gam.cut<-cut(dist,breaks=c(0,3,6,9,12,Inf)) ##binning data
table(x.gam.cut) ## binned data table
fit.gamma <- fitdist(dist, distr = "gamma", method = "mme", lower = c(0, 0))
# Check result
a.est = coefficients(fit.gamma)["shape"]
l.est = coefficients(fit.gamma)["rate"]
num_of_samples = length(dist)
#x.gam.cut
## computing expected frequencies
p1 = (pgamma(3,shape=a.est,rate=l.est)-pgamma(0,shape=a.est,rate=l.est))*num_of_samples
p2 = (pgamma(6,shape=a.est,rate=l.est)-pgamma(3,shape=a.est,rate=l.est))*num_of_samples
p3 = (pgamma(9,shape=a.est,rate=l.est)-pgamma(6,shape=a.est,rate=l.est))*num_of_samples
p4 = (pgamma(12,shape=a.est,rate=l.est)-pgamma(9,shape=a.est,rate=l.est))*num_of_samples
p5 = (pgamma(Inf,shape=a.est,rate=l.est)-pgamma(12,shape=a.est,rate=l.est))*num_of_samples
f.ex<-c(p1, p2, p3, p4, p5) ## expected frequencies vector
f.os<-vector()
for(i in 1:5) f.os[i]<- table(x.gam.cut)[[i]] ## empirical frequencies
X2<-sum(((f.os-f.ex)^2)/f.ex) ## chi-square statistic
gdl<-5-2-1 ## degrees of freedom
1-pchisq(X2,gdl) ## p-value
dist <- combined[, "TD"]
x.gam.cut<-cut(dist,breaks=c(0,3,6,9,12,Inf)) ##binning data
table(x.gam.cut) ## binned data table
fit.gamma <- fitdist(dist, distr = "gamma", method = "mge", lower = c(0, 0))
# Check result
a.est = coefficients(fit.gamma)["shape"]
l.est = coefficients(fit.gamma)["rate"]
num_of_samples = length(dist)
#x.gam.cut
## computing expected frequencies
p1 = (pgamma(3,shape=a.est,rate=l.est)-pgamma(0,shape=a.est,rate=l.est))*num_of_samples
p2 = (pgamma(6,shape=a.est,rate=l.est)-pgamma(3,shape=a.est,rate=l.est))*num_of_samples
p3 = (pgamma(9,shape=a.est,rate=l.est)-pgamma(6,shape=a.est,rate=l.est))*num_of_samples
p4 = (pgamma(12,shape=a.est,rate=l.est)-pgamma(9,shape=a.est,rate=l.est))*num_of_samples
p5 = (pgamma(Inf,shape=a.est,rate=l.est)-pgamma(12,shape=a.est,rate=l.est))*num_of_samples
f.ex<-c(p1, p2, p3, p4, p5) ## expected frequencies vector
f.os<-vector()
for(i in 1:5) f.os[i]<- table(x.gam.cut)[[i]] ## empirical frequencies
X2<-sum(((f.os-f.ex)^2)/f.ex) ## chi-square statistic
gdl<-5-2-1 ## degrees of freedom
1-pchisq(X2,gdl) ## p-value
dist <- combined[, "TD"]
x.gam.cut<-cut(dist,breaks=c(0,3,6,9,12,Inf)) ##binning data
table(x.gam.cut) ## binned data table
fit.gamma <- fitdist(dist, distr = "gamma", method = "qge", lower = c(0, 0))
# Check result
a.est = coefficients(fit.gamma)["shape"]
l.est = coefficients(fit.gamma)["rate"]
num_of_samples = length(dist)
#x.gam.cut
## computing expected frequencies
p1 = (pgamma(3,shape=a.est,rate=l.est)-pgamma(0,shape=a.est,rate=l.est))*num_of_samples
p2 = (pgamma(6,shape=a.est,rate=l.est)-pgamma(3,shape=a.est,rate=l.est))*num_of_samples
p3 = (pgamma(9,shape=a.est,rate=l.est)-pgamma(6,shape=a.est,rate=l.est))*num_of_samples
p4 = (pgamma(12,shape=a.est,rate=l.est)-pgamma(9,shape=a.est,rate=l.est))*num_of_samples
p5 = (pgamma(Inf,shape=a.est,rate=l.est)-pgamma(12,shape=a.est,rate=l.est))*num_of_samples
f.ex<-c(p1, p2, p3, p4, p5) ## expected frequencies vector
f.os<-vector()
for(i in 1:5) f.os[i]<- table(x.gam.cut)[[i]] ## empirical frequencies
X2<-sum(((f.os-f.ex)^2)/f.ex) ## chi-square statistic
gdl<-5-2-1 ## degrees of freedom
1-pchisq(X2,gdl) ## p-value
dist <- combined[, "TD"]
x.gam.cut<-cut(dist,breaks=c(0,3,6,9,12,Inf)) ##binning data
table(x.gam.cut) ## binned data table
fit.gamma <- fitdist(dist, distr = "gamma", method = "qme", lower = c(0, 0))
# Check result
a.est = coefficients(fit.gamma)["shape"]
l.est = coefficients(fit.gamma)["rate"]
num_of_samples = length(dist)
#x.gam.cut
## computing expected frequencies
p1 = (pgamma(3,shape=a.est,rate=l.est)-pgamma(0,shape=a.est,rate=l.est))*num_of_samples
p2 = (pgamma(6,shape=a.est,rate=l.est)-pgamma(3,shape=a.est,rate=l.est))*num_of_samples
p3 = (pgamma(9,shape=a.est,rate=l.est)-pgamma(6,shape=a.est,rate=l.est))*num_of_samples
p4 = (pgamma(12,shape=a.est,rate=l.est)-pgamma(9,shape=a.est,rate=l.est))*num_of_samples
p5 = (pgamma(Inf,shape=a.est,rate=l.est)-pgamma(12,shape=a.est,rate=l.est))*num_of_samples
f.ex<-c(p1, p2, p3, p4, p5) ## expected frequencies vector
f.os<-vector()
for(i in 1:5) f.os[i]<- table(x.gam.cut)[[i]] ## empirical frequencies
X2<-sum(((f.os-f.ex)^2)/f.ex) ## chi-square statistic
gdl<-5-2-1 ## degrees of freedom
1-pchisq(X2,gdl) ## p-value
=======
View(model1)
View(model1)
View(SWTIresults_II)
View(SWTIresults_II)
View(SWTIIModelParameters)
View(SWTIIModelParameters)
load("D:/Research Projects/UMLx/data/TransactionWeighting/10-2.RData")
<<<<<<< HEAD
>>>>>>> f98318d8abf4bdfd4a0e02cac1ce966d2d1c13b7
=======
load("D:/Research Projects/UMLx/data/TransactionWeighting/9-4.RData")
source('D:/Research Projects/UMLx/data/TransactionWeighting/transaction_weights_calibration4.R')
testData = effort
mmre <- abs(testData - pred)/testData
mean_value <- mean(mmre)
mean_value
>>>>>>> 1fe44868607a8dc76e3327a35b3a002751dfc82b
