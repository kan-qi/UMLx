cutPoints <- matrix(NA, nrow = length(parameters), ncol = i + 1)
rownames(cutPoints) <- parameters
for (p in parameters) {
cutPoints[p, ] <- discretize(combinedData[, p], i)
}
regressionData <- matrix(nrow = length(dir(folder)), ncol = i^length(parameters) + 1)
rownames(regressionData) <- dir(folder)
colnames(regressionData) <- c(genColNames(parameters, i), "Effort")
for (file in dir(folder)) {
fileData <- read.csv(paste(folder, file, sep = "/"))
regressionData[file, ] <- c(classify(fileData, cutPoints), effortData[file, "Effort"])
}
regressionData <- as.data.frame(regressionData)
searchResults[[i]] <- list(MSE = crossValidate(regressionData, k), model = lm(Effort ~ ., regressionData), data = regressionData)
}
searchResults
}
effort <- as.data.frame(matrix(c(263, 268.5, 1482.5, 1392, 804.5, 3113, 1592), nrow = 7, ncol = 1))
colnames(effort) <- c("Effort")
rownames(effort) <- c("F14a_REFERsy.csv",
"F14a_soccer_data_web_crawler.csv",
"F15a_construction_meeting_minutes_application.csv",
"s16_bad_driver.csv",
"S16b_flower_seeker.csv",
"s16b_Picshare_AA.csv",
"s16b_Picshare_RA.csv")
results <- performSearch(4, "./Transaction Data", effort)
# This script performs search for the optimal model in predicting effort from
# software size. The measure of software size used is the sum of weighted
# transactions calculated in the following manner:
# 1. Discretize the transaction data based on quantiles of a normal distribution
#    obtained from aggregate data.
# 2. Classify each transaction based on the quantile it falls in.
# 3. Count the number of points that fall into each quantile and apply weights.
# 4. Sum all the weights.
# This script determines the optimal number of bins and the weights to apply
# using linear regression and k-fold cross validation.
#
# Usage:
#   1. Put all the transaction analytics files into a folder.
#   2. Construct a data frame with the effort values.
#   3. Call performSearch() with desired arguments.
#   4. Examine the object returned by performSearch().
#       Ex. results <- performSearch(4, "./Transaction Data", effort)
#       a. Return the mean MSE result for i bins: results[[i]]$MSE
#       b. Return the weights results for i bins: results[[i]]$model
#       c. Return the discretization/classification data for i bins: results[[i]]$data
library(ggplot2)
combineData <- function(folder) {
# Combines all the data from multiple transaction analytics files into one
# data frame.
#
# Args:
#   folder: folder containing all the files
#
# Returns:
#   A data frame containing all the data in all the files.
data <- NULL
for (file in dir(folder)) {
filepath <- paste(folder, file, sep="/")
if (is.null(data)) {
data <- read.csv(filepath)[, -c(1)]
}
else {
new <- read.csv(filepath)[, -c(1)]
data <- rbind(data, new)
}
}
data
}
discretize <- function(data, n) {
# Discretizes continuous data into different levels of complexity based on
# quantiles of normal distribution defined by the data.
#
# Args:
#   data: vector of data to discretize
#   n: number of bins to discretize into
#
# Returns:
#   A vector of cut points
if (n <= 1) {
return(c(-Inf, Inf))
}
quantiles <- seq(1/n, 1 - (1/n), 1/n)
cutPoints <- qnorm(quantiles, mean(data), sd(data), lower.tail = TRUE)
cutPoints <- c(-Inf, cutPoints, Inf)
}
classify <- function(data, cutPoints) {
# Classify data into different levels of complexity based on
# the quantile the data falls in.
#
# Args:
#   data: A dataframe of transactions to classfiy.
#   cutPoints: Matrix where each row is a vector of cut points. Each row
#     should be named according to the parameter they cut.
#
# Returns:
#   A vector that indicates how many data points fall into each bin.
numVariables <- nrow(cutPoints)
numBins <- length(cutPoints[1, ]) - 1
totalClassifications <- numBins^numVariables
result <- rep(0, totalClassifications)
names(result) <- genColNames(rownames(cutPoints), numBins)
for (i in 1:nrow(data)) {
classifications <- c()
for (p in rownames(cutPoints)) {
parameterResult <- cut(data[i, p], breaks = cutPoints[p, ], labels = FALSE)
classifications <- c(classifications, paste(p, parameterResult, sep = ""))
}
combinedClass <- paste(classifications, sep = "", collapse = "")
result[combinedClass] <- result[combinedClass] + 1
}
result
}
crossValidate <- function(data, k) {
# Performs k-fold cross validation with linear regression as training method.
#
# Args:
#   data: the data to perform cross-validation with
#   k: number of folds to use
#
# Returns:
#   The mean MSE for all folds.
folds <- cut(seq(1, nrow(data)), breaks = k, labels = FALSE)
foldMSE <- vector(length = k)
for (i in 1:k) {
testIndexes <- which(folds == i, arr.ind = TRUE)
testData <- data[testIndexes, ]
trainData <- data[-testIndexes, ]
model <- lm(Effort ~ ., data = trainData)
predicted <- predict.lm(model, newdata = testData)
foldMSE[i] <- mean((predicted - testData$Effort)^2)
}
mean(foldMSE)
}
genColNames <- function(parameters, nBins) {
# Helper function that generates a vector strings representing all possible
# classifications.
#
# Args:
#   parameters: vector of parameters being analyzed
#   nBins: number of bins being analyzed
#
# Returns:
#   A vector of strings for all possible classifications.
if (length(parameters) == 1) {
return(paste(parameters[1], 1:nBins, sep = ""))
}
else if (length(parameters) == 2) {
first <- paste(parameters[1], 1:nBins, sep = "")
second <- paste(parameters[2], 1:nBins, sep = "")
return (as.vector(sapply(first, paste, second, sep = "")))
}
else {
first <- paste(parameters[1], 1:nBins, sep = "")
second <- paste(parameters[2], 1:nBins, sep = "")
third <- paste(parameters[3], 1:nBins, sep = "")
result <- sapply(sapply(first, paste, second, sep = ""), paste, third, sep = "")
return(as.vector(result))
}
}
performSearch <- function(n, folder, effortData, parameters = c("TL", "TD", "DETs"), k = 5) {
# Performs search for the optimal number of bins and weights to apply to each
# bin through linear regression.
#
# Args:
#   n: Specifies up to how many bins per parameter to search.
#   folder: Folder containg all the transaction analytics data to analyze.
#   effortData: a data frame containing effort data corresponding to each of
#               the files contained in the folder argument. Rows must be named
#               the same as the filename and effort column should be named "Effort".
#   parameters: A vector of which parameters to analyze. Ex. "TL", "TD", "DETs"
#   k: How many folds to use for k-fold cross validation.
#
# Returns:
#   A list in which the ith index gives the results of the search for i bins.
combinedData <- combineData(folder)
paramAvg <- colMeans(combinedData[, parameters])
paramSD <- apply(combinedData[, parameters], 2, sd)
searchResults <- list()
for (i in seq(1,n)) {
cutPoints <- matrix(NA, nrow = length(parameters), ncol = i + 1)
rownames(cutPoints) <- parameters
for (p in parameters) {
cutPoints[p, ] <- discretize(combinedData[, p], i)
}
regressionData <- matrix(nrow = length(dir(folder)), ncol = i^length(parameters) + 1)
rownames(regressionData) <- dir(folder)
colnames(regressionData) <- c(genColNames(parameters, i), "Effort")
for (file in dir(folder)) {
fileData <- read.csv(paste(folder, file, sep = "/"))
regressionData[file, ] <- c(classify(fileData, cutPoints), effortData[file, "Effort"])
}
regressionData <- as.data.frame(regressionData)
searchResults[[i]] <- list(MSE = crossValidate(regressionData, k), model = lm(Effort ~ ., regressionData), data = regressionData)
}
searchResults
}
effort <- as.data.frame(matrix(c(263, 268.5, 1482.5, 1392, 804.5, 3113, 1592), nrow = 7, ncol = 1))
colnames(effort) <- c("Effort")
rownames(effort) <- c("F14a_REFERsy.csv",
"F14a_soccer_data_web_crawler.csv",
"F15a_construction_meeting_minutes_application.csv",
"s16_bad_driver.csv",
"S16b_flower_seeker.csv",
"s16b_Picshare_AA.csv",
"s16b_Picshare_RA.csv")
results <- performSearch(4, "./Transaction Data", effort)
# This script calculates the number of active contributors of an open source
# project using the Github API. Active contributors are defined as in:
# Software effort estimation based on open source projects: Case study of Github
# Information and Software Technology 92(2017)145-157
# After calculating the number of active and inactive contributors. The effort
# is simulated using simEffort().
library(jsonlite)
library(httr)
library(stringr)
getTotalPages <- function(linkHeaderStr) {
# Finds how many pages of results are contained in an API GET request.
#
# Args:
#   linkHeaderStr: the character string in the link header
#
# Returns:
#   The number of pages
matched <-
stringr::str_match(linkHeaderStr, "page=(\\d+)>; rel=\\\"last\\\"")
as.integer(matched[, 2])
}
dateInActiveDays <- function(x, dates) {
# Checks if a given day is in a vector of dates.
#
# Args:
#   x: date to check
#   dates: vector of dates to check
#
# Returns:
#   TRUE if x is in dates, otherwise FALSE.
answer = FALSE
for (day in dates) {
if (x == day) {
answer = TRUE
}
}
answer
}
getAllContributors <- function(url, user, pw) {
# Gets a list of all contributors to a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A dataframe containing all the contributors for that project.
contributors <- list()
info <- GET(paste(url, "/contributors?page=1", sep = ''), authenticate(user, pw))
pages <- getTotalPages(info$headers$link)
for (i in seq(1:pages)) {
resp <- GET(paste(url, "/contributors?page=", i, sep = ''), authenticate(user, pw))
currentPage <- fromJSON(content(resp, "text"), flatten = TRUE)
contributors[[i]] <- currentPage
}
contributors <- rbind_pages(contributors)
contributors
}
getAllCommits <- function(url, user, pw) {
# Gets a list of all commits to a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A dataframe containing all the commits for that project.
commits <- list()
info <-GET(paste(url, "/commits?page=1", sep = ''), authenticate(user, pw))
pages <- getTotalPages(info$headers$link)
for (i in seq(1:pages)) {
resp <- GET(paste(url, "/commits?page=", i, sep = ''), authenticate(user, pw))
currentPage <- fromJSON(content(resp, "text"), flatten = TRUE)
commits[[i]] <- currentPage
}
commits <- rbind_pages(commits)
commits <- commits[order(as.Date(commits$commit.committer.date)), ]
commits
}
getActiveContributors <- function(url, user, pw) {
# Gets a list of all active/inactive contributors every 30 active days for a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A list of active/inactive users every 30 active days.
active <- list()
commits <- getAllCommits(url, user, pw)
currentMonthActiveDays <- c()
currentMonthCommitCounts <- c()
lifetimeCommitsCounts <- c()
for (i in seq(1:nrow(commits))) {
commitAuthor <- if (is.na(commits[i, "author.login"])) commits[i, "commit.committer.name"] else commits[i, "author.login"]
if (length(currentMonthActiveDays) >= 30) {
# Record the active/inactive contributors for this month
activeThisMonth <- names(currentMonthCommitCounts[currentMonthCommitCounts / 30 >= 1])
activeCurrentLifetime <- names(lifetimeCommitsCounts[lifetimeCommitsCounts >= mean(lifetimeCommitsCounts)])
activeUsers = intersect(activeThisMonth, activeCurrentLifetime)
inactiveUsers = setdiff(names(currentMonthCommitCounts), activeUsers)
active[[length(active) + 1]] <- list(active = activeUsers, inactive = inactiveUsers,  days = as.Date(currentMonthActiveDays, origin = "1970-01-01"))
# Reset data for next month
currentMonthActiveDays <- c()
currentMonthCommitCounts <- c()
}
# Updating current month commit counts
if (commitAuthor %in% names(currentMonthCommitCounts)) {
currentMonthCommitCounts[commitAuthor] <-
currentMonthCommitCounts[commitAuthor] + 1
}
else {
currentMonthCommitCounts[commitAuthor] <- 1
}
# Updating current lifetime commit counts
if (commitAuthor %in% names(lifetimeCommitsCounts)) {
lifetimeCommitsCounts[commitAuthor] <-
lifetimeCommitsCounts[commitAuthor] + 1
}
else {
lifetimeCommitsCounts[commitAuthor] <- 1
}
# Updating active days
if (!dateInActiveDays(as.Date(commits[i, "commit.committer.date"]), currentMonthActiveDays)) {
currentMonthActiveDays <-
c(currentMonthActiveDays, as.Date(commits[i, "commit.committer.date"]))
}
}
active
}
simEffort <- function(url, user, pw) {
# Simulates effort of given GitHUb repository based on active and inactive
# contributors.
#
# Args:
#   url: the repo's github api url to simulate
#   user: your github username
#   pw: your github pw
#
# Returns:
#   Effort in person-hours
activeContributors <- getActiveContributors(url, user, pw)
effort <- 0
for (i in 1:length(activeContributors)) {
effort <- effort + (length(activeContributors[[i]]$active)*152) + (length(activeContributors[[i]]$inactive)*51)
}
effort
}
View(simEffort)
# This script calculates the number of active contributors of an open source
# project using the Github API. Active contributors are defined as in:
# Software effort estimation based on open source projects: Case study of Github
# Information and Software Technology 92(2017)145-157
# After calculating the number of active and inactive contributors. The effort
# is simulated using simEffort().
library(jsonlite)
library(httr)
library(stringr)
getTotalPages <- function(linkHeaderStr) {
# Finds how many pages of results are contained in an API GET request.
#
# Args:
#   linkHeaderStr: the character string in the link header
#
# Returns:
#   The number of pages
matched <-
stringr::str_match(linkHeaderStr, "page=(\\d+)>; rel=\\\"last\\\"")
as.integer(matched[, 2])
}
dateInActiveDays <- function(x, dates) {
# Checks if a given day is in a vector of dates.
#
# Args:
#   x: date to check
#   dates: vector of dates to check
#
# Returns:
#   TRUE if x is in dates, otherwise FALSE.
answer = FALSE
for (day in dates) {
if (x == day) {
answer = TRUE
}
}
answer
}
getAllContributors <- function(url, user, pw) {
# Gets a list of all contributors to a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A dataframe containing all the contributors for that project.
contributors <- list()
info <- GET(paste(url, "/contributors?page=1", sep = ''), authenticate(user, pw))
pages <- getTotalPages(info$headers$link)
for (i in seq(1:pages)) {
resp <- GET(paste(url, "/contributors?page=", i, sep = ''), authenticate(user, pw))
currentPage <- fromJSON(content(resp, "text"), flatten = TRUE)
contributors[[i]] <- currentPage
}
contributors <- rbind_pages(contributors)
contributors
}
getAllCommits <- function(url, user, pw) {
# Gets a list of all commits to a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A dataframe containing all the commits for that project.
commits <- list()
info <-GET(paste(url, "/commits?page=1", sep = ''), authenticate(user, pw))
pages <- getTotalPages(info$headers$link)
for (i in seq(1:pages)) {
resp <- GET(paste(url, "/commits?page=", i, sep = ''), authenticate(user, pw))
currentPage <- fromJSON(content(resp, "text"), flatten = TRUE)
commits[[i]] <- currentPage
}
commits <- rbind_pages(commits)
commits <- commits[order(as.Date(commits$commit.committer.date)), ]
commits
}
getActiveContributors <- function(url, user, pw) {
# Gets a list of all active/inactive contributors every 30 active days for a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A list of active/inactive users every 30 active days.
active <- list()
commits <- getAllCommits(url, user, pw)
currentMonthActiveDays <- c()
currentMonthCommitCounts <- c()
lifetimeCommitsCounts <- c()
for (i in seq(1:nrow(commits))) {
commitAuthor <- if (is.na(commits[i, "author.login"])) commits[i, "commit.committer.name"] else commits[i, "author.login"]
if (length(currentMonthActiveDays) >= 30) {
# Record the active/inactive contributors for this month
activeThisMonth <- names(currentMonthCommitCounts[currentMonthCommitCounts / 30 >= 1])
activeCurrentLifetime <- names(lifetimeCommitsCounts[lifetimeCommitsCounts >= mean(lifetimeCommitsCounts)])
activeUsers = intersect(activeThisMonth, activeCurrentLifetime)
inactiveUsers = setdiff(names(currentMonthCommitCounts), activeUsers)
active[[length(active) + 1]] <- list(active = activeUsers, inactive = inactiveUsers,  days = as.Date(currentMonthActiveDays, origin = "1970-01-01"))
# Reset data for next month
currentMonthActiveDays <- c()
currentMonthCommitCounts <- c()
}
# Updating current month commit counts
if (commitAuthor %in% names(currentMonthCommitCounts)) {
currentMonthCommitCounts[commitAuthor] <-
currentMonthCommitCounts[commitAuthor] + 1
}
else {
currentMonthCommitCounts[commitAuthor] <- 1
}
# Updating current lifetime commit counts
if (commitAuthor %in% names(lifetimeCommitsCounts)) {
lifetimeCommitsCounts[commitAuthor] <-
lifetimeCommitsCounts[commitAuthor] + 1
}
else {
lifetimeCommitsCounts[commitAuthor] <- 1
}
# Updating active days
if (!dateInActiveDays(as.Date(commits[i, "commit.committer.date"]), currentMonthActiveDays)) {
currentMonthActiveDays <-
c(currentMonthActiveDays, as.Date(commits[i, "commit.committer.date"]))
}
}
active
}
simEffort <- function(url, user, pw) {
# Simulates effort of given GitHUb repository based on active and inactive
# contributors.
#
# Args:
#   url: the repo's github api url to simulate
#   user: your github username
#   pw: your github pw
#
# Returns:
#   Effort in person-hours
activeContributors <- getActiveContributors(url, user, pw)
effort <- 0
for (i in 1:length(activeContributors)) {
effort <- effort + (length(activeContributors[[i]]$active)*152) + (length(activeContributors[[i]]$inactive)*51)
}
effort
}
simEffort("https://github.com/apache/carbondata", "flyqk", "qk@github/910304")
