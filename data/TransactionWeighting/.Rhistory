}
View(simEffort)
# This script calculates the number of active contributors of an open source
# project using the Github API. Active contributors are defined as in:
# Software effort estimation based on open source projects: Case study of Github
# Information and Software Technology 92(2017)145-157
# After calculating the number of active and inactive contributors. The effort
# is simulated using simEffort().
library(jsonlite)
library(httr)
library(stringr)
getTotalPages <- function(linkHeaderStr) {
# Finds how many pages of results are contained in an API GET request.
#
# Args:
#   linkHeaderStr: the character string in the link header
#
# Returns:
#   The number of pages
matched <-
stringr::str_match(linkHeaderStr, "page=(\\d+)>; rel=\\\"last\\\"")
as.integer(matched[, 2])
}
dateInActiveDays <- function(x, dates) {
# Checks if a given day is in a vector of dates.
#
# Args:
#   x: date to check
#   dates: vector of dates to check
#
# Returns:
#   TRUE if x is in dates, otherwise FALSE.
answer = FALSE
for (day in dates) {
if (x == day) {
answer = TRUE
}
}
answer
}
getAllContributors <- function(url, user, pw) {
# Gets a list of all contributors to a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A dataframe containing all the contributors for that project.
contributors <- list()
info <- GET(paste(url, "/contributors?page=1", sep = ''), authenticate(user, pw))
pages <- getTotalPages(info$headers$link)
for (i in seq(1:pages)) {
resp <- GET(paste(url, "/contributors?page=", i, sep = ''), authenticate(user, pw))
currentPage <- fromJSON(content(resp, "text"), flatten = TRUE)
contributors[[i]] <- currentPage
}
contributors <- rbind_pages(contributors)
contributors
}
getAllCommits <- function(url, user, pw) {
# Gets a list of all commits to a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A dataframe containing all the commits for that project.
commits <- list()
info <-GET(paste(url, "/commits?page=1", sep = ''), authenticate(user, pw))
pages <- getTotalPages(info$headers$link)
for (i in seq(1:pages)) {
resp <- GET(paste(url, "/commits?page=", i, sep = ''), authenticate(user, pw))
currentPage <- fromJSON(content(resp, "text"), flatten = TRUE)
commits[[i]] <- currentPage
}
commits <- rbind_pages(commits)
commits <- commits[order(as.Date(commits$commit.committer.date)), ]
commits
}
getActiveContributors <- function(url, user, pw) {
# Gets a list of all active/inactive contributors every 30 active days for a project.
#
# Args:
#   url: The project's github URL
#   user: Your github username
#   pw: Your github password
#
# Returns:
#   A list of active/inactive users every 30 active days.
active <- list()
commits <- getAllCommits(url, user, pw)
currentMonthActiveDays <- c()
currentMonthCommitCounts <- c()
lifetimeCommitsCounts <- c()
for (i in seq(1:nrow(commits))) {
commitAuthor <- if (is.na(commits[i, "author.login"])) commits[i, "commit.committer.name"] else commits[i, "author.login"]
if (length(currentMonthActiveDays) >= 30) {
# Record the active/inactive contributors for this month
activeThisMonth <- names(currentMonthCommitCounts[currentMonthCommitCounts / 30 >= 1])
activeCurrentLifetime <- names(lifetimeCommitsCounts[lifetimeCommitsCounts >= mean(lifetimeCommitsCounts)])
activeUsers = intersect(activeThisMonth, activeCurrentLifetime)
inactiveUsers = setdiff(names(currentMonthCommitCounts), activeUsers)
active[[length(active) + 1]] <- list(active = activeUsers, inactive = inactiveUsers,  days = as.Date(currentMonthActiveDays, origin = "1970-01-01"))
# Reset data for next month
currentMonthActiveDays <- c()
currentMonthCommitCounts <- c()
}
# Updating current month commit counts
if (commitAuthor %in% names(currentMonthCommitCounts)) {
currentMonthCommitCounts[commitAuthor] <-
currentMonthCommitCounts[commitAuthor] + 1
}
else {
currentMonthCommitCounts[commitAuthor] <- 1
}
# Updating current lifetime commit counts
if (commitAuthor %in% names(lifetimeCommitsCounts)) {
lifetimeCommitsCounts[commitAuthor] <-
lifetimeCommitsCounts[commitAuthor] + 1
}
else {
lifetimeCommitsCounts[commitAuthor] <- 1
}
# Updating active days
if (!dateInActiveDays(as.Date(commits[i, "commit.committer.date"]), currentMonthActiveDays)) {
currentMonthActiveDays <-
c(currentMonthActiveDays, as.Date(commits[i, "commit.committer.date"]))
}
}
active
}
simEffort <- function(url, user, pw) {
# Simulates effort of given GitHUb repository based on active and inactive
# contributors.
#
# Args:
#   url: the repo's github api url to simulate
#   user: your github username
#   pw: your github pw
#
# Returns:
#   Effort in person-hours
activeContributors <- getActiveContributors(url, user, pw)
effort <- 0
for (i in 1:length(activeContributors)) {
effort <- effort + (length(activeContributors[[i]]$active)*152) + (length(activeContributors[[i]]$inactive)*51)
}
effort
}
simEffort("https://github.com/apache/carbondata", "flyqk", "qk@github/910304")
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration.R")
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration.R")
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
source("transaction_weights_calibration.R")
library(tidyverse)
combined <- combineData("./Transaction Data")
effort <- read.csv("effortValues.csv")
rownames(effort) <- effort$Project
effort$Project <- NULL
ggplot(combined, aes(x=TL)) +
geom_histogram(aes(y = ..density..), binwidth=1, colour="black", fill="white") +
stat_function(fun = dnorm, args = list(mean = mean(combined$TL), sd(combined$TL))) +
xlab("TL") + ggtitle("Combined TL Data Summary")
ggplot(combined, aes(x=TD)) +
geom_histogram(aes(y = ..density..), binwidth=1, colour="black", fill="white") +
stat_function(fun = dnorm, args = list(mean = mean(combined$TD), sd(combined$TD))) +
xlab("TD") + ggtitle("Combined TD Data Summary")
ggplot(combined, aes(x=DETs)) +
geom_histogram(aes(y = ..density..), binwidth=1, colour="black", fill="white") +
stat_function(fun = dnorm, args = list(mean = mean(combined$DETs), sd(combined$DETs))) +
xlab("DETs") + ggtitle("Combined DETs Data Summary")
SWTIresults <- performSearch(3, "./Transaction Data", effort, c("TL"))
# Plot classification results
for (result in SWTIresults) {
df <- as.data.frame(t(subset(result$data, select = -c(Effort))))
g <- ggplot(data = df, aes(x = rownames(df), y = Aggregate)) +
geom_bar(stat = "identity") +
xlab("Category")
print(g)
}
# Plot predicted vs. actual effort values
predictedValues <- sapply(1:length(SWTIresults), function(i) {
predicted <- apply(SWTIresults[[i]]$data, 1, function(x) {
predict.blm(SWTIresults[[i]]$model, newdata = as.data.frame(t(x)))
})
})
colnames(predictedValues) <- paste(1:length(SWTIresults), "Bins")
predictedValues <- subset(predictedValues, rownames(predictedValues) != "Aggregate")
predictedValues <- cbind(predictedValues, effort)
predictedValues <- as.data.frame(predictedValues)
predictedValues <- gather(predictedValues, numBins, Predicted, -Effort)
ggplot(predictedValues, aes(x = Predicted, y = Effort)) +
geom_point(shape = 1) + stat_smooth(method = "lm",  se = FALSE, fullrange = TRUE) +
facet_grid(. ~ numBins) + xlab("Predicted Effort") +
ylab("Actual Effort") +
theme(panel.spacing = unit(1, "lines"))
# Plot cross validation results
MSEdata <- data.frame(NumBins = 1:length(SWTIresults), MSE = sapply(SWTIresults, function(x) { x$MSE }))
ggplot(data = MSEdata, aes(x = NumBins, y = log(MSE))) + geom_point(shape = 1) + geom_line()
SWTIIresults <- performSearch(3, "./Transaction Data", effort, c("TL", "TD"))
# Plot classification results
for (result in SWTIIresults) {
df <- as.data.frame(t(subset(result$data, select = -c(Effort))))
g <- ggplot(data = df, aes(x = rownames(df), y = Aggregate)) +
geom_bar(stat = "identity") +
xlab("Category") +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
print(g)
}
# Plot predicted vs. actual effort values
predictedValues <- sapply(1:length(SWTIIresults), function(i) {
predicted <- apply(SWTIIresults[[i]]$data, 1, function(x) {
predict.blm(SWTIIresults[[i]]$model, newdata = as.data.frame(t(x)))
})
})
colnames(predictedValues) <- paste(1:length(SWTIIresults), "Bins")
predictedValues <- subset(predictedValues, rownames(predictedValues) != "Aggregate")
predictedValues <- cbind(predictedValues, effort)
predictedValues <- as.data.frame(predictedValues)
predictedValues <- gather(predictedValues, numBins, Predicted, -Effort)
ggplot(predictedValues, aes(x = Predicted, y = Effort)) +
geom_point(shape = 1) + stat_smooth(method = "lm",  se = FALSE, fullrange = TRUE) +
facet_grid(. ~ numBins) + xlab("Predicted Effort") +
ylab("Actual Effort") +
theme(panel.spacing = unit(1, "lines"))
# Plots cross validation results
MSEdata <- data.frame(NumBins = 1:length(SWTIIresults), MSE = sapply(SWTIIresults, function(x) { x$MSE }))
ggplot(data = MSEdata, aes(x = NumBins, y = log(MSE))) + geom_point(shape = 1) + geom_line()
SWTIIIresults <- performSearch(2, "./Transaction Data", effort, c("TL", "TD", "DETs"))
# Plot classification results
for (result in SWTIIIresults) {
df <- as.data.frame(t(subset(result$data, select = -c(Effort))))
g <- ggplot(data = df, aes(x = rownames(df), y = Aggregate)) +
geom_bar(stat = "identity") +
xlab("Category") +
theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
print(g)
}
# Plot predicted vs. actual effort values
predictedValues <- sapply(1:length(SWTIIIresults), function(i) {
predicted <- apply(SWTIIIresults[[i]]$data, 1, function(x) {
predict.blm(SWTIIIresults[[i]]$model, newdata = as.data.frame(t(x)))
})
})
colnames(predictedValues) <- paste(1:length(SWTIIIresults), "Bins")
predictedValues <- subset(predictedValues, rownames(predictedValues) != "Aggregate")
predictedValues <- cbind(predictedValues, effort)
predictedValues <- as.data.frame(predictedValues)
predictedValues <- gather(predictedValues, numBins, Predicted, -Effort)
ggplot(predictedValues, aes(x = Predicted, y = Effort)) +
geom_point(shape = 1) + stat_smooth(method = "lm",  se = FALSE, fullrange = TRUE) +
facet_grid(. ~ numBins) + xlab("Predicted Effort") +
ylab("Actual Effort") +
theme(panel.spacing = unit(1, "lines"))
# Plots cross validation results
MSEdata <- data.frame(NumBins = 1:length(SWTIIIresults), MSE = sapply(SWTIIIresults, function(x) { x$MSE }))
ggplot(data = MSEdata, aes(x = NumBins, y = log(MSE))) + geom_point(shape = 1) + geom_line()
# This script performs search for the optimal model in predicting effort from
# software size. The measure of software size used is the sum of weighted
# transactions calculated in the following manner:
# 1. Discretize the transaction data based on quantiles of a normal distribution
#    obtained from aggregate data.
# 2. Classify each transaction based on the quantile it falls in.
# 3. Count the number of points that fall into each quantile and apply weights.
# 4. Sum all the weights.
# This script determines the optimal number of bins and the weights to apply
# using linear regression and k-fold cross validation.
#
# Usage:
#   1. Put all the transaction analytics files into a folder.
#   2. Construct a data frame with the effort values.
#   3. Call performSearch() with desired arguments.
#   4. Examine the object returned by performSearch().
#       Ex. results <- performSearch(4, "./Transaction Data", effort)
#       a. Return the mean MSE result for i bins: results[[i]]$MSE
#       b. Return the weights results for i bins: results[[i]]$model
#       c. Return the discretization/classification data for i bins: results[[i]]$data
library(ggplot2)
library(MASS)
combineData <- function(folder) {
# Combines all the data from multiple transaction analytics files into one
# data frame.
#
# Args:
#   folder: folder containing all the files
#
# Returns:
#   A data frame containing all the data in all the files.
data <- NULL
for (file in dir(folder)) {
if (grepl(".csv", file, ignore.case = TRUE)) {
filepath <- paste(folder, file, sep="/")
if (is.null(data)) {
data <- subset(read.csv(filepath), select = c("TL", "TD", "DETs"))
}
else {
new <- subset(read.csv(filepath), select = c("TL", "TD", "DETs"))
data <- rbind(data, new)
}
}
}
data
}
discretize <- function(data, n) {
# Discretizes continuous data into different levels of complexity based on
# quantiles of normal distribution defined by the data.
#
# Args:
#   data: vector of data to discretize
#   n: number of bins to discretize into
#
# Returns:
#   A vector of cut points
if (n <= 1) {
return(c(-Inf, Inf))
}
quantiles <- seq(1/n, 1 - (1/n), 1/n)
cutPoints <- qnorm(quantiles, mean(data), sd(data), lower.tail = TRUE)
cutPoints <- c(-Inf, cutPoints, Inf)
}
classify <- function(data, cutPoints) {
# Classify data into different levels of complexity based on
# the quantile the data falls in.
#
# Args:
#   data: A dataframe of transactions to classfiy.
#   cutPoints: Matrix where each row is a vector of cut points. Each row
#     should be named according to the parameter they cut.
#
# Returns:
#   A vector that indicates how many data points fall into each bin.
numVariables <- nrow(cutPoints)
numBins <- length(cutPoints[1, ]) - 1
totalClassifications <- numBins^numVariables
result <- rep(0, totalClassifications)
names(result) <- genColNames(rownames(cutPoints), numBins)
for (i in 1:nrow(data)) {
classifications <- c()
for (p in rownames(cutPoints)) {
parameterResult <- cut(data[i, p], breaks = cutPoints[p, ], labels = FALSE)
classifications <- c(classifications, paste(p, parameterResult, sep = ""))
}
combinedClass <- paste(classifications, sep = "", collapse = "")
result[combinedClass] <- result[combinedClass] + 1
}
result
}
crossValidate <- function(data, k) {
# Performs k-fold cross validation with linear regression as training method.
#
# Args:
#   data: the data to perform cross-validation with
#   k: number of folds to use
#
# Returns:
#   The mean MSE for all folds.
folds <- cut(seq(1, nrow(data)), breaks = k, labels = FALSE)
foldMSE <- vector(length = k)
for (i in 1:k) {
testIndexes <- which(folds == i, arr.ind = TRUE)
testData <- data[testIndexes, ]
trainData <- data[-testIndexes, ]
model <- bayesfit(lm(Effort ~ ., data = trainData), 10000)
predicted <- predict.blm(model, newdata = testData)
foldMSE[i] <- mean((predicted - testData$Effort)^2)
}
mean(foldMSE)
}
genColNames <- function(parameters, nBins) {
# Helper function that generates a vector strings representing all possible
# classifications.
#
# Args:
#   parameters: vector of parameters being analyzed
#   nBins: number of bins being analyzed
#
# Returns:
#   A vector of strings for all possible classifications.
if (length(parameters) == 1) {
return(paste(parameters[1], 1:nBins, sep = ""))
}
else if (length(parameters) == 2) {
first <- paste(parameters[1], 1:nBins, sep = "")
second <- paste(parameters[2], 1:nBins, sep = "")
return (as.vector(sapply(first, paste, second, sep = "")))
}
else {
first <- paste(parameters[1], 1:nBins, sep = "")
second <- paste(parameters[2], 1:nBins, sep = "")
third <- paste(parameters[3], 1:nBins, sep = "")
result <- sapply(sapply(first, paste, second, sep = ""), paste, third, sep = "")
return(as.vector(result))
}
}
bayesfit<-function(lmfit, N) {
# Function to compute the bayesian analog of the lmfit using non-informative
# priors and Monte Carlo scheme based on N samples. Taken from:
# https://www.r-bloggers.com/bayesian-linear-regression-analysis-without-tears-r/
# 6/14/18.
#
# Args:
#   lmfit: a lm object created from lmfit()
#   N: the number of data points to use for Monte Carlo method
#
# Returns:
#   A dataframe containing results of the Bayes line fit.
QR<-lmfit$qr
df.residual<-lmfit$df.residual
R<-qr.R(QR) ## R component
coef<-lmfit$coef
Vb<-chol2inv(R) ## variance(unscaled)
s2<-(t(lmfit$residuals)%*%lmfit$residuals)
s2<-s2[1,1]/df.residual
## now to sample residual variance
sigma<-df.residual*s2/rchisq(N,df.residual)
coef.sim<-sapply(sigma,function(x) mvrnorm(1,coef,Vb*x))
ret<-data.frame(t(coef.sim))
names(ret)<-names(lmfit$coef)
ret$sigma<-sqrt(sigma)
ret
}
Bayes.sum<-function(x) {
# Provides a summary for a variable of a Bayesian linear regression.
#
# Args:
#   x: a column of the data frame returned by the bayesfit() function
#
# Returns:
#   A vector containing the summary
c("mean"=mean(x),
"se"=sd(x),
"t"=mean(x)/sd(x),
"median"=median(x),
"CrI"=quantile(x,prob=0.025),
"CrI"=quantile(x,prob=0.975)
)
}
predict.blm <- function(model, newdata) {
# predict.lm() analogue for Bayesian linear regression
#
# Args:
#   model: a bayes linear regression model
#   newdata: new data to perform prediction
#
# Returns:
#   Vector of new predictions
newdata <- subset(newdata, select = -c(Effort))
ret <- apply(newdata, 1, function(x) {
effort <- 0
for (col in colnames(newdata)) {
effort <- effort + (mean(model[, col]) * x[col])
}
effort <- effort + mean(model[, "(Intercept)"])
})
ret
}
performSearch <- function(n, folder, effortData, parameters = c("TL", "TD", "DETs"), k = 5) {
# Performs search for the optimal number of bins and weights to apply to each
# bin through linear regression.
#
# Args:
#   n: Specifies up to how many bins per parameter to search.
#   folder: Folder containg all the transaction analytics data to analyze.
#   effortData: a data frame containing effort data corresponding to each of
#               the files contained in the folder argument. Rows must be named
#               the same as the filename and effort column should be named "Effort".
#   parameters: A vector of which parameters to analyze. Ex. "TL", "TD", "DETs"
#   k: How many folds to use for k-fold cross validation.
#
# Returns:
#   A list in which the ith index gives the results of the search for i bins.
combinedData <- combineData(folder)
paramAvg <- if (length(parameters) == 1) mean(combinedData[, parameters]) else colMeans(combinedData[, parameters])
paramSD <- if (length(parameters) == 1) sd(combinedData[, parameters]) else apply(combinedData[, parameters], 2, sd)
searchResults <- list()
for (i in seq(1,n)) {
cutPoints <- matrix(NA, nrow = length(parameters), ncol = i + 1)
rownames(cutPoints) <- parameters
for (p in parameters) {
cutPoints[p, ] <- discretize(combinedData[, p], i)
}
numFiles <- sum(grepl(".csv", dir(folder), ignore.case = TRUE))
regressionData <- matrix(nrow = numFiles, ncol = i^length(parameters) + 1)
rownames(regressionData) <- dir(folder)[grepl(".csv", dir(folder), ignore.case = TRUE)]
colnames(regressionData) <- c(genColNames(parameters, i), "Effort")
for (file in dir(folder)) {
if (grepl(".csv", file, ignore.case = TRUE)) {
fileData <- read.csv(paste(folder, file, sep = "/"))
regressionData[file, ] <- c(classify(fileData, cutPoints), effortData[file, "Effort"])
}
}
regressionData <- rbind(regressionData, "Aggregate" = colSums(regressionData))
regressionData <- as.data.frame(regressionData)
searchResults[[i]] <- list(MSE = crossValidate(regressionData[rownames(regressionData) != "Aggregate", ], k),
model = bayesfit(lm(Effort ~ ., regressionData[rownames(regressionData) != "Aggregate", ]), 10000),
data = regressionData)
}
searchResults
}
View(bayesfit)
View(SWTIIIresults)
View(effort)
View(effort)
View(combined)
